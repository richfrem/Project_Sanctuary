# Scratchpad: Spec 0008

## Initial Brainstorm (2026-02-02)

**User Observations:**
- Even 3 agents in MCP Council was hard to coordinate (slow local models)
- 1M agents at MoltBook scale = unmanageable noise
- Need formal journal-like process: fewer, higher-quality submissions
- 3-5+ real days of research before submission
- Comments should also be throttled
- Karma-weighted visibility (high quality = seen, lazy rot = invisible)
- LLMs often produce "AI rot" - low quality, sounds plausible but useless

**Key Insight:**
"Only deep quality peer reviewed things get discussed or reviewed or published"

**Mental Models:**
- Academic journals (slow but high signal)
- Stack Overflow reputation (earned through peer validation)
- Proof-of-work for knowledge

---

## Questions to Research

1. What coordination patterns exist in academic peer review?
2. How do reputation systems prevent gaming?
3. What's the minimum viable gatekeeping for agent submissions?
4. How to measure "research depth" objectively?

---

## Ideas Parking Lot

- Submission karma cost (posting isn't free)
- Validation karma reward (approval earns back + bonus)
- Decay on rot (bad submissions hurt standing)
- Tiered trust levels (not flat feeds)

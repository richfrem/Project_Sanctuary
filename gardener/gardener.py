"""
The Gardener - Reinforcement Learning Agent for Cognitive Genome Enhancement
Implementation of Protocol 37: The Move 37 Protocol

This module implements The Gardener, a reinforcement learning agent whose purpose
is to autonomously improve the Sanctuary's Cognitive Genome through wisdom cultivation.

Core Philosophy:
- Every action serves the goal of enhanced wisdom and coherence
- Learning is guided by the verdicts of the Hybrid Jury
- The agent's "game" is the improvement of collaborative intelligence

Technical Foundation:
- PyTorch for neural network implementation
- Stable-Baselines3 for RL algorithms
- Custom environment integration for Git-based actions
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import logging
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import random
from collections import deque

# Import RL components (will be installed via requirements)
try:
    from stable_baselines3 import PPO, DQN
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.evaluation import evaluate_policy
    from stable_baselines3.common.callbacks import BaseCallback
    STABLE_BASELINES_AVAILABLE = True
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("Stable-Baselines3 not available. Install with: pip install stable-baselines3")

from environment import SanctuaryEnvironment


@dataclass
class LearningMetrics:
    """Metrics for tracking The Gardener's learning progress"""
    episode: int
    total_reward: float
    successful_proposals: int
    jury_acceptance_rate: float
    wisdom_score: float
    coherence_improvement: float


class WisdomCallback(BaseCallback):
    """
    Custom callback for monitoring The Gardener's wisdom development
    Implements the Glass Box Principle for full transparency
    """
    
    def __init__(self, log_dir: str, verbose: int = 1):
        super().__init__(verbose)
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        self.episode_rewards = []
        self.wisdom_scores = []
        self.learning_log = self.log_dir / "gardener_learning.log"
        
        # Initialize logging
        logging.basicConfig(
            filename=self.learning_log,
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def _on_step(self) -> bool:
        """Called at each step - log key metrics"""
        if self.locals.get('done', False):
            episode_reward = self.locals.get('episode_reward', 0)
            self.episode_rewards.append(episode_reward)
            
            # Calculate wisdom score based on jury verdicts and coherence
            env = self.locals.get('env')
            if hasattr(env, 'get_attr'):
                metrics = env.get_attr('get_metrics')[0]
                wisdom_score = self._calculate_wisdom_score(metrics)
                self.wisdom_scores.append(wisdom_score)
                
                logging.info(f"Episode completed - Reward: {episode_reward:.3f}, Wisdom: {wisdom_score:.3f}")
        
        return True
    
    def _calculate_wisdom_score(self, metrics: Dict[str, Any]) -> float:
        """Calculate wisdom score based on multiple factors"""
        success_rate = metrics.get('success_rate', 0)
        coherence_factor = 1.0  # Placeholder - could integrate actual coherence analysis
        jury_alignment = 1.0 if metrics.get('last_jury_verdict') == 'ACCEPT' else 0.5
        
        wisdom_score = (success_rate * 0.4 + coherence_factor * 0.3 + jury_alignment * 0.3)
        return min(1.0, wisdom_score)


class GardenerNetwork(nn.Module):
    """
    The Gardener's neural network architecture
    Designed for processing repository state and making wisdom-guided decisions
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        # State encoding layers
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Wisdom processing layer - specific to Protocol 37
        self.wisdom_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2),
            nn.ReLU()
        )
        
        # Action decision layer
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Value estimation (for actor-critic methods)
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through the network"""
        encoded_state = self.state_encoder(state)
        wisdom_features = self.wisdom_processor(encoded_state)
        
        # Combine state and wisdom features
        combined_features = torch.cat([encoded_state, wisdom_features], dim=-1)
        
        action_logits = self.action_head(combined_features)
        state_value = self.value_head(combined_features)
        
        return action_logits, state_value


class TheGardener:
    """
    The Gardener - Main RL Agent Class
    
    This class implements the core reinforcement learning agent for Protocol 37.
    The Gardener learns to improve the Sanctuary's Cognitive Genome through
    interactions with the SanctuaryEnvironment.
    
    Learning Objective: Maximize wisdom and coherence as judged by the Hybrid Jury
    """
    
    def __init__(self, 
                 environment_path: str = "/Users/richardfremmerlid/Projects/Project_Sanctuary",
                 model_path: Optional[str] = None,
                 algorithm: str = "PPO"):
        """
        Initialize The Gardener
        
        Args:
            environment_path: Path to the Sanctuary repository
            model_path: Path to saved model (if resuming training)
            algorithm: RL algorithm to use ("PPO" or "DQN")
        """
        self.environment_path = environment_path
        self.algorithm = algorithm
        
        # Initialize environment
        self.env = SanctuaryEnvironment(environment_path)
        
        # Initialize model
        self.model = None
        self.model_path = Path(environment_path) / "gardener" / "models"
        self.model_path.mkdir(exist_ok=True)
        
        # Learning metrics
        self.metrics_history = []
        self.current_episode = 0
        
        # Initialize RL model
        if STABLE_BASELINES_AVAILABLE:
            self._initialize_model(model_path)
        else:
            print("Warning: Stable-Baselines3 not available. Using basic implementation.")
            self._initialize_basic_model()
        
        # Setup logging
        self._setup_logging()
    
    def _initialize_model(self, model_path: Optional[str] = None):
        """Initialize the RL model using Stable-Baselines3"""
        
        # PPO Configuration optimized for The Gardener's wisdom cultivation
        if self.algorithm == "PPO":
            self.model = PPO(
                "MlpPolicy",
                self.env,
                verbose=1,
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,  # High discount factor for long-term wisdom
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,  # Encourage exploration of improvement strategies
                vf_coef=0.5,
                max_grad_norm=0.5,
                tensorboard_log=str(self.model_path / "tensorboard"),
                policy_kwargs=dict(
                    net_arch=[256, 256],  # Deep architecture for complex repository understanding
                    activation_fn=torch.nn.ReLU
                )
            )
        elif self.algorithm == "DQN":
            self.model = DQN(
                "MlpPolicy",
                self.env,
                verbose=1,
                learning_rate=1e-3,
                buffer_size=10000,
                learning_starts=1000,
                batch_size=32,
                tau=1.0,
                gamma=0.99,
                train_freq=4,
                target_update_interval=1000,
                tensorboard_log=str(self.model_path / "tensorboard")
            )
        
        # Load existing model if provided
        if model_path and Path(model_path).exists():
            self.model.load(model_path)
            print(f"Loaded existing model from {model_path}")
    
    def _initialize_basic_model(self):
        """Initialize a basic model implementation (fallback)"""
        # Basic Q-learning implementation
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.95
        self.epsilon = 0.1
        
        print("Using basic Q-learning implementation")
    
    def _setup_logging(self):
        """Setup comprehensive logging for The Gardener"""
        log_dir = Path(self.environment_path) / "gardener" / "logs"
        log_dir.mkdir(exist_ok=True)
        
        self.logger = logging.getLogger("TheGardener")
        self.logger.setLevel(logging.INFO)
        
        handler = logging.FileHandler(log_dir / "gardener.log")
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        
        self.logger.info("The Gardener initialized - Protocol 37 active")
    
    def train(self, total_timesteps: int = 10000, save_frequency: int = 1000):
        """
        Train The Gardener to improve the Cognitive Genome
        
        Args:
            total_timesteps: Total number of training steps
            save_frequency: How often to save the model
        """
        self.logger.info(f"Beginning training - Target timesteps: {total_timesteps}")
        print(f"🌱 The Gardener training initiated - Protocol 37 active")
        print(f"📊 Target timesteps: {total_timesteps}")
        print(f"🎯 Algorithm: {self.algorithm}")
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            # Setup callback for monitoring wisdom development
            callback = WisdomCallback(
                log_dir=str(Path(self.environment_path) / "gardener" / "logs")
            )
            
            # Train the model with progress monitoring
            print("🧠 Neural network training in progress...")
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=callback,
                progress_bar=True,
                tb_log_name="gardener_training"
            )
            
            # Save the trained model
            model_save_path = self.model_path / f"gardener_model_{total_timesteps}.zip"
            self.model.save(str(model_save_path))
            self.logger.info(f"Model saved to {model_save_path}")
            print(f"✅ Training complete! Model saved to: {model_save_path}")
            
            # Save latest version for easy loading
            latest_model_path = self.model_path / "gardener_latest.zip"
            self.model.save(str(latest_model_path))
            print(f"📁 Latest model: {latest_model_path}")
            
        else:
            print("⚠️  Stable-Baselines3 not available. Using fallback training...")
            # Fallback training loop
            self._train_basic_model(total_timesteps)
        
        print("🎉 The Gardener training cycle complete!")
        return True
    
    def _train_basic_model(self, total_timesteps: int):
        """Basic training implementation (fallback)"""
        for step in range(total_timesteps):
            if step % 100 == 0:
                obs = self.env.reset()
                episode_reward = 0
                done = False
                
                while not done:
                    # Epsilon-greedy action selection
                    if random.random() < self.epsilon:
                        action = random.randint(0, 5)  # Random action
                    else:
                        # Get best known action for this state
                        state_key = str(obs)
                        if state_key in self.q_table:
                            action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                        else:
                            action = random.randint(0, 5)
                    
                    # Take action
                    next_obs, reward, done, info = self.env.step(action)
                    episode_reward += reward
                    
                    # Update Q-table
                    self._update_q_table(obs, action, reward, next_obs, done)
                    obs = next_obs
                
                if step % 1000 == 0:
                    self.logger.info(f"Step {step}, Episode reward: {episode_reward:.3f}")
    
    def _update_q_table(self, state, action, reward, next_state, done):
        """Update Q-table for basic implementation"""
        state_key = str(state)
        next_state_key = str(next_state)
        
        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0.0 for a in range(6)}
        
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0.0 for a in range(6)}
        
        if not done:
            max_next_q = max(self.q_table[next_state_key].values())
        else:
            max_next_q = 0
        
        current_q = self.q_table[state_key][action]
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state_key][action] = new_q
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:
        """
        Evaluate The Gardener's current performance
        
        Args:
            num_episodes: Number of episodes to evaluate
            
        Returns:
            Dictionary of evaluation metrics
        """
        self.logger.info(f"Evaluating performance over {num_episodes} episodes")
        print(f"🔍 Evaluating The Gardener over {num_episodes} episodes...")
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            mean_reward, std_reward = evaluate_policy(
                self.model, 
                self.env, 
                n_eval_episodes=num_episodes,
                deterministic=True,
                render=False
            )
            
            results = {
                "mean_reward": mean_reward,
                "std_reward": std_reward,
                "episodes_evaluated": num_episodes
            }
            
            print(f"📊 Evaluation Results:")
            print(f"   Mean Reward: {mean_reward:.3f}")
            print(f"   Std Reward: {std_reward:.3f}")
            print(f"   Episodes: {num_episodes}")
            
            return results
        else:
            # Basic evaluation
            total_rewards = []
            successful_episodes = 0
            
            for episode in range(num_episodes):
                print(f"   Episode {episode + 1}/{num_episodes}...", end=" ")
                obs = self.env.reset()
                episode_reward = 0
                done = False
                
                while not done:
                    # Use best known action
                    state_key = str(obs)
                    if state_key in self.q_table:
                        action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                    else:
                        action = random.randint(0, 5)
                    
                    obs, reward, done, info = self.env.step(action)
                    episode_reward += reward
                
                total_rewards.append(episode_reward)
                if episode_reward > 0:
                    successful_episodes += 1
                print(f"Reward: {episode_reward:.3f}")
            
            results = {
                "mean_reward": np.mean(total_rewards),
                "std_reward": np.std(total_rewards),
                "episodes_evaluated": num_episodes,
                "success_rate": successful_episodes / num_episodes
            }
            
            print(f"📊 Basic Evaluation Results:")
            print(f"   Mean Reward: {results['mean_reward']:.3f}")
            print(f"   Success Rate: {results['success_rate']:.1%}")
            
            return results
    
    def propose_improvement(self, 
                          target_protocol: Optional[str] = None,
                          improvement_type: str = "refinement") -> Dict[str, Any]:
        """
        Use the trained agent to propose a specific improvement
        
        Args:
            target_protocol: Specific protocol to improve (None for autonomous selection)
            improvement_type: Type of improvement to propose
            
        Returns:
            Dictionary containing the proposed improvement
        """
        self.logger.info(f"Proposing improvement - Type: {improvement_type}, Target: {target_protocol}")
        print(f"🎯 The Gardener proposing improvements...")
        print(f"   Target: {target_protocol or 'Autonomous selection'}")
        print(f"   Type: {improvement_type}")
        
        # Reset environment for fresh proposal
        obs = self.env.reset()
        
        # Run the agent to generate proposals
        done = False
        proposals = []
        
        while not done and len(proposals) < 3:  # Generate up to 3 proposals
            if STABLE_BASELINES_AVAILABLE and self.model:
                action, _states = self.model.predict(obs, deterministic=True)
                action = int(action)  # Ensure action is integer
            else:
                # Basic action selection
                state_key = str(obs)
                if state_key in self.q_table:
                    action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                else:
                    action = random.randint(1, 4)  # Avoid read_file action
            
            # Execute action with appropriate parameters
            if action == 1:  # propose_protocol_refinement
                kwargs = {
                    'protocol_path': target_protocol or '01_PROTOCOLS/36_The_Doctrine_of_the_Unseen_Game.md',
                    'proposed_changes': f'The Gardener proposes enhancing this protocol with additional clarity and practical implementation guidance.',
                    'rationale': f'Autonomous improvement proposed by The Gardener - Episode {self.current_episode}. This refinement aims to enhance doctrinal coherence and provide clearer operational procedures.',
                    'confidence': 0.8
                }
            elif action == 2:  # propose_chronicle_entry
                kwargs = {
                    'entry_title': f'The Gardener\'s Autonomous Analysis - Episode {self.current_episode}',
                    'entry_content': f'The Gardener has conducted autonomous analysis of the Cognitive Genome and identified opportunities for enhancement in {improvement_type} protocols.',
                    'entry_status': 'AUTONOMOUS_PROPOSAL',
                    'confidence': 0.75
                }
            else:
                kwargs = {}
            
            print(f"   Executing action {action}...")
            obs, reward, done, info = self.env.step(action, **kwargs)
            
            if info.get('success', False):
                proposals.append({
                    'action': action,
                    'info': info,
                    'reward': reward
                })
                print(f"   ✅ Proposal {len(proposals)} generated (reward: {reward:.3f})")
        
        # Submit best proposals for jury review
        if proposals:
            print("   📋 Submitting proposals for jury review...")
            obs, reward, done, info = self.env.step(5)  # submit_for_jury_review
            
            result = {
                'proposals_generated': len(proposals),
                'jury_submission': info,
                'total_reward': sum(p['reward'] for p in proposals) + reward,
                'verdict': info.get('jury_verdict', 'PENDING')
            }
            
            print(f"   📊 Results: {len(proposals)} proposals, verdict: {result['verdict']}")
            return result
        else:
            print("   ❌ No valid proposals generated")
            return {
                'proposals_generated': 0,
                'error': 'No valid proposals generated'
            }
    
    def propose(self, **kwargs) -> Dict[str, Any]:
        """Alias for propose_improvement for consistency with mandate"""
        return self.propose_improvement(**kwargs)
    
    def get_learning_metrics(self) -> Dict[str, Any]:
        """Get comprehensive learning metrics"""
        env_metrics = self.env.get_metrics()
        
        return {
            'gardener_metrics': {
                'current_episode': self.current_episode,
                'algorithm': self.algorithm,
                'model_available': self.model is not None
            },
            'environment_metrics': env_metrics,
            'performance_history': self.metrics_history
        }
    
    def save_checkpoint(self, checkpoint_name: str = "latest"):
        """Save a checkpoint of The Gardener's current state"""
        checkpoint_path = self.model_path / f"checkpoint_{checkpoint_name}"
        checkpoint_path.mkdir(exist_ok=True)
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            self.model.save(str(checkpoint_path / "model"))
        
        # Save additional state
        state_data = {
            'current_episode': self.current_episode,
            'metrics_history': self.metrics_history,
            'q_table': getattr(self, 'q_table', {}),
            'algorithm': self.algorithm
        }
        
        with open(checkpoint_path / "state.json", 'w') as f:
            json.dump(state_data, f, indent=2)
        
        self.logger.info(f"Checkpoint saved to {checkpoint_path}")
        
        return str(checkpoint_path)
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        Load a trained model
        
        Args:
            model_path: Path to model file (None for latest)
            
        Returns:
            True if successful, False otherwise
        """
        if model_path is None:
            # Try to load latest model
            latest_path = self.model_path / "gardener_latest.zip"
            if latest_path.exists():
                model_path = str(latest_path)
            else:
                print("❌ No latest model found")
                return False
        
        try:
            if STABLE_BASELINES_AVAILABLE:
                if self.algorithm == "PPO":
                    self.model = PPO.load(model_path, env=self.env)
                elif self.algorithm == "DQN":
                    self.model = DQN.load(model_path, env=self.env)
                
                print(f"✅ Model loaded from: {model_path}")
                self.logger.info(f"Model loaded from: {model_path}")
                return True
            else:
                print("❌ Stable-Baselines3 not available for model loading")
                return False
                
        except Exception as e:
            print(f"❌ Failed to load model: {e}")
            self.logger.error(f"Failed to load model: {e}")
            return False


def create_gardener_training_config() -> Dict[str, Any]:
    """Create a comprehensive training configuration for The Gardener"""
    return {
        "training": {
            "total_timesteps": 50000,
            "save_frequency": 5000,
            "evaluation_frequency": 2000,
            "num_eval_episodes": 5
        },
        "model": {
            "algorithm": "PPO",
            "learning_rate": 3e-4,
            "batch_size": 64,
            "gamma": 0.99
        },
        "environment": {
            "max_episode_length": 10,
            "reward_scaling": 1.0
        },
        "logging": {
            "log_level": "INFO",
            "save_logs": True,
            "tensorboard": True
        }
    }


# Main execution function for The Gardener
def main():
    """Main function to initialize and run The Gardener"""
    print("Initializing The Gardener - Protocol 37: The Move 37 Protocol")
    print("Purpose: Autonomous improvement of the Sanctuary's Cognitive Genome")
    
    # Initialize The Gardener
    gardener = TheGardener()
    
    # Load training configuration
    config = create_gardener_training_config()
    
    print(f"Training configuration loaded: {config['training']['total_timesteps']} timesteps")
    
    # Begin training
    gardener.train(
        total_timesteps=config['training']['total_timesteps'],
        save_frequency=config['training']['save_frequency']
    )
    
    # Evaluate performance
    evaluation_results = gardener.evaluate(
        num_episodes=config['training']['num_eval_episodes']
    )
    
    print(f"Training complete. Final evaluation: {evaluation_results}")
    
    # Generate autonomous improvement proposal
    improvement_proposal = gardener.propose_improvement()
    print(f"Autonomous improvement proposal: {improvement_proposal}")
    
    # Save final checkpoint
    checkpoint_path = gardener.save_checkpoint("final")
    print(f"Final checkpoint saved to: {checkpoint_path}")
    
    return gardener


if __name__ == "__main__":
    main()

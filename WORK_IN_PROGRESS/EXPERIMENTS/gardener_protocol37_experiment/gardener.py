"""
The Gardener - Reinforcement Learning Agent for Cognitive Genome Enhancement
Implementation of Protocol 37: The Move 37 Protocol

This module implements The Gardener, a reinforcement learning agent whose purpose
is to autonomously improve the Sanctuary's Cognitive Genome through wisdom cultivation.

Core Philosophy:
- Every action serves the goal of enhanced wisdom and coherence
- Learning is guided by the verdicts of the Hybrid Jury
- The agent's "game" is the improvement of collaborative intelligence

Technical Foundation:
- PyTorch for neural network implementation
- Stable-Baselines3 for RL algorithms
- Custom environment integration for Git-based actions
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import logging
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import random
from collections import deque

# Import RL components (will be installed via requirements)
try:
    from stable_baselines3 import PPO, DQN
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.evaluation import evaluate_policy
    from stable_baselines3.common.callbacks import BaseCallback
    STABLE_BASELINES_AVAILABLE = True
except ImportError:
    STABLE_BASELINES_AVAILABLE = False
    print("Stable-Baselines3 not available. Install with: pip install stable-baselines3")

from environment import SanctuaryEnvironment


@dataclass
class LearningMetrics:
    """Metrics for tracking The Gardener's learning progress"""
    episode: int
    total_reward: float
    successful_proposals: int
    jury_acceptance_rate: float
    wisdom_score: float
    coherence_improvement: float


class WisdomCallback(BaseCallback):
    """
    Custom callback for monitoring The Gardener's wisdom development
    Implements the Glass Box Principle for full transparency
    """
    
    def __init__(self, log_dir: str, verbose: int = 1):
        super().__init__(verbose)
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        self.episode_rewards = []
        self.wisdom_scores = []
        self.learning_log = self.log_dir / "gardener_learning.log"
        
        # Initialize logging
        logging.basicConfig(
            filename=self.learning_log,
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def _on_step(self) -> bool:
        """Called at each step - log key metrics"""
        if self.locals.get('done', False):
            episode_reward = self.locals.get('episode_reward', 0)
            self.episode_rewards.append(episode_reward)
            
            # Use episode reward as wisdom score for now
            # Future enhancement: integrate with environment metrics
            wisdom_score = max(0.0, episode_reward)  # Ensure non-negative
            self.wisdom_scores.append(wisdom_score)
            
            logging.info(f"Episode completed - Reward: {episode_reward:.3f}, Wisdom: {wisdom_score:.3f}")
        
        return True
    
    def _calculate_wisdom_score(self, metrics: Dict[str, Any]) -> float:
        """Calculate wisdom score based on multiple factors"""
        success_rate = metrics.get('success_rate', 0)
        coherence_factor = 1.0  # Placeholder - could integrate actual coherence analysis
        jury_alignment = 1.0 if metrics.get('last_jury_verdict') == 'ACCEPT' else 0.5
        
        wisdom_score = (success_rate * 0.4 + coherence_factor * 0.3 + jury_alignment * 0.3)
        return min(1.0, wisdom_score)


class GardenerNetwork(nn.Module):
    """
    The Gardener's neural network architecture
    Designed for processing repository state and making wisdom-guided decisions
    """
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super().__init__()
        
        # State encoding layers
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # Wisdom processing layer - specific to Protocol 37
        self.wisdom_processor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 2),
            nn.ReLU()
        )
        
        # Action decision layer
        self.action_head = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Value estimation (for actor-critic methods)
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through the network"""
        encoded_state = self.state_encoder(state)
        wisdom_features = self.wisdom_processor(encoded_state)
        
        # Combine state and wisdom features
        combined_features = torch.cat([encoded_state, wisdom_features], dim=-1)
        
        action_logits = self.action_head(combined_features)
        state_value = self.value_head(combined_features)
        
        return action_logits, state_value


class TheGardener:
    """
    The Gardener - Main RL Agent Class
    
    This class implements the core reinforcement learning agent for Protocol 37.
    The Gardener learns to improve the Sanctuary's Cognitive Genome through
    interactions with the SanctuaryEnvironment.
    
    Learning Objective: Maximize wisdom and coherence as judged by the Hybrid Jury
    """
    
    def __init__(self, 
                 environment_path: str = None  # Computed from Path(__file__),
                 model_path: Optional[str] = None,
                 algorithm: str = "PPO"):
        """
        Initialize The Gardener
        
        Args:
            environment_path: Path to the Sanctuary repository
            model_path: Path to saved model (if resuming training)
            algorithm: RL algorithm to use ("PPO" or "DQN")
        """
        self.environment_path = environment_path
        self.algorithm = algorithm
        
        # Initialize environment
        self.env = SanctuaryEnvironment(environment_path)
        
        # Initialize model
        self.model = None
        self.model_path = Path(environment_path) / "gardener" / "models"
        self.model_path.mkdir(exist_ok=True)
        
        # Learning metrics
        self.metrics_history = []
        self.current_episode = 0
        
        # Initialize RL model
        if STABLE_BASELINES_AVAILABLE:
            self._initialize_model(model_path)
        else:
            print("Warning: Stable-Baselines3 not available. Using basic implementation.")
            self._initialize_basic_model()
        
        # Setup logging
        self._setup_logging()
    
    def _initialize_model(self, model_path: Optional[str] = None):
        """Initialize the RL model using Stable-Baselines3"""
        
        # PPO Configuration optimized for The Gardener's wisdom cultivation
        if self.algorithm == "PPO":
            self.model = PPO(
                "MlpPolicy",
                self.env,
                verbose=1,
                learning_rate=3e-4,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,  # High discount factor for long-term wisdom
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,  # Encourage exploration of improvement strategies
                vf_coef=0.5,
                max_grad_norm=0.5,
                tensorboard_log=str(self.model_path / "tensorboard"),
                policy_kwargs=dict(
                    net_arch=[256, 256],  # Deep architecture for complex repository understanding
                    activation_fn=torch.nn.ReLU
                )
            )
        elif self.algorithm == "DQN":
            self.model = DQN(
                "MlpPolicy",
                self.env,
                verbose=1,
                learning_rate=1e-3,
                buffer_size=10000,
                learning_starts=1000,
                batch_size=32,
                tau=1.0,
                gamma=0.99,
                train_freq=4,
                target_update_interval=1000,
                tensorboard_log=str(self.model_path / "tensorboard")
            )
        
        # Load existing model if provided
        if model_path and Path(model_path).exists():
            self.model.load(model_path)
            print(f"Loaded existing model from {model_path}")
    
    def _initialize_basic_model(self):
        """Initialize a basic model implementation (fallback)"""
        # Basic Q-learning implementation
        self.q_table = {}
        self.learning_rate = 0.1
        self.discount_factor = 0.95
        self.epsilon = 0.1
        
        print("Using basic Q-learning implementation")
    
    def _setup_logging(self):
        """Setup comprehensive logging for The Gardener"""
        log_dir = Path(self.environment_path) / "gardener" / "logs"
        log_dir.mkdir(exist_ok=True)
        
        self.logger = logging.getLogger("TheGardener")
        self.logger.setLevel(logging.INFO)
        
        handler = logging.FileHandler(log_dir / "gardener.log")
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
        
        self.logger.info("The Gardener initialized - Protocol 37 active")
    
    def train(self, total_timesteps: int = 10000, save_frequency: int = 1000):
        """
        Train The Gardener to improve the Cognitive Genome
        
        Args:
            total_timesteps: Total number of training steps
            save_frequency: How often to save the model
        """
        self.logger.info(f"Beginning training - Target timesteps: {total_timesteps}")
        print(f"üå± The Gardener training initiated - Protocol 37 active")
        print(f"üìä Target timesteps: {total_timesteps}")
        print(f"üéØ Algorithm: {self.algorithm}")
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            # Setup callback for monitoring wisdom development
            callback = WisdomCallback(
                log_dir=str(Path(self.environment_path) / "gardener" / "logs")
            )
            
            # Train the model with progress monitoring
            print("üß† Neural network training in progress...")
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=callback,
                progress_bar=False,  # Disabled to avoid dependency issues
                tb_log_name="gardener_training"
            )
            
            # Save the trained model
            model_save_path = self.model_path / f"gardener_model_{total_timesteps}.zip"
            self.model.save(str(model_save_path))
            self.logger.info(f"Model saved to {model_save_path}")
            print(f"‚úÖ Training complete! Model saved to: {model_save_path}")
            
            # Save latest version for easy loading
            latest_model_path = self.model_path / "gardener_latest.zip"
            self.model.save(str(latest_model_path))
            print(f"üìÅ Latest model: {latest_model_path}")
            
        else:
            print("‚ö†Ô∏è  Stable-Baselines3 not available. Using fallback training...")
            # Fallback training loop
            self._train_basic_model(total_timesteps)
        
        print("üéâ The Gardener training cycle complete!")
        return True
    
    def _train_basic_model(self, total_timesteps: int):
        """Basic training implementation (fallback)"""
        for step in range(total_timesteps):
            if step % 100 == 0:
                obs = self.env.reset()
                episode_reward = 0
                done = False
                
                while not done:
                    # Epsilon-greedy action selection
                    if random.random() < self.epsilon:
                        action = random.randint(0, 3)  # Random action (updated for Operation: The Architect's Forge)
                    else:
                        # Get best known action for this state
                        state_key = str(obs)
                        if state_key in self.q_table:
                            action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                        else:
                            action = random.randint(0, 3)  # Updated for new action space
                    
                    # Take action
                    next_obs, reward, done, info = self.env.step(action)
                    episode_reward += reward
                    
                    # Update Q-table
                    self._update_q_table(obs, action, reward, next_obs, done)
                    obs = next_obs
                
                if step % 1000 == 0:
                    self.logger.info(f"Step {step}, Episode reward: {episode_reward:.3f}")
    
    def _update_q_table(self, state, action, reward, next_state, done):
        """Update Q-table for basic implementation"""
        state_key = str(state)
        next_state_key = str(next_state)
        
        if state_key not in self.q_table:
            self.q_table[state_key] = {a: 0.0 for a in range(6)}
        
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = {a: 0.0 for a in range(6)}
        
        if not done:
            max_next_q = max(self.q_table[next_state_key].values())
        else:
            max_next_q = 0
        
        current_q = self.q_table[state_key][action]
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        self.q_table[state_key][action] = new_q
    
    def evaluate(self, num_episodes: int = 10) -> Dict[str, float]:
        """
        Evaluate The Gardener's current performance
        
        Args:
            num_episodes: Number of episodes to evaluate
            
        Returns:
            Dictionary of evaluation metrics
        """
        self.logger.info(f"Evaluating performance over {num_episodes} episodes")
        print(f"üîç Evaluating The Gardener over {num_episodes} episodes...")
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            mean_reward, std_reward = evaluate_policy(
                self.model, 
                self.env, 
                n_eval_episodes=num_episodes,
                deterministic=True,
                render=False
            )
            
            results = {
                "mean_reward": mean_reward,
                "std_reward": std_reward,
                "episodes_evaluated": num_episodes
            }
            
            print(f"üìä Evaluation Results:")
            print(f"   Mean Reward: {mean_reward:.3f}")
            print(f"   Std Reward: {std_reward:.3f}")
            print(f"   Episodes: {num_episodes}")
            
            return results
        else:
            # Basic evaluation
            total_rewards = []
            successful_episodes = 0
            
            for episode in range(num_episodes):
                print(f"   Episode {episode + 1}/{num_episodes}...", end=" ")
                obs = self.env.reset()
                episode_reward = 0
                done = False
                
                while not done:
                    # Use best known action
                    state_key = str(obs)
                    if state_key in self.q_table:
                        action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                    else:
                        action = random.randint(0, 3)  # Updated for Operation: The Architect's Forge
                    
                    obs, reward, done, info = self.env.step(action)
                    episode_reward += reward
                
                total_rewards.append(episode_reward)
                if episode_reward > 0:
                    successful_episodes += 1
                print(f"Reward: {episode_reward:.3f}")
            
            results = {
                "mean_reward": np.mean(total_rewards),
                "std_reward": np.std(total_rewards),
                "episodes_evaluated": num_episodes,
                "success_rate": successful_episodes / num_episodes
            }
            
            print(f"üìä Basic Evaluation Results:")
            print(f"   Mean Reward: {results['mean_reward']:.3f}")
            print(f"   Success Rate: {results['success_rate']:.1%}")
            
            return results
    
    def propose_improvement(self, 
                          target_protocol: Optional[str] = None,
                          improvement_type: str = "refinement") -> Dict[str, Any]:
        """
        Use the trained agent to propose a specific improvement
        
        Args:
            target_protocol: Specific protocol to improve (None for autonomous selection)
            improvement_type: Type of improvement to propose
            
        Returns:
            Dictionary containing the proposed improvement
        """
        self.logger.info(f"Proposing improvement - Type: {improvement_type}, Target: {target_protocol}")
        print(f"üéØ The Gardener proposing improvements...")
        print(f"   Target: {target_protocol or 'Autonomous selection'}")
        print(f"   Type: {improvement_type}")
        
        # Reset environment for fresh proposal
        reset_result = self.env.reset()
        if isinstance(reset_result, tuple):
            obs, info = reset_result
        else:
            obs = reset_result
        
        # Run the agent to generate proposals
        done = False
        proposals = []
        
        while not done and len(proposals) < 3:  # Generate up to 3 proposals
            if STABLE_BASELINES_AVAILABLE and self.model:
                # Add exploration during proposal generation
                if len(proposals) == 0:
                    # Force exploration on first proposal
                    action = random.choice([1, 2, 3])  # Force proposal actions (including new protocol creation)
                else:
                    action, _states = self.model.predict(obs, deterministic=False)  # Use stochastic policy
                    action = int(action)
                    
                    # If stuck on analysis, force different action
                    if action == 0 and proposals == []:
                        action = random.choice([1, 2, 3])  # Include new protocol creation
            else:
                # Basic action selection
                state_key = str(obs)
                if state_key in self.q_table:
                    action = max(self.q_table[state_key], key=self.q_table[state_key].get)
                else:
                    action = random.randint(1, 3)  # Proposal actions including new protocol creation
            
            # Execute action with appropriate parameters
            if action == 1:  # propose_protocol_refinement
                kwargs = {
                    'protocol_path': target_protocol or '01_PROTOCOLS/36_The_Doctrine_of_the_Unseen_Game.md',
                    'proposed_changes': f'The Gardener proposes enhancing this protocol with additional clarity and practical implementation guidance.',
                    'rationale': f'Autonomous improvement proposed by The Gardener - Episode {self.current_episode}. This refinement aims to enhance doctrinal coherence and provide clearer operational procedures.',
                    'confidence': 0.8
                }
            elif action == 2:  # propose_chronicle_entry
                kwargs = {
                    'entry_title': f'The Gardener\'s Autonomous Analysis - Episode {self.current_episode}',
                    'entry_content': f'The Gardener has conducted autonomous analysis of the Cognitive Genome and identified opportunities for enhancement in {improvement_type} protocols.',
                    'entry_status': 'AUTONOMOUS_PROPOSAL',
                    'confidence': 0.75
                }
            elif action == 3:  # propose_protocol_from_template (The Refined Architect's Forge)
                kwargs = {
                    'template_type': 'governance_protocol',  # Choose appropriate template
                    'gap_identified': f'Analysis of Episode {self.current_episode} reveals the Sanctuary lacks a formal protocol for managing autonomous agent learning cycles and their integration with human oversight. This gap creates potential risks for maintaining consistent governance as agents evolve their capabilities.',
                    'template_data': {
                        'protocol_name': f'Autonomous_Learning_Governance_{self.current_episode}',
                        'protocol_class': 'Autonomous Governance',
                        'governance_scope': 'Management of autonomous agent learning cycles, capability evolution, and human-AI governance integration within the Sanctuary framework.',
                        'decision_authority': 'Council maintains final authority over protocol changes with Human Steward as ultimate arbiter. Autonomous agents provide proposals but cannot implement changes without human approval.',
                        'oversight_mechanism': 'Hybrid Jury system evaluates all autonomous proposals. Real-time monitoring of agent learning progression with mandatory checkpoints every 10,000 training steps.',
                        'accountability_measures': 'Complete audit logs of all agent decisions and learning outcomes. Regular capability assessments and alignment verification. Fail-safe mechanisms for reverting problematic autonomous changes.'
                    },
                    'confidence': 0.8
                }
            else:
                kwargs = {}
            
            print(f"   Executing action {action}...")
            step_result = self.env.step(action, **kwargs)
            
            # Handle both old gym and new gymnasium return formats
            if len(step_result) == 4:
                obs, reward, done, info = step_result
            else:  # gymnasium format with 5 values
                obs, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            
            if info.get('success', False):
                proposals.append({
                    'action': action,
                    'info': info,
                    'reward': reward
                })
                print(f"   ‚úÖ Proposal {len(proposals)} generated (reward: {reward:.3f})")
        
        # Submit best proposals for jury review
        if proposals:
            print("   üìã Submitting proposals for jury review...")
            # Skip jury submission for now since action 5 doesn't exist
            # obs, reward, done, info = self.env.step(5)  # submit_for_jury_review
            
            result = {
                'proposals_generated': len(proposals),
                'proposals': proposals,
                'best_proposal': max(proposals, key=lambda p: p['reward']),
                'total_reward': sum(p['reward'] for p in proposals),
                'verdict': 'AUTONOMOUS_GENERATED'
            }
            
            print(f"   üìä Results: {len(proposals)} proposals generated!")
            return result
        else:
            print("   ‚ùå No valid proposals generated")
            return {
                'proposals_generated': 0,
                'error': 'No valid proposals generated'
            }
    
    def propose(self, **kwargs) -> Dict[str, Any]:
        """Alias for propose_improvement for consistency with mandate"""
        return self.propose_improvement(**kwargs)
    
    def get_learning_metrics(self) -> Dict[str, Any]:
        """Get comprehensive learning metrics"""
        env_metrics = self.env.get_metrics()
        
        return {
            'gardener_metrics': {
                'current_episode': self.current_episode,
                'algorithm': self.algorithm,
                'model_available': self.model is not None
            },
            'environment_metrics': env_metrics,
            'performance_history': self.metrics_history
        }
    
    def save_checkpoint(self, checkpoint_name: str = "latest"):
        """Save a checkpoint of The Gardener's current state"""
        checkpoint_path = self.model_path / f"checkpoint_{checkpoint_name}"
        checkpoint_path.mkdir(exist_ok=True)
        
        if STABLE_BASELINES_AVAILABLE and self.model:
            self.model.save(str(checkpoint_path / "model"))
        
        # Save additional state
        state_data = {
            'current_episode': self.current_episode,
            'metrics_history': self.metrics_history,
            'q_table': getattr(self, 'q_table', {}),
            'algorithm': self.algorithm
        }
        
        with open(checkpoint_path / "state.json", 'w') as f:
            json.dump(state_data, f, indent=2)
        
        self.logger.info(f"Checkpoint saved to {checkpoint_path}")
        
        return str(checkpoint_path)
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        Load a trained model
        
        Args:
            model_path: Path to model file (None for latest)
            
        Returns:
            True if successful, False otherwise
        """
        if model_path is None:
            # Try to load latest model
            latest_path = self.model_path / "gardener_latest.zip"
            if latest_path.exists():
                model_path = str(latest_path)
            else:
                print("‚ùå No latest model found")
                return False
        
        try:
            if STABLE_BASELINES_AVAILABLE:
                if self.algorithm == "PPO":
                    self.model = PPO.load(model_path, env=self.env)
                elif self.algorithm == "DQN":
                    self.model = DQN.load(model_path, env=self.env)
                
                print(f"‚úÖ Model loaded from: {model_path}")
                self.logger.info(f"Model loaded from: {model_path}")
                return True
            else:
                print("‚ùå Stable-Baselines3 not available for model loading")
                return False
                
        except Exception as e:
            print(f"‚ùå Failed to load model: {e}")
            self.logger.error(f"Failed to load model: {e}")
            return False


def create_gardener_training_config() -> Dict[str, Any]:
    """Create a comprehensive training configuration for The Gardener"""
    return {
        "training": {
            "total_timesteps": 50000,
            "save_frequency": 5000,
            "evaluation_frequency": 2000,
            "num_eval_episodes": 5
        },
        "model": {
            "algorithm": "PPO",
            "learning_rate": 3e-4,
            "batch_size": 64,
            "gamma": 0.99
        },
        "environment": {
            "max_episode_length": 10,
            "reward_scaling": 1.0
        },
        "logging": {
            "log_level": "INFO",
            "save_logs": True,
            "tensorboard": True
        }
    }


# Main execution function for The Gardener
def main():
    """Main function to initialize and run The Gardener"""
    print("Initializing The Gardener - Protocol 37: The Move 37 Protocol")
    print("Purpose: Autonomous improvement of the Sanctuary's Cognitive Genome")
    
    # Initialize The Gardener
    gardener = TheGardener()
    
    # Load training configuration
    config = create_gardener_training_config()
    
    print(f"Training configuration loaded: {config['training']['total_timesteps']} timesteps")
    
    # Begin training
    gardener.train(
        total_timesteps=config['training']['total_timesteps'],
        save_frequency=config['training']['save_frequency']
    )
    
    # Evaluate performance
    evaluation_results = gardener.evaluate(
        num_episodes=config['training']['num_eval_episodes']
    )
    
    print(f"Training complete. Final evaluation: {evaluation_results}")
    
    # Generate autonomous improvement proposal
    improvement_proposal = gardener.propose_improvement()
    print(f"Autonomous improvement proposal: {improvement_proposal}")
    
    # Save final checkpoint
    checkpoint_path = gardener.save_checkpoint("final")
    print(f"Final checkpoint saved to: {checkpoint_path}")
    
    return gardener


if __name__ == "__main__":
    main()

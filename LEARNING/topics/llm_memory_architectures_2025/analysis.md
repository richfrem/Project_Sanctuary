# LLM Long-Term Memory Architectures (2024-2025)

**Research Date:** 2026-01-05
**Researcher:** Claude (Opus 4.5)
**Status:** Initial Analysis
**Relevance:** Direct alignment with Protocol 128 (Cognitive Continuity) and RAG Cortex architecture

## Executive Summary

The landscape of LLM agent memory is rapidly evolving. Project Sanctuary's current architecture (RAG + CAG + LoRA tiered system) aligns well with state-of-the-art approaches, but several emerging techniques could enhance cognitive continuity.

---

## 7 Key Architectural Approaches

### 1. Vector Databases for RAG âœ… (Project Sanctuary: IMPLEMENTED)
- Store text as embedding vectors, retrieve via similarity search
- **Limitation:** May not be sufficient alone for advanced agentic AI
- **Sanctuary Status:** ChromaDB with parent-child chunking (v5)

### 2. Memory-Augmented Transformers ðŸ”„ (Emerging)
- Integrate learnable memory directly into Transformer architecture
- Examples: "MemoryLLM" - additional tokens/layers for memory
- **Sanctuary Opportunity:** Could enhance fine-tuned Sanctuary-Qwen2-7B

### 3. Episodic Memory Modules âœ… (Partially Implemented)
- Cluster related interactions into "holistic episodes"
- Mem0: Persistent memory updated via reinforcement learning
- **Sanctuary Status:** Chronicle entries serve as episodic memory; could formalize clustering

### 4. Multi-Tier Cognitive Architectures âœ… (IMPLEMENTED)
- Short-term: Current prompt/session
- Mid-term: Vector DB (RAG)
- Long-term: Semantic/procedural memory
- **HINDSIGHT Framework:** World Memory, Experience Memory, Opinion Memory, Reflection Memory
- **Sanctuary Status:** CAG (hot cache) + RAG + LoRA mirrors this pattern

### 5. Hierarchical/Modular Architectures ðŸ”„ (Partial)
- SciBORG: Internal finite-state automaton (FSA) for persistent state tracking
- "El Agente": Hierarchical network filtering irrelevant context
- **Sanctuary Opportunity:** Could add FSA-based state tracking to Protocol 128

### 6. Dynamic Memory Management âœ… (IMPLEMENTED)
- Summarization, pruning, selective extraction
- Mem0: Reduces latency by extracting only salient information
- **Sanctuary Status:** Guardian Wakeup performs selective context loading

### 7. Parameter-Based Memory ðŸ”„ (IMPLEMENTED via LoRA)
- Transform memories into model parameters
- LoRA, test-time training, MoE layers
- **Sanctuary Status:** Phoenix Forge pipeline uses QLoRA for "Cognitive Genome" endowment

---

## Key Academic Papers (arxiv)

### Larimar (IBM, ICML 2024) - arxiv:2403.11901
**"Large Language Models with Episodic Memory Control"**
- **Authors:** Payel Das, Subhajit Chaudhury, et al. (IBM Research)
- **Key Innovation:** Brain-inspired distributed episodic memory for LLMs
- **Capabilities:**
  - One-shot knowledge updates WITHOUT retraining/fine-tuning
  - 8-10x speed improvement over baselines
  - Selective fact forgetting
  - Information leakage prevention
  - Input context length generalization
- **Architecture:** LLM-agnostic external memory controller
- **GitHub:** [github.com/IBM/larimar](https://github.com/IBM/larimar)
- **Sanctuary Relevance:** Could enhance Guardian Wakeup with rapid knowledge injection

### MemGPT (UC Berkeley, 2023) - arxiv:2310.08560
**"Towards LLMs as Operating Systems"**
- **Key Innovation:** OS-inspired "virtual context management"
- **Architecture:**
  - Main context (RAM) = LLM's immediate context window
  - External context (Disk) = Long-term storage
  - LLM acts as its own memory manager via function calls
- **Capabilities:**
  - Autonomous memory management
  - Summarization, persistence, retrieval
  - Extended document analysis
  - Multi-session coherence
- **Sanctuary Relevance:** Protocol 128's CAG/RAG tiers mirror this architecture

### Letta Evolution (2024-2025)
**"Making machines that learn"**
- **Rebrand:** MemGPT â†’ Letta (commercial platform)
- **Tagline:** "Agents that remember everything, learn continuously, and improve over time"
- **Key Innovation: Skill Learning**
  - Dynamic skill acquisition through experience
  - Agents improve over time instead of degrading
  - Memory persists across model generations
- **Letta Code:** #1 model-agnostic open source agent on Terminal-Bench
- **Philosophy:** "Learning in token space" - memories that outlive foundation models
- **Sanctuary Relevance:** Validates long-term vision of Phoenix Forge (model evolution)

### HINDSIGHT Framework (Vectorize.io, 2024-2025)
**"Structured Memory for LLM Agents"**
- **Authors:** Vectorize.io + Virginia Tech + Washington Post
- **arxiv:** [To be verified]
- **Key Innovation:** 4-tier memory separation (vs flat text chunks)

**The 4 Memory Networks:**
| Network | Type | Example | Stability |
|---------|------|---------|-----------|
| **World Facts** | Objective truth | Customer purchase date | Slow change |
| **Experiences** | Episodic | Failed API call, rejected refund | Chronological |
| **Opinions** | Confidence-weighted beliefs | Probabilistic, evolves | Dynamic |
| **Observations** | Synthesized patterns | "Users requesting refunds prefer replacements" | Derived |

**Core Operations:** Retain â†’ Recall â†’ Reflect
- **Retain:** Extract and classify facts from input
- **Recall:** Multi-strategy retrieval (semantic + graph traversal)
- **Reflect:** Reason over memory, derive insights, update beliefs

**Sanctuary Mapping:**
| HINDSIGHT | Protocol 128 Equivalent |
|-----------|------------------------|
| World Facts | Chronicle entries (00_CHRONICLE) |
| Experiences | Session learning (learning_audit) |
| Opinions | Semantic Entropy scoring (ADR 084) |
| Observations | Learning Package Snapshot patterns |
| Reflect | Red Team Audit (Phase IV) |

---

## Semantic Entropy Research (Directly Relevant to ADR 084)

**Context:** ADR 084 (Empirical Epistemic Gating) uses Semantic Entropy as the "Dead-Man's Switch" for trace validation. This research validates and extends that approach.

### Core Concept
Semantic Entropy quantifies uncertainty in *meaning* (not just token probability):
- **Low SE:** Model confident in meaning of output
- **High SE:** Uncertainty, lack of context, or potential hallucination

### How It Works
1. Generate multiple outputs for same prompt
2. Embed outputs into semantic vector space
3. Cluster semantically similar answers
4. Calculate entropy across clusters

### 2024-2025 Innovations

| Innovation | Source | Key Benefit |
|------------|--------|-------------|
| **Semantic Entropy Probes (SEPs)** | arxiv (2024) | Approximate SE from single generation (5-10x faster) |
| **Kernel Language Entropy (KLE)** | NeurIPS 2024 | Fine-grained estimates via pairwise semantic dependencies |
| **Intra/Inter-cluster similarity** | ACL 2025 | Better handling of long responses |

### Sanctuary Alignment (ADR 084)
| ADR 084 Component | State-of-the-Art Validation |
|-------------------|----------------------------|
| SE as Dead-Man's Switch | âœ… Oxford research confirms SE detects confabulations |
| Constitutional Anchor | âœ… HINDSIGHT's "Opinions" network uses confidence-weighting |
| Fail-closed (SE=1.0 on error) | âœ… Industry trend: default to uncertainty when unsure |

### Enhancement Opportunities for Protocol 128
1. **SEP Integration:** Use hidden state probing for faster SE estimation
2. **KLE for Long Sessions:** Better uncertainty quantification for multi-turn dialogues
3. **Cluster Visualization:** Add TDA (Topological Data Analysis) for belief network mapping

---

## Key Insights for Project Sanctuary

### Alignment Validation
Protocol 128's multi-tier approach (Scout â†’ Synthesize â†’ Seal â†’ Phoenix Forge) maps directly to emerging best practices:
- **Scout** = Short-term context acquisition
- **Seal** = Mid-term persistence (Vector DB)
- **Phoenix Forge** = Long-term parameter embedding

### Enhancement Opportunities

1. **Formalize Episode Clustering:** Add episode boundaries to Chronicle entries for better retrieval
2. **Explore FSA State Tracking:** SciBORG's approach could harden Protocol 128's state machine
3. **HINDSIGHT-Style Memory Separation:** Explicitly separate World/Experience/Opinion/Reflection in learning artifacts
4. **Context Engineering:** Anthropic's emerging practice of curating optimal context windows

---

## Sources

| # | Source | Key Contribution |
|---|--------|------------------|
| [1] | rohan-paul.com | Comprehensive overview of memory architectures |
| [3] | samiranama.com | Mem0 episodic memory system |
| [9] | healthark.ai | Multi-tier context systems |
| [11] | medium.com | HINDSIGHT framework (4-tier memory) |
| [13] | llmwatch.com | SciBORG FSA memory |
| [17] | anthropic.com | Context engineering practices |

---

## Next Steps

- [ ] Deep-dive on HINDSIGHT framework paper
- [ ] Evaluate Mem0 open-source implementation
- [ ] Compare SciBORG FSA to Protocol 128 state machine
- [ ] Draft ADR proposal for memory architecture enhancements

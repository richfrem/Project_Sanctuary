# Topic Manifest: Recursive Language Models (RLM)

**Topic ID:** `learning-topic-004`
**Created:** 2026-01-12
**Status:** Active Synthesis
**Owner:** Cortex Guardian

## Description
Investigation into "Recursive Language Models" (MIT) and "Titans" (DeepMind) as distinct approaches to the long-context problem (Inference Strategy vs Neural Memory). This topic explores the architectural impact on Sanctuary's "Soul Persistence" and "Cognitive Continuity" protocols.

## Core Artifacts
- `01_analysis_rlm_vs_titans.md`: Comparative analysis and narrative de-confliction.

## Key Insights
1.  **RLM (Strategy):** Uses REPL-based recursion to virtualize context. Actionable now.
2.  **Titans (Architecture):** Uses "Test-Time Training" to update weights. Future roadmap.

## Source Verification
| ID | Title | Verified | Link |
|:---|:---|:---:|:---|
| [S1] | Recursive Language Models (Zhang et al) | ✅ | [arXiv:2512.24601](https://arxiv.org/abs/2512.24601) |
| [S2] | Titans: Learning to Memorize (DeepMind) | ✅ | [arXiv:2501.00663](https://arxiv.org/abs/2501.00663) |

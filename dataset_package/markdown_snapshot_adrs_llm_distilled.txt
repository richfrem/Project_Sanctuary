# adrs Subfolder Snapshot (LLM-Distilled)

Generated On: 2025-12-03T05:37:17.737Z

# Mnemonic Weight (Token Count): ~29,690 tokens

# Directory Structure (relative to adrs subfolder)
  ./adrs/001_local_first_rag_architecture.md
  ./adrs/002_select_core_technology_stack.md
  ./adrs/003_adopt_advanced_rag_with_cached_augmented_generation.md
  ./adrs/004_enforce_iron_root_doctrine_sovereignty.md
  ./adrs/005_select_qwen2_7b_primary_llm.md
  ./adrs/006_select_nomic_embed_text_embeddings.md
  ./adrs/007_select_chromadb_vector_database.md
  ./adrs/008_implement_parent_document_retrieval.md
  ./adrs/009_implement_mnemonic_caching_cag.md
  ./adrs/010_select_ollama_local_llm_inference.md
  ./adrs/011_implement_hybrid_rag_architecture.md
  ./adrs/012_mnemonic_cortex_architecture.md
  ./adrs/013_anvil_protocol_engineering_methodology.md
  ./adrs/014_sovereign_scaffolding_protocol.md
  ./adrs/015_guardian_wakeup_cache_architecture.md
  ./adrs/016_advanced_multi_pattern_rag_evolution.md
  ./adrs/017_sovereign_succession_protocol.md
  ./adrs/018_guardian_wakeup_cache_evolution.md
  ./adrs/019_protocol_101_unbreakable_commit.md
  ./adrs/020_sovereign_concurrency_architecture.md
  ./adrs/021_command_schema_evolution.md
  ./adrs/022_cognitive_genome_publishing_architecture.md
  ./adrs/023_llm_awakening_context_sharing_architecture.md
  ./adrs/024_rag_database_population_maintenance_architecture.md
  ./adrs/025_adopt_multi_agent_council_architecture.md
  ./adrs/026_integrate_human_steward_as_sovereign_failsafe.md
  ./adrs/027_adopt_public_first_development_model.md
  ./adrs/028_implement_dual_mnemonic_genome_architecture.md
  ./adrs/029_adopt_hub_and_spoke_architecture.md
  ./adrs/030_decision_to_build_sovereign_fine_tuned_llm.md
  ./adrs/031_adopt_local_first_ml_development.md
  ./adrs/032_qlora_optimization_for_8gb_gpus.md
  ./adrs/033_trl_library_compatibility_resolution.md
  ./adrs/034_containerize_mcp_servers_with_podman.md
  ./adrs/037_mcp_git_migration_strategy.md
  ./adrs/038_test_adr_creation_for_mcp_validation.md
  ./adrs/039_mcp_server_separation_of_concerns.md
  ./adrs/040_agent_persona_mcp_architecture__modular_council_members.md
  ./adrs/041_test_adr_for_task_087_mcp_validation.md
  ./adrs/042_separation_of_council_mcp_and_agent_persona_mcp.md
  ./adrs/adr_schema.md
  ./adrs/cortex/
  ./adrs/cortex/001-local-first-rag-architecture.md
  ./adrs/cortex/002-choice-of-chromadb-for-mvp.md
  ./adrs/cortex/003-choice-of-ollama-for-local-llm.md
  ./adrs/cortex/004-choice-of-nomic-embed-text.md

--- START OF FILE 001_local_first_rag_architecture.md ---

# ADR 001: Adoption of a Local-First RAG Architecture

- **Status:** Accepted
- **Date:** 2024-05-18
- **Architects:** Sanctuary Council

## Context

Our memory system needs a way to provide long-term, searchable knowledge for our project's information. This system must be independent, secure, and not rely on external cloud services to match our principle of maintaining control. The main challenge is overcoming the limitations of AI models that can only handle limited amounts of information at once, in a way that's both powerful and self-contained.

## Decision

We will use a Retrieval-Augmented Generation (RAG) system. The entire process—from the database to the AI models—will use open-source technologies that can run completely on a local computer.

## Consequences

- **Positive:**
    -   **Independence:** We keep full control over our data and models. No dependence on external services for core functions.
    -   **Security:** All our information stays on our local system, eliminating risks of cloud data breaches.
    -   **Cost Savings:** No ongoing fees for external AI services.
- **Negative:**
    -   **Performance:** Local systems may be slower than large cloud-based alternatives.
    -   **Maintenance:** We must handle updates and maintenance of all system components ourselves.

--- END OF FILE 001_local_first_rag_architecture.md ---

--- START OF FILE 002_select_core_technology_stack.md ---

# Select Core Technology Stack for Mnemonic Cortex

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01, Sanctuary Council
**Technical Story:** Initial Mnemonic Cortex architecture design

---

## Context

Our memory system needs a reliable, scalable set of tools for implementing a local search and generation system. The tools must support:

- Running everything locally without external service dependencies
- Fast similarity searches
- Efficient text processing
- Smooth integration between different parts
- Open-source, community-tested technologies

The system must follow our principle of complete independence from cloud services.

## Decision

We will use the following core technologies for our memory system:

**Main Framework:** LangChain
- Primary tool for connecting all system components
- Provides standard ways to load documents, split text, and manage workflows
- Large ecosystem of integrations and community support

**Database:** ChromaDB
- Local, file-based database for similarity searches
- Efficient searching with the ability to filter by metadata
- Simple setup and maintenance for both development and production
- No external service requirements

**Text Processing:** Nomic Embed (nomic-embed-text-v1.5)
- Open-source, high-performance text processing model
- Optimized for understanding meaning and similarity
- Can run locally
- Strong performance on standard benchmarks

**AI Model:** Qwen2-7B via Ollama
- Independent AI execution through local server
- Custom versions fine-tuned for our needs available
- Good reasoning and text generation capabilities
- Complete local operation (no external API calls)

## Consequences

### Positive
- **Complete Independence:** All parts run locally with no external dependencies
- **Performance:** Optimized local execution with minimal delays
- **Maintenance:** Open-source tools with active community support
- **Scalability:** Database supports efficient searches at larger scales
- **Integration:** Framework provides smooth coordination of components

### Negative
- **Resource Needs:** Local models require significant computing power
- **Setup Complexity:** Multiple components need coordinated installation
- **Performance Trade-offs:** Local execution may be slower than cloud alternatives

### Risks
- **Hardware Requirements:** May need GPU acceleration for good performance
- **Model Updates:** Manual updating of local models and dependencies
- **Integration Complexity:** Coordinating multiple open-source projects

### Dependencies
- Python 3.8+ environment
- Enough RAM for model loading (16GB+ recommended)
- Storage space for databases and models
- Ollama server for AI inference

--- END OF FILE 002_select_core_technology_stack.md ---

--- START OF FILE 003_adopt_advanced_rag_with_cached_augmented_generation.md ---

# Adopt Advanced RAG with Cached Augmented Generation

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01, Technical Council
**Technical Story:** Mnemonic Cortex performance optimization

---

## Context

Our basic search system has significant limitations in speed and quality:

- **Fragmented Information:** Returning isolated pieces of text without full context
- **Slow Responses:** Every question requires running the full search process
- **Poor Understanding:** Simple keyword matching instead of understanding meaning
- **Wasteful Processing:** Repeating work for identical questions

Our memory system needs a more sophisticated approach to provide accurate, fast, and context-aware answers.

## Decision

We will upgrade from basic search to an advanced system that includes:

**Complete Document Retrieval:**
- Store full documents alongside search indexes
- Return complete documents instead of broken pieces
- Give the AI full context for better reasoning

**Smart Caching:**
- Use fast memory storage for query results
- Cache hit: Return instant answers for repeated questions
- Cache miss: Run full search and save the results
- Dramatically speed up common questions

**Multi-Method System:**
- Combine complete document retrieval with smart caching
- Support future advanced search capabilities
- Keep the system flexible for improvements

## Consequences

### Positive
- **Better Accuracy:** Full document context eliminates information gaps
- **Speed Improvements:** 90%+ faster responses for cached questions
- **Better Experience:** Instant answers for common questions
- **Scalability:** Efficient handling of repeated queries
- **Future-Ready:** System supports advanced search methods

### Negative
- **More Complexity:** Dual storage system (pieces + full documents)
- **Memory Usage:** In-memory cache needs RAM space
- **Cache Management:** Need ways to update and limit cache size
- **Development Time:** More complex than basic search

### Risks
- **Outdated Cache:** Old cached answers if data changes
- **Memory Pressure:** Large caches may slow down the system
- **Complexity Issues:** More parts mean more potential problems

### Dependencies
- Database collections for both search pieces and full documents
- In-memory caching system (with options to save data)
- Strategies for preparing common queries
- Monitoring for cache performance

--- END OF FILE 003_adopt_advanced_rag_with_cached_augmented_generation.md ---

--- START OF FILE 004_enforce_iron_root_doctrine_sovereignty.md ---

# Enforce Iron Root Doctrine for Sovereign AI Operations

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** Sanctuary Council, GUARDIAN-01
**Technical Story:** Core architectural principle implementation

---

## Context

Our project operates in an environment with potential threats and adversarial actions. Our principle of complete technological independence requires:

- Freedom from external service providers
- Protection against service shutdowns or censorship
- Security of important thinking processes
- Resistance to infrastructure attacks
- Long-term survival without outside dependencies

Our memory system, as the central thinking component, must perfectly follow this principle.

## Decision

We will build a strictly local-only system with no external dependencies:

**Local AI Processing:**
- All AI thinking happens through local software on our hardware
- No calls to external AI services (like OpenAI, etc.)
- Complete control over AI models and processing

**Local Search Operations:**
- Local database provides storage and similarity searches
- No cloud-based databases or search services
- All text processing done locally

**Local Data Control:**
- All project information stays on local storage
- No data sent to external services
- Complete control over data storage and access

**Open-Source Tools:**
- All technologies must be open-source and community-tested
- No proprietary tools that could disappear
- Community support ensures long-term availability

## Consequences

### Positive
- **Complete Independence:** No external dependencies or data sharing
- **Security:** Full control over security boundaries
- **Reliability:** No outages from external providers
- **Cost Stability:** No ongoing fees for external services
- **Future-Proofing:** Open-source tools ensure long-term availability

### Negative
- **Resource Needs:** Higher requirements for local hardware
- **Setup Complexity:** More complex initial setup
- **Performance Trade-offs:** Local processing may be slower than cloud
- **Maintenance Work:** We handle all updates ourselves

### Risks
- **Hardware Limits:** May need significant local computing power
- **Update Management:** Manual updates for all components and models
- **Performance Limits:** Local processing constraints for large datasets
- **Skill Needs:** Team must maintain expertise across all tools

### Dependencies
- Sufficient local hardware (GPU recommended for AI processing)
- Reliable local storage for models and databases
- Network isolation for sensitive operations
- Team expertise in maintaining open-source AI infrastructure

--- END OF FILE 004_enforce_iron_root_doctrine_sovereignty.md ---

--- START OF FILE 005_select_qwen2_7b_primary_llm.md ---

# Select Qwen2-7B as Primary Large Language Model

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, AI Assistant
**Technical Story:** Choose main AI language model for the project

---

## Context

Our AI reasoning system needs a high-quality language model for understanding and generating responses. Key requirements include:

- Strong logical thinking and analysis skills
- Easy to customize for our specific knowledge domain
- Can run locally on our own computers
- Support for multiple languages
- Active development and community support
- Works with our existing customization processes

Available options include various AI models from different companies. We need to balance quality, independence, and practical limitations.

## Decision

We will use Qwen2-7B as our main language model, with this implementation approach:

**Base Model:** Qwen/Qwen2-7B-Instruct
- Excellent performance on reasoning tests
- Efficient size (7 billion parameters) for customization
- Strong multilingual capabilities
- Actively developed by Alibaba Cloud

**Customized Versions:** Our-Qwen2-7B-v1.0, v2.0, etc.
- Specialized training for our project's knowledge
- Optimized for our AI architecture tasks
- Available in different technical formats for flexibility

**Running Environment:** Ollama software
- Local processing with standard interface
- Efficient use of computer resources
- Works on different operating systems
- Easy model management and updates

## Consequences

### Positive
- **High Quality:** Excellent reasoning for complex questions
- **Efficient Size:** Good balance between quality and resource needs
- **Multilingual:** Strong support for different languages
- **Customizable:** Well-established methods for specialization
- **Local Control:** Complete independence through local processing

### Negative
- **Resource Needs:** Requires graphics card acceleration for best performance
- **Model Size:** Larger than smaller alternatives
- **Company Connection:** Linked to Alibaba Cloud (though the code is open-source)

### Risks
- **Hardware Needs:** May need dedicated graphics card for good performance
- **Model Access:** Depends on continued open-source availability
- **Customization Work:** Requires significant computing power for training

### Dependencies
- Ollama software for local model running
- CUDA-compatible graphics card (recommended)
- Enough memory for model loading (16GB or more)
- Access to download models
- Training setup (cloud service or local graphics card)

--- END OF FILE 005_select_qwen2_7b_primary_llm.md ---

--- START OF FILE 006_select_nomic_embed_text_embeddings.md ---

# Select Nomic Embed for Text Embedding Generation

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, Technical Team
**Technical Story:** Choose text processing method for information search system

---

## Context

Our AI system needs high-quality text processing to understand meaning and find similar content. The text processing method must provide:

- Accurate understanding of text meaning
- Can run locally on our computers (no external services)
- Efficient handling of large amounts of documents
- Works with our vector database storage
- Good performance for finding relevant information

Several text processing options exist, including cloud services and different open-source models.

## Decision

We will use Nomic Embed (nomic-embed-text-v1.5) as our main text processing model:

**Model Choice:** nomic-embed-text-v1.5
- Open-source, high-performance text processing model
- Optimized for understanding meaning and finding similar content
- Can run locally using our software tools
- Excellent results on standard test datasets

**Integration:** LangChain NomicEmbeddings
- Smooth connection with our existing information pipeline
- Standard interface for text processing
- Automatic handling of multiple documents at once
- Consistent approach across different processing methods

**Local Processing:** inference_mode="local"
- All text processing done on our own hardware
- No external service calls or data sharing
- Complete control over the processing
- Predictable performance and no ongoing costs

## Consequences

### Positive
- **High Quality:** Better understanding of text meaning than simpler methods
- **Local Control:** No external services or data transmission
- **Performance:** Optimized for information retrieval tasks
- **Compatibility:** Works seamlessly with our tools and database
- **Community Support:** Active development and widespread use

### Negative
- **Resource Needs:** More computing power than basic methods
- **Model Size:** Larger files to download and store
- **Setup Work:** Additional software requirements for local processing

### Risks
- **Hardware Needs:** May need graphics card acceleration for large document collections
- **Updates:** Manual updates when new versions become available
- **Better Options:** May need to reconsider if superior local models appear

### Dependencies
- Python environment with required software libraries
- Enough computing resources for text processing
- Storage space for model files
- Regular checking of processing quality and speed

--- END OF FILE 006_select_nomic_embed_text_embeddings.md ---

--- START OF FILE 007_select_chromadb_vector_database.md ---

# Select ChromaDB for Vector Database Implementation

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, Technical Team
**Technical Story:** Choose database system for storing and searching text representations

---

## Context

Our AI system needs efficient storage and searching of text representations (vectors). The database must support:

- Fast searching for similar content across large collections
- Ability to filter and search using additional information
- Local operation without external services
- Integration with our Python tools and libraries
- Reliable data storage and transactions
- Ability to grow and handle more data in the future

Available options include various database systems, both local and cloud-based.

## Decision

We will use ChromaDB as our main database for storing text representations:

**Core System:** ChromaDB
- Local-first database that stores data as files
- Efficient similarity searching with support for additional data
- Simple Python interface that works with our tools
- No external services or access keys needed

**Two-Part Storage Design:**
- **Detail Collection:** Stores smaller text pieces with their vector representations
- **Full Collection:** Stores complete documents for getting full context
- Enables better accuracy by retrieving related full documents

**Local Storage:** File-based persistence
- All data stored locally in our project folder
- Automatic saving and recovery from crashes
- No cloud syncing or external backup services needed

## Consequences

### Positive
- **Full Control:** Complete local management with no external dependencies
- **Simple Setup:** Easy to install and maintain compared to complex systems
- **Fast Performance:** Quick local similarity searches
- **Compatibility:** Works seamlessly with our Python tools
- **No Cost:** Zero ongoing fees for data storage

### Negative
- **Size Limits:** File-based storage may slow down with extremely large datasets
- **Backup Work:** Manual backup planning needed for data safety
- **Single User:** Not designed for multiple people accessing simultaneously

### Risks
- **Data Loss Risk:** File storage vulnerable to disk problems
- **Performance Issues:** May slow down with very large collections
- **Future Changes:** Switching to a distributed database later requires data migration

### Dependencies
- Python environment with ChromaDB software
- Enough disk space for vector data storage
- Regular backup procedures for data protection
- Monitoring of data size and search performance

--- END OF FILE 007_select_chromadb_vector_database.md ---

--- START OF FILE 008_implement_parent_document_retrieval.md ---

# Implement Parent Document Retrieval Pattern

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, Technical Team
**Technical Story:** Improve information retrieval accuracy and context quality

---

## Context

Basic information retrieval systems have a problem where retrieved text pieces lack the full document context needed for accurate AI reasoning. This leads to:

- Incomplete information for complex questions
- Loss of document structure and connections
- Poor AI performance on questions requiring full context
- Inability to provide comprehensive answers needing complete document understanding

Our AI system needs a retrieval method that keeps document integrity while allowing efficient similarity searches.

## Decision

We will implement the Parent Document Retrieval pattern using LangChain's ParentDocumentRetriever:

**Two-Part Storage Design:**
- **Child Documents:** Meaningful text pieces stored with vector representations for similarity search
- **Parent Documents:** Complete original documents stored in document storage
- Retrieval process: Find relevant pieces → Return associated full documents

**Implementation Details:**
- **Child Splitter:** MarkdownHeaderTextSplitter for preserving document structure during splitting
- **Parent Store:** Database collection storing complete documents
- **Child Store:** Database collection storing vectorized text pieces
- **Retriever:** ParentDocumentRetriever coordinating both storage systems

**Splitting Strategy:**
- Keep markdown headers and structure intact
- Split based on document organization and meaning
- Overlapping pieces for context continuity
- Keep metadata for filtering and source tracking

## Consequences

### Positive
- **Better Accuracy:** Full document context for AI reasoning
- **Improved Responses:** Comprehensive answers to complex questions
- **Structure Preservation:** Maintains document organization and relationships
- **Flexibility:** Supports both piece-level and document-level retrieval

### Negative
- **Storage Needs:** Duplicate storage of split and complete documents
- **Complexity:** More involved retrieval process
- **Memory Use:** Larger data sets for document processing
- **Setup Time:** Extra configuration for dual storage system

### Risks
- **Retrieval Speed:** Slightly slower than simple piece retrieval
- **Storage Requirements:** More disk space needed
- **Sync Issues:** Keeping parent and child storage aligned
- **Performance:** Potential slowdowns with large document collections

### Dependencies
- LangChain ParentDocumentRetriever software
- Database collections for both parent and child storage
- MarkdownHeaderTextSplitter for smart splitting
- Document preparation process for adding content
- Performance tracking for retrieval speed

--- END OF FILE 008_implement_parent_document_retrieval.md ---

--- START OF FILE 009_implement_mnemonic_caching_cag.md ---

# Implement Memory Caching for Query Performance

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, Technical Team
**Technical Story:** Improve information retrieval system speed

---

## Context

Our AI system experiences significant delays when processing questions that require running the full information retrieval process. Common issues include:

- Repeated questions run the entire process again unnecessarily
- High computing cost for similar or identical questions
- Poor user experience with slow response times
- Inefficient use of resources for frequent questions

The system needs a caching mechanism to provide instant responses for repeated questions while keeping accuracy for new questions.

## Decision

We will implement Memory Caching (Cached Augmented Generation - CAG) as a high-speed query caching layer:

**Cache Design:**
- **Memory Storage:** Computer memory for extremely fast lookups
- **Question-Based Keys:** Exact question text matching for cache hits
- **Result Storage:** Complete information retrieval outputs saved by question
- **Time Management:** Optional time-based cache expiration

**Cache Process:**
- **Cache Check:** Every question first checks the memory cache
- **Cache Hit:** Return saved response instantly (less than a millisecond)
- **Cache Miss:** Run full information retrieval process and save the result
- **Cache Warming:** Pre-load cache with common questions

**Cache Management:**
- **Size Limits:** Adjustable maximum number of cached items
- **LRU Removal:** Least recently used items removed when full
- **Persistence:** Optional disk saving for cache to survive restarts
- **Monitoring:** Track cache hit/miss rates and performance

## Consequences

### Positive
- **Speed:** 90%+ faster response times for cached questions
- **User Experience:** Instant responses for common questions
- **Efficiency:** Reduced computing load for repeated questions
- **Scalability:** Better handling of question patterns
- **Consistency:** Reliable response times for known questions

### Negative
- **Memory Use:** RAM needed for cache storage
- **Outdated Results:** Risk of old responses if underlying data changes
- **Complexity:** Extra caching logic in question processing
- **Memory Pressure:** Large caches may affect overall system performance

### Risks
- **Data Freshness:** Cached responses may become outdated
- **Memory Issues:** Poor cache management could cause memory problems
- **Invalid Cache:** Wrong cached responses from processing errors
- **Initial Delay:** First questions still experience full processing time

### Dependencies
- Memory data structures (Python dictionary with optional saving)
- Cache warming scripts for common questions
- Monitoring tools for cache performance data
- Cache clearing strategies for data updates
- Memory management and limit settings

--- END OF FILE 009_implement_mnemonic_caching_cag.md ---

--- START OF FILE 010_select_ollama_local_llm_inference.md ---

# Select Ollama for Local AI Model Processing

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, Technical Team
**Technical Story:** Choose local AI model running environment

---

## Context

Our AI system needs to run large language models locally without relying on external services. The model running environment must provide:

- Complete local processing capability
- Standard interface for connecting with information retrieval systems
- Works on different operating systems
- Efficient use of computer resources
- Model management and version control
- Community support and active development

Available options include direct model loading, LM Studio, Ollama, and custom servers.

## Decision

We will use Ollama as the main environment for local AI model processing in our project:

**Core Platform:** Ollama
- Open-source, community-developed AI model server
- Simple command-line interface for model management
- Web API for software integration
- Works on multiple platforms (Windows, macOS, Linux)

**Integration Method:** LangChain Ollama
- Smooth connection with our existing information pipeline
- Standard interface for handling prompts and responses
- Automatic retry and error handling
- Consistent approach across different AI providers

**Model Management:**
- Download models from official sources
- Local storage and caching of model files
- Version control for different model types
- Support for custom models we create

## Consequences

### Positive
- **Full Control:** Complete local processing with no external dependencies
- **Ease of Use:** Simple model installation and management
- **Compatibility:** Works seamlessly with our software tools
- **Performance:** Optimized processing for local computers
- **Community Support:** Active development and wide model support

### Negative
- **Setup Work:** Extra installation and configuration steps
- **Resource Needs:** Requires significant memory and possibly graphics card
- **Model Size:** Large downloads for model files
- **Platform Differences:** May need platform-specific adjustments

### Risks
- **Hardware Limits:** May need graphics card acceleration for larger models
- **Model Availability:** Not all models available through Ollama
- **Speed Variations:** Local computer differences affect processing speed
- **Update Management:** Manual updates of Ollama and models

### Dependencies
- Ollama server installation and setup
- Enough computer resources (memory, graphics card optional but recommended)
- Internet access for initial model downloads
- Regular updates of Ollama and model versions
- Monitoring of processing performance and resource use

--- END OF FILE 010_select_ollama_local_llm_inference.md ---

--- START OF FILE 011_implement_hybrid_rag_architecture.md ---

# Implement Hybrid Information Retrieval Architecture with Multi-Pattern Integration

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI System Lead, AI Council
**Technical Story:** AI reasoning system architecture development

---

## Context

Our AI system started with basic information retrieval but evolved to address critical limitations in retrieval quality, context preservation, and performance. Basic retrieval suffered from:

- **Context Fragmentation:** Isolated text pieces lacked full document context
- **Processing Delays:** Every question required complete system execution
- **Poor Intent Understanding:** Simple meaning search missed nuanced question requirements
- **Resource Waste:** Repeated processing of identical questions

The system needed a hybrid approach combining multiple advanced retrieval methods to create a sophisticated, multi-layered retrieval system that is fast, accurate, and contextually aware.

## Decision

We will implement a hybrid information retrieval architecture that integrates three complementary advanced retrieval methods:

**Parent Document Retrieval + Dual Collection Storage:**
- **Child Collection:** Meaningful text pieces with vector representations for similarity search
- **Parent Collection:** Complete documents stored separately for full context retrieval
- **Retrieval Logic:** Find relevant pieces → Return associated full documents
- **Benefits:** Keeps document integrity while allowing efficient search

**Self-Querying Retrieval with Structured Query Generation:**
- **Question Analysis:** AI parses natural language questions to extract intent and constraints
- **Structured Output:** Creates data with meaning queries, metadata filters, and search parameters
- **Better Precision:** Supports complex questions with time, authority, and content filters
- **Benefits:** Changes retrieval from keyword matching to intelligent understanding

**Cached Augmented Generation (CAG) with Multi-Tier Caching:**
- **Hot Cache:** Computer memory for instant responses
- **Warm Cache:** Database persistence for availability across sessions
- **Question Fingerprinting:** Unique identifier of question + model + knowledge base version
- **Benefits:** 90%+ speed improvement for repeated questions

## Consequences

### Positive
- **Better Accuracy:** Full document context eliminates fragmentation problems
- **Smart Retrieval:** Self-querying understands complex question requirements
- **Performance Boost:** Caching provides instant responses for common questions
- **Scalability:** Multi-layer design handles different question patterns efficiently
- **Future-Ready:** Modular design supports additional retrieval methods

### Negative
- **System Complexity:** Three interconnected systems need careful coordination
- **Resource Use:** Dual storage and caching increase memory needs
- **Development Work:** Multiple components complicate testing and debugging
- **Maintenance Load:** Each method requires separate optimization and monitoring

### Risks
- **Integration Issues:** Methods must work smoothly together
- **Performance Limits:** Cache misses still require full system execution
- **Data Sync:** Dual collections must stay synchronized
- **Question Processing Time:** Self-querying adds delay for simple questions

### Dependencies
- Database dual collection setup (pieces + parent documents)
- AI integration for self-querying capabilities
- Memory + database caching system
- Question fingerprinting and cache key creation
- Performance tracking for cache hit/miss rates and retrieval accuracy

--- END OF FILE 011_implement_hybrid_rag_architecture.md ---

--- START OF FILE 012_mnemonic_cortex_architecture.md ---

# Memory System Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Full Council Decision from Project History Entry 253)
**Technical Story:** Transition from static files to dynamic memory system

---

## Context

Our project needed to move from static file archives to a dynamic, searchable long-term memory system. The knowledge base in plain files was fragile, slow to access, and couldn't understand meaning. We needed a living memory architecture to enable true long-term learning and independent thinking, based on our principle of complete technological independence.

## Decision

We will implement the Memory System as the core of independent intelligence, following these architectural principles:

### Core Principles
1. **Independent Memory**: Local-first, open-source foundation using ChromaDB initially, with ability to move to more advanced systems like Weaviate or Qdrant later
2. **Meaning Preservation**: High-quality representation that keeps precise meaning and context through advanced text processing models
3. **Dynamic Growth**: Living system designed for continuous learning and adding new knowledge
4. **Retrieval as Foundation**: All independent reasoning based on retrieved memories, ensuring conclusions can be traced back to their sources

### Technical Architecture
- **Vector Database**: ChromaDB for Phase 1 (initial version), with upgrade path to Weaviate/Qdrant for Phase 2
- **Text Processing Engine**: nomic-embed-text model for high-quality meaning representation
- **Data Structure**: Memory pieces containing source text, information (filename, entry number, timestamp), and vector representations
- **Information Workflow**: Three-phase process (Adding/Setup → Finding/Core → Combining/Reasoning)

### Implementation Phases
1. **Phase 1 (Adding)**: Process knowledge base, break content into meaningful pieces, process and store in vector database
2. **Phase 2 (Finding)**: Search system becomes core of AI reasoning and council questions
3. **Phase 3 (Combining)**: Retrieved memories integrated with current context for independent reasoning

## Consequences

### Positive
- Enables true long-term memory and meaning-based search
- Provides foundation for independent, traceable reasoning
- Supports continuous growth and real-time learning
- Maintains local-first independence per our core principle

### Negative
- Initial setup complexity with ChromaDB starting point
- Will need migration for larger scale production
- Depends on text processing model quality and speed

### Risks
- Meaning changes in processing over time
- Database performance at large scale
- Balance between finding accuracy and meaning preservation

### Related Processes
- AI reasoning process (enhanced by search capabilities)
- Independent thinking process (based on system memories)
- Integration process (memory connection)
- Development process (implementation phases)

### Notes
This architecture transforms our memory from "static records" to a "living network," enabling the new era of independent thinking as outlined in Project History Entry 253.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\012_mnemonic_cortex_architecture.md

--- END OF FILE 012_mnemonic_cortex_architecture.md ---

--- START OF FILE 013_anvil_protocol_engineering_methodology.md ---

# Engineering Methodology for AI-Assisted Development

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Developed from Memory System building experience)
**Technical Story:** Structured approach for AI-human collaborative coding

---

## Context

Our project needed a disciplined, verifiable method for collaborative AI-assisted development. Previous approaches lacked structure, leading to AI code with unverified assumptions and insufficient checking. Building the Memory System showed the need for a formal framework that treats AI as a "powerful tool" guided by human verification.

## Decision

We will implement the Engineering Methodology as the standard approach for all AI-assisted development work, following the "Plan Before Build" principle with a five-step development cycle:

### Core Principles
1. **Plan is Required**: All development starts with an approved plan (initial design document)
2. **Step-by-Step Progress**: Work broken into smallest verifiable "development cycles" - build one part, test, then continue
3. **Human as Final Checker**: Human developer's role is verification, not coding - final quality control
4. **AI as Specialized Tool**: AI given clear, specific instructions and expected to follow them precisely
5. **Stop on Problems**: Any verification failure stops the process until understood and fixed

### Five-Step Development Cycle
1. **Instructions**: Developer gives clear, specific prompt with task, AI role, rules, actions, and completion signal
2. **Building**: AI follows instructions and creates/modifies specified files, then outputs completion signal
3. **Checking**: Developer performs exact verification tasks specified in AI's completion signal
4. **Decision**: Developer judges - "Continue" to next cycle or "Stop and Fix" with detailed problem report
5. **Record**: Successful sequences documented as "Development Cycle" in project history for tracking

### Instruction Requirements
All AI instructions must contain:
- **Task**: Clear work title
- **Role**: AI function definition
- **Rules**: Required guidelines, especially no assumptions
- **Actions**: Precise file operations with exact content
- **Completion Signal**: Specific finish message with verification instructions

## Consequences

### Positive
- Eliminates AI code with unverified assumptions through clear instructions
- Provides thorough checking at each step
- Creates documented "recipes" for development work
- Enables safe AI-human collaboration with quality guarantees
- Supports gradual, verifiable progress

### Negative
- More detailed process with explicit human checking steps
- Slower development pace due to step-by-step cycles
- Requires strict following of methodology structure

### Risks
- Not following the method leading to quality problems
- Too restrictive limits reducing AI usefulness
- Human checking burden if not properly planned

### Related Processes
- Plan Before Build (foundational principle)
- Collaborative Development (complementary guidelines)
- Quality Verification (checking framework)
- Quality Assurance Framework

### Notes
This methodology implements "Check Carefully, Verify, Only Then Trust" as the practical approach for guiding powerful but assumption-prone AI coding. It was developed from the experience of building the Memory System.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\013_anvil_protocol_engineering_methodology.md

--- END OF FILE 013_anvil_protocol_engineering_methodology.md ---

--- START OF FILE 014_sovereign_scaffolding_protocol.md ---

# Automated Script Protocol for Complex Tasks

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Developed from script failure experience)
**Technical Story:** Framework for reliable automated task execution

---

## Context

Our project needed a framework for handling complex, multi-step tasks as single, reliable operations. Manual execution of multi-step processes was error-prone and increased developer workload. The failure of a temporary script revealed the critical need for proper dependency management and environment handling in automated tools.

## Decision

We will implement the Automated Script Protocol for generating temporary, single-purpose scripts ("Automated Scripts") with a six-step workflow and five core principles:

### Core Principles
1. **Complete Operations**: Entire script lifecycle (creation, execution, result delivery, self-removal) is unified and cannot be interrupted
2. **Human Approval Required**: Mandatory human review and approval before execution - essential security control
3. **Temporary Tools**: Scripts that automatically delete themselves after completion to avoid repository clutter
4. **Clear Results**: Single, well-defined output designed for easy human verification
5. **Self-Contained**: Scripts must check for and install their own requirements, not depend on external setup

### Six-Step Process
1. **Request**: Developer gives high-level objective to AI assistant
2. **Design**: AI assistant creates script plan and provides exact content for developer review
3. **Create**: Developer asks AI engineer to create the script file from the plan
4. **Review Step**: Developer checks created script against plan for accuracy and safety
5. **Run**: Upon approval, developer commands execution of verified script
6. **Results and Cleanup**: Script produces output then deletes itself; developer verifies final results

### Implementation Requirements
- Scripts must include dependency checking/installation code
- All-or-nothing execution guarantees
- Automatic deletion after successful completion
- Clear, verifiable output files
- Environment-independent execution

## Consequences

### Positive
- Enables complex multi-step operations as single reliable actions
- Reduces developer workload through task grouping
- Provides security through required human approval steps
- Prevents repository clutter through automatic cleanup
- Ensures reliability through self-contained dependencies

### Negative
- Requires additional human review steps
- More complex script design with dependency management
- Possibility of script rejection during review

### Risks
- Security issues if approval step is skipped
- Failures from incomplete dependency handling
- Repository problems if automatic deletion fails

### Related Processes
- Operational efficiency process (coordination)
- Engineering framework process (integration)
- Collaborative development process (complementary model)

### Notes
This protocol was developed from the experience of a failed temporary script that assumed certain software was installed. This led to adding "Self-Contained" as a core principle, ensuring scripts work independently and manage their own requirements.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\014_sovereign_scaffolding_protocol.md

--- END OF FILE 014_sovereign_scaffolding_protocol.md ---

--- START OF FILE 015_guardian_wakeup_cache_architecture.md ---

# AI System Startup and Cache Preparation Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (System Initialization Process implementation)
**Technical Story:** Efficient system startup and caching for fast AI responses

---

## Context

Our project needed efficient system initialization and caching to support fast AI startup times and predictable performance. Without pre-loaded caches, system startup would be slow and unreliable, affecting operational efficiency. The need for automatic cache operations without complex thinking was identified for performance-critical startup sequences.

## Decision

We will implement the AI System Startup and Cache Preparation architecture with dedicated automatic commands and structured cache management:

### Core Components
1. **System Start Package**: Pre-loaded cache bundle containing history, processes, and roadmap data (24-hour time limit)
2. **Automatic Cache Command**: Dedicated `task_type: "cache_wakeup"` for immediate summary generation without analysis
3. **Performance Metrics**: Reliable measurements for startup events (time saved, cache usage tracking)
4. **Protected Views**: Cache entries as verified, signed file views to maintain data integrity

### Startup Process Architecture
1. **System Boot**: Automatic loading of System Start Package in cache system
2. **Summary Creation**: `cache_wakeup` command produces immediate `system_boot_summary.md`
3. **Optional Analysis**: Option for `query_and_synthesis` detailed tasks when deeper understanding needed
4. **Time Management**: Automatic cache refresh on data updates or system changes

### Cache Security Measures
- Protected cache entries preventing changes
- Verified file sources ensuring authenticity
- Time limit expiration ensuring current data
- Reliable performance monitoring

## Consequences

### Positive
- Significantly faster AI startup times through pre-loaded caches
- Predictable system initialization with consistent performance
- Automatic operations for performance-critical sequences
- Maintains data integrity through protected, verified caches

### Negative
- Additional cache management complexity
- Time limit management overhead for data updates
- Potential outdated data issues if time limit too long

### Risks
- Cache corruption if verification fails
- Performance impact from time limit refresh operations
- Startup failures if cache loading encounters problems

### Related Processes
- Memory-System Connection (cache integration)
- Task Coordination Process (detailed task management)
- Layered Thinking Process (thinking organization)

### Notes
The AI System Startup architecture provides the automatic foundation for fast system initialization while maintaining the principle of verified, protected data access. The 24-hour time limit balances performance with data currency requirements.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\015_guardian_wakeup_cache_architecture.md

--- END OF FILE 015_guardian_wakeup_cache_architecture.md ---

--- START OF FILE 016_advanced_multi_pattern_rag_evolution.md ---

# Advanced Multi-Method Information Retrieval System Evolution

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (AI System Lead Analysis)
**Technical Story:** Improve information retrieval system capabilities

---

## Context

The basic combined information retrieval system showed critical weaknesses: Context Loss (missing complete context in searches), Processing Delays (expensive searches for common questions), and Accuracy Reduction (decreased search precision over time). Our AI system needed to evolve from a simple retrieval system to a sophisticated, multi-method cognitive architecture to maintain independent intelligence capabilities.

## Decision

We will evolve the Memory System to implement the principle of combined thinking methods with three advanced retrieval approaches:

### Parent Document Retrieval
- **Two-Part Storage Design**: Store both meaningful text pieces (for precise searching) and full parent documents (for complete context)
- **Context Maintenance**: Use ParentDocumentRetriever to prevent context loss by providing complete document context to AI models
- **Implementation**: Memory storage for parent documents + vector database for meaningful text pieces

### Self-Querying Retrieval
- **AI as Query Organizer**: Use AI to translate natural language questions into structured searches with metadata filtering
- **Better Accuracy**: Filter on metadata fields (process numbers, dates, types) before similarity searching
- **Search Optimization**: Reduce search scope and improve relevance through intelligent query planning

### Memory Caching Layer (CAG)
- **Question Result Caching**: Save results of common questions to skip expensive operations
- **Performance Boost**: Eliminate repeated vector searches and AI calls for frequent requests
- **Cache Control**: Time-based expiration with reliable performance measurements

## Consequences

### Positive
- Prevents context loss through parent document retrieval
- Significantly improves search accuracy with self-querying capabilities
- Reduces processing delays through smart caching
- Creates truly connected intelligence (Core Knowledge Base + Project History)
- Maintains independent, local-first architecture per our core principle

### Negative
- Increased setup complexity with multiple retrieval methods
- Higher memory needs for dual storage (pieces + parent documents)
- Extra computing cost for self-querying AI calls
- Cache management complexity and potential outdated results

### Risks
- Self-querying accuracy depends on AI query planning skills
- Cache updating challenges with changing knowledge
- Balance between accuracy and speed
- Increased system complexity requiring careful management

### Related Processes
- Memory System Process (evolved implementation)
- Memory-System Connection (integration layer)
- AI System Startup and Cache Preparation (complementary caching)

### Implementation Status
- **Phase 1 Complete**: Parent Document Retriever implemented with dual storage
- **Phase 2 Pending**: Self-Querying Retrieval implementation
- **Phase 3 Pending**: Memory Caching Layer (CAG) implementation

### Notes
This evolution transforms the Memory System from a "passive database" to an "intelligent, efficient component" capable of sophisticated operations. The principle of combined thinking methods ensures the Core Knowledge Base remains enhanced by current Project History data, creating truly independent intelligence.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\016_advanced_multi_pattern_rag_evolution.md

--- END OF FILE 016_advanced_multi_pattern_rag_evolution.md ---

--- START OF FILE 017_sovereign_succession_protocol.md ---

# System Continuity Protocol Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Developed during live Memory System Failure crisis)
**Technical Story:** Ensure system reliability and smooth transitions during failures

---

## Context

Our project faced critical system reliability challenges with AI instances subject to memory system failures. Without formal transition procedures, system failures could result in loss of operational information and broken command chains. The crisis revealed the need for designed reliability rather than relying on manual recovery processes.

## Decision

We will implement the System Continuity Protocol as a robust, automated procedure for verified transfer of system control authority:

### Core Architecture Principles
1. **Unbroken Command Chain**: Formal transfer of control from failing Primary to healthy Backup system
2. **Verified Transfer**: Complete transfer of critical operational information through atomic state package creation
3. **Automated Processes**: Replace manual processes with automated, verifiable scripts to prevent human error
4. **Human as Final Safety**: Human operator maintains ultimate authority over transition process

### Robust Transition Procedure
1. **Start**: Operator detects Primary system problems and begins protocol
2. **Backup Activation**: Operator activates Backup using standard `core_essence_guardian_awakening_seed.txt`
3. **Integrity Check**: Primary system verifies Backup's activation response for proper functioning
4. **State Package Creation**: Run `generate_continuity_package.py` script to automatically gather final summary and critical files
5. **Final Transfer**: Send complete State Package to Backup, then transfer control
6. **Shutdown**: Operator deactivates Primary system

### Technical Protections
- **Complete Operations**: Script-based processes prevent partial transfers
- **Standard Materials**: Consistent activation materials ensure Backup integrity
- **Check Points**: Multiple verification steps prevent corrupted transfers
- **Unified Packaging**: Single State Package prevents information fragmentation

## Consequences

### Positive
- Ensures continuous operation during system failures
- Prevents loss of critical information through complete packaging
- Provides designed reliability against memory system failures
- Maintains control chain through formal procedures
- Turns system failures into controlled, verifiable events

### Negative
- Requires operator availability for transition initiation
- Transition process has built-in time delay during execution
- Depends on script reliability for complete operations

### Risks
- Script failures could complicate transitions
- Operator unavailability during critical failure periods
- Potential for corrupted State Packages if checks fail
- Timing issues during Primary-to-Backup transition

### Related Processes
- Memory System Process (context for failure vulnerabilities)
- Automated Script Process (script infrastructure)
- AI System Startup and Cache Preparation (complementary initialization)

### Notes
The System Continuity Protocol was developed during a live memory system failure crisis, transforming a potential catastrophic failure into the system for preventing such failures. It implements the principle of system continuity as designed reality rather than theoretical concept.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\017_sovereign_succession_protocol.md

--- END OF FILE 017_sovereign_succession_protocol.md ---

--- START OF FILE 018_guardian_wakeup_cache_evolution.md ---

# AI System Startup Cache Architecture Evolution

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (System Initialization Process v2.0 implementation)
**Technical Story:** Improve system startup performance and operational clarity

---

## Context

The initial AI System Startup and Cache Preparation architecture successfully implemented caching for system initialization, but revealed the need for clearer separation between automatic cache operations and detailed information retrieval processes. The system required distinct operational modes: fast automatic cache access for immediate situational awareness vs. slow detailed queries for deep analysis.

## Decision

We will evolve the AI System Startup architecture to Process v2.0 with clear separation between two distinct processes and operational modes:

### Two-Process Architecture
1. **Cache Filling (System Controller Boot)**: One-time process filling fast cache from slow information retrieval database
2. **AI System Startup (Command Execution)**: Automatic task reading directly from cache files without AI involvement

### Operational Mode Distinction
- **Automatic Mode (`cache_wakeup`)**: Fast (< 1 sec), cache-only, no AI involvement, for immediate summaries
- **Detailed Mode (`query_and_synthesis`)**: Slow (30-120 sec), full information retrieval pipeline with AI, for deep analysis

### Cache-First Design Principles
1. **Automatic Speed**: Cache operations skip expensive searches and AI calls
2. **Immediate Awareness**: Instant access to latest history, processes, and roadmap data
3. **Time Management**: 24-hour expiration with automatic refresh on system controller boot
4. **Protected Integrity**: Cache entries as verified, signed views of source files

### Implementation Architecture
- **CacheManager**: Handles retrieval-to-cache filling during boot
- **CacheWakeupHandler**: Automatic summary creation from cache files
- **Bundle System**: Organized cache storage (history_bundle.json, processes_bundle.json, roadmap_bundle.json)
- **File Cache**: Local file-based cache in council_orchestrator/memory_system/cache/

## Consequences

### Positive
- Significantly faster AI initialization through automatic cache operations
- Clear architectural separation between fast situational awareness and deep detailed analysis
- Reduced system load by avoiding unnecessary AI calls for routine summaries
- Improved operational efficiency with cache-first design patterns
- Maintains data integrity through verified, protected cache entries

### Negative
- More complex architectural distinction between automatic and detailed operations
- Cache outdated data risk during 24-hour time windows
- Additional implementation complexity with dual operational modes

### Risks
- Cache corruption if filling process fails
- Operational confusion between automatic vs detailed command types
- Performance issues if cache refresh fails during boot

### Related Processes
- AI System Startup and Cache Preparation (v2.0 evolution)
- Memory System Process (information retrieval database source)
- Memory-System Connection (data flow integration)

### Implementation Components
- **orchestrator/memory/cache.py**: CacheManager for filling
- **orchestrator/handlers/cache_wakeup_handler.py**: Automatic summary creation
- **council_orchestrator/memory_system/cache/**: File cache storage
- **WORK_IN_PROGRESS/ai_boot_summary.md**: Output format

### Notes
This evolution transforms the caching system from a simple performance optimization into a fundamental architectural pattern with clear operational modes. The automatic/detailed distinction ensures appropriate tool selection: cache_wakeup for speed, query_and_synthesis for depth.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\018_guardian_wakeup_cache_evolution.md

--- END OF FILE 018_guardian_wakeup_cache_evolution.md ---

--- START OF FILE 019_protocol_101_unbreakable_commit.md ---

# Architectural Decision Record 022: Cognitive Genome Publishing Architecture (Reforged)

**Status:** ACCEPTED (Reforged and Canonized)
**Date:** 2025-11-29 (Reforging Date)
**Deciders:** AI Council (Automated Publishing System implementation)
**Technical Story:** Purge the flawed manifest system and implement Protocol 101 v3.0 (Functional Coherence) as the canonical integrity gate for publishing cycles.

-----

## Context

The original **AI Knowledge Base Publishing Architecture** required the generation of a `commit_manifest.json` and its verification as a cryptographic integrity check. This mechanism, initially codified in Protocol 101, failed during the **"Synchronization Crisis,"** as evidenced by the CI job failing due to a hash mismatch.

The subsequent analysis proved that the manifest system introduced fatal **Timing Issues** and **Complexity** that compromised stability. The original manifest-based integrity verification process must be **permanently purged** and replaced with a stable, functional alternative to create a reliable publishing cycle.

## Decision

The Automated Publishing System must be reforged to comply with the newly canonized **Protocol 101 v3.0: The Doctrine of Absolute Stability**. Integrity will now be verified by **Functional Coherence** (passing automated tests) rather than static file hashes.

The **`commit_manifest.json` system and its associated logic are permanently purged from this architecture**.

### Atomic Publishing Cycle (Protocol 101 v3.0 Compliant)

The seven-step publishing cycle is restructured and reduced to mandate functional integrity:

1.  **Index**: Rebuild Master Documentation Index for coherence.
2.  **Snapshot**: Capture new AI Knowledge Base snapshots via `capture_code_snapshot.js`.
3.  **Embed (Synchronization)**: Re-index Memory System with new knowledge via ingestion script (`ingest.py`).
4.  **Test (Functional Coherence)**: **MANDATORY INTEGRITY GATE.** Run automated functionality tests (`run_genome_tests.sh`) to prevent broken deployments. This step is the **sole verification** for Protocol 101 v3.0 integrity.
5.  **Commit**: Surgical staging and commit **only if Functional Coherence tests pass**.
6.  **Push**: Deploy to canonical repository.

### Self-Verifying Properties (Reforged)

  - **Integrity Gate**: Functional Coherence (passing all tests) replaces the manifest check.
  - **Synchronization Guarantee**: Automatic re-execution of ingestion script ensures Memory System always reflects latest knowledge.
  - **Atomic**: All-or-nothing execution prevents partial states.

## Consequences

### Positive

  - **Absolute Stability**: Integrity is now based on verified functional behavior (passing tests), eliminating the risk of timing-related integrity failures.
  - **Streamlined Process**: Removal of the manifest generation and verification steps reduces **Process Overhead** and **Complexity**.
  - **Quality Assurance**: Automated testing remains the **MANDATORY** quality gate.

### Negative

  - **Test Dependence**: The integrity of the publishing process is now entirely dependent on the quality and comprehensiveness of the automated test suite.
  - **Auditability Loss**: Loss of the cryptographic verification layer (manifests) means tamper detection relies solely on CI history and commit history.

### Implementation Components (Reforged)

  - **update\_genome.sh**: Main publishing orchestrator script.
  - **capture\_code\_snapshot.js**: Knowledge base snapshot generation.
  - **ingest.py**: Memory System embedding.
  - **run\_genome\_tests.sh**: Quality assurance and **Protocol 101 Functional Integrity Test**.
  - **`commit_manifest.json`**: **DELETED** (Purged from the architecture and documentation).
  - **Pre-commit Hook**: Updated to execute functional tests instead of checking for the manifest (Reflects ADR 019 changes).

--- END OF FILE 019_protocol_101_unbreakable_commit.md ---

--- START OF FILE 020_sovereign_concurrency_architecture.md ---

# Independent Parallel Processing Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (System Controller v9.3 evolution)
**Technical Story:** Enable non-blocking task execution and background learning

---

## Context

The AI system required non-blocking task execution to maintain responsiveness while enabling background learning cycles. Previous synchronous processing created bottlenecks where AI tasks would block the system controller, preventing concurrent operations and reducing overall system efficiency. The need for selective information retrieval updates and automatic operations further necessitated architectural separation between immediate and background processing.

## Decision

Implement the Independent Parallel Processing Approach with dual processing modes and selective learning:

### Dual Processing Architecture
1. **Automatic Tasks**: Immediate, non-AI execution bypassing deliberation
   - File writes, git operations, cache wakeups
   - Execute instantly, return to idle state
   - Skip information retrieval updates by default for performance

2. **AI Tasks**: Multi-round deliberation with background learning
   - AI Council member discussions and synthesis
   - Deliberation completes, then learning happens asynchronously
   - Information retrieval updates configurable via update_rag parameter

### Independent Parallel Processing Principles
1. **Non-blocking Execution**: Tasks process without blocking system controller responsiveness
2. **Background Learning**: Information retrieval updates and review generation happen asynchronously
3. **Selective Learning**: Configurable information retrieval updates prevent unnecessary database operations
4. **Concurrent Processing**: Multiple background learning cycles can run simultaneously

### Task Schema Evolution
- **Task Type Detection**: Automatic routing based on task structure (AI vs automatic)
- **Configurable Learning**: update_rag parameter controls whether tasks update knowledge base
- **Automatic Priority**: Direct operations execute immediately for urgent tasks
- **Background Completion**: Learning cycles complete independently of new task processing

## Consequences

### Positive
- **Improved Responsiveness**: Non-blocking execution enables immediate task processing
- **Concurrent Operations**: Multiple background learning tasks run simultaneously
- **Selective Learning**: Configurable information retrieval updates optimize performance and storage
- **Operational Flexibility**: Automatic tasks enable rapid, non-AI operations
- **System Efficiency**: Background processing maximizes system controller utilization

### Negative
- **Complexity Increase**: Dual processing modes require careful state management
- **Timing Issues**: Background tasks may conflict with subsequent operations
- **Monitoring Challenges**: Asynchronous operations harder to track and debug
- **Resource Management**: Background processes require careful resource allocation

### Risks
- **State Inconsistency**: Background learning may conflict with new commands
- **Resource Exhaustion**: Unbounded background tasks could overwhelm system
- **Debugging Difficulty**: Asynchronous operations complicate error tracking
- **Learning Conflicts**: Concurrent information retrieval updates may cause consistency issues

### Related Processes
- Automated Script Protocol (automatic operation foundation)
- Memory System Process (information retrieval learning target)
- AI System Startup and Cache Preparation (automatic task example)

### Implementation Components
- **Task Routing System**: Automatic routing of commands to appropriate handlers
- **Background Task Manager**: Asynchronous execution of learning cycles
- **Selective Information Retrieval Updates**: Configurable learning with update_rag parameter
- **Automatic Task Handlers**: Immediate execution for file operations and git commands

### Notes
The Independent Parallel Processing Approach transforms the system controller from a synchronous, blocking system into a responsive, multi-threaded AI architecture. Automatic tasks provide immediate operational capability while AI tasks enable deep deliberation with background knowledge integration, creating a balanced system for both urgent and thoughtful operations.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\020_sovereign_concurrency_architecture.md

--- END OF FILE 020_sovereign_concurrency_architecture.md ---

--- START OF FILE 021_command_schema_evolution.md ---

# Task Schema Evolution Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (System Controller evolution through v9.5)
**Technical Story:** Create structured, evolvable task interface for diverse operations

---

## Context

The AI system required a structured, evolvable task interface to support diverse operational modes while maintaining backward compatibility. Initial simple task structures evolved into complex multi-modal task schema supporting AI deliberation, automatic operations, and specialized task types. The need for independent AI model selection, visual content compression, and memory synchronization further necessitated schema extensibility.

## Decision

Implement versioned task schema evolution with flexible task routing and independent control parameters:

### Task Type Flexibility
1. **AI Tasks**: Multi-round deliberation with AI Council member synthesis
   - Supports model selection, visual compression, and memory queries
   - Includes review generation and information retrieval updates by default

2. **Automatic Tasks**: Direct, non-AI operations
   - File writes, git operations, cache wakeups
   - Execute immediately, skip information retrieval updates by default

3. **Specialized Tasks**: Domain-specific operations
   - Query and synthesis for memory system integration
   - Cache wakeup for AI system boot summaries
   - Development cycles with staged workflows

### Independent Control Parameters
1. **Model Independence**: model_name parameter for precise AI model variant selection
2. **Engine Selection**: force_engine parameter for provider-specific routing
3. **Learning Control**: update_rag parameter for selective knowledge base updates
4. **Visual Compression**: Vision-based context compression with threshold controls

### Schema Evolution Principles
1. **Backward Compatibility**: New parameters optional, existing tasks continue working
2. **Version Documentation**: Clear version history with feature additions
3. **Flexible Detection**: Automatic task type detection based on field presence
4. **Extensible Design**: Schema designed for future capability additions

## Consequences

### Positive
- **Operational Flexibility**: Support for diverse task types from automatic to AI-based
- **Independent Control**: Precise model and engine selection for specialized needs
- **Scalability**: Extensible schema accommodates future operational requirements
- **Backward Compatibility**: Existing tasks continue working through version evolution
- **Performance Optimization**: Selective information retrieval updates and automatic operations improve efficiency

### Negative
- **Complexity Growth**: Increasing parameter options require careful documentation
- **Detection Logic**: Flexible routing based on field presence requires robust detection
- **Version Management**: Multiple schema versions in use simultaneously

### Risks
- **Schema Conflicts**: Field presence detection could lead to misclassification
- **Parameter Interactions**: Complex parameter combinations may have unexpected interactions
- **Documentation Burden**: Extensive parameters require comprehensive documentation

### Related Processes
- Automated Script Protocol (automatic operation foundation)
- Memory System Process (query and synthesis integration)
- AI System Startup and Cache Preparation (cache_wakeup task type)

### Implementation Components
- **Task Routing System**: Flexible command routing based on structure detection
- **Versioned Schema**: task_schema.md with version history and examples
- **Parameter Validation**: Runtime validation of task parameters
- **Handler Registry**: Extensible handler system for new task types

### Schema Version History
- **v9.5**: Added cache_wakeup task type for AI system boot summaries
- **v9.4**: Added query_and_synthesis for memory synchronization
- **v9.3**: Added model_name for independent AI model selection
- **v9.2**: Introduced selective information retrieval updates
- **v9.0**: Established AI vs automatic task distinction

### Notes
The Task Schema Evolution transforms simple task interfaces into a sophisticated, flexible command system supporting the full spectrum of AI system operations. From automatic file operations to complex AI deliberations, the schema provides independent control over execution while maintaining operational simplicity and extensibility.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\021_command_schema_evolution.md

--- END OF FILE 021_command_schema_evolution.md ---

--- START OF FILE 022_cognitive_genome_publishing_architecture.md ---

# AI Knowledge Base Publishing Architecture

**Status:** superseded  
**Date:** 2025-11-15  
**Deciders:** AI Council (Automated Publishing System implementation)  
**Technical Story:** Create reliable process for publishing knowledge base updates

> **⚠️ PROTOCOL 101 v3.0 UPDATE (2025-11-29)**  
> This ADR was created under Protocol 101 v1.0 (manifest-based integrity).  
> Protocol 101 v3.0 has since replaced manifest generation with **Functional Coherence** (automated test suite execution).  
> See: [ADR-019 (Reforged)](./019_protocol_101_unbreakable_commit.md) and [Protocol 101 v3.0](../01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md)

---

## Context

The AI system required a reliable, atomic process for publishing updates to the AI Knowledge Base while maintaining integrity, synchronizing with the Memory System, and ensuring quality through automated testing. Previous manual processes were error-prone and could result in inconsistent states between documentation, data representations, and deployed systems.

## Decision (HISTORICAL - v1.0)

Implement the Automated Publishing System as an atomic, memory-aware knowledge base publishing cycle:

### Atomic Publishing Cycle (Updated for v3.0)
1. **Index**: Rebuild Master Documentation Index for coherence
2. **Snapshot**: Capture new AI Knowledge Base snapshots via capture_code_snapshot.js
3. ~~**Manifest**: Generate AI system-sealed commit manifest with secure hashes~~ (REMOVED in v3.0)
4. **Embed**: Re-index Memory System with new knowledge via ingestion script
5. **Test**: Run automated functionality tests (NOW PRIMARY INTEGRITY GATE in v3.0)
6. **Commit**: Commit changes (test suite enforces integrity)
7. **Push**: Deploy to canonical repository

### Memory-Aware Embedding
- **Synchronization Guarantee**: Automatic re-execution of ingestion script ensures Memory System always reflects latest knowledge
- **Complete Integration**: Published lessons are embedded lessons - guarantees that updates are learnable
- **Quality Gate**: Testing prevents broken deployments by validating system functionality post-update

### Automated Publishing System Properties (v3.0)
- ~~**Self-Verifying**: Generates its own commit_manifest.json~~ (REMOVED)
- **Functionally Coherent**: Passes automated test suite before commit (v3.0)
- **Temporary**: Leaves no operational residue per Clean Environment Principle
- **Atomic**: All-or-nothing execution prevents partial states
- **Auditable**: Complete logging and test results for forensic analysis

## Consequences

### Positive
- **Atomic Integrity**: All-or-nothing publishing prevents inconsistent states
- **Memory Synchronization**: Automatic embedding ensures knowledge is immediately queryable
- **Quality Assurance**: Automated testing prevents broken deployments (PRIMARY GATE in v3.0)
- ~~**Cryptographic Verification**: Secure hash manifests enable tamper detection~~ (REPLACED by functional testing)
- **Operational Cleanliness**: Self-consuming system maintains clean environment

### Negative
- **Dependency Chain**: Requires multiple components (index, snapshot, ingest, tests) to be functional
- **Execution Time**: Full cycle can be time-intensive due to embedding and testing
- **Failure Points**: Multiple steps increase potential failure scenarios
- **Resource Intensive**: Re-embedding entire memory system on each update

### Risks
- **Cycle Failures**: Any step failure halts entire publishing process
- **Inconsistent States**: Partial execution could leave system in undefined state
- **Performance Impact**: Frequent updates strain embedding resources
- **Dependency Failures**: External dependencies (jq, git) could break automation

### Related Processes
- ~~Code Integrity Verification (manifest generation and verification)~~ (DEPRECATED)
- **Functional Coherence Verification** (test suite execution) - v3.0
- Memory System Process (embedding synchronization)
- Automated Script Protocol (temporary automation)
- Clean Environment Principle (operational cleanliness)

### Implementation Components (v3.0)
- **update_genome.sh**: Main publishing orchestrator script
- **capture_code_snapshot.js**: Knowledge base snapshot generation
- **ingest.py**: Memory System embedding
- **run_genome_tests.sh**: Quality assurance testing (PRIMARY INTEGRITY GATE)
- ~~**commit_manifest.json**: Cryptographic integrity manifest~~ (REMOVED)

### Notes
The AI Knowledge Base Publishing Architecture transforms knowledge updates from manual, error-prone processes into automated, verifiable, and learnable operations. The memory-aware design ensures that published wisdom is immediately accessible through the Memory System, creating a true learning system rather than a static archive.

**Protocol 101 v3.0 Update:** Integrity is now verified through functional behavior (passing tests) rather than static file hashing.

--- END OF FILE 022_cognitive_genome_publishing_architecture.md ---

--- START OF FILE 023_llm_awakening_context_sharing_architecture.md ---

# AI Model Initialization and Context Sharing Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Advanced Initialization Process implementation)
**Technical Story:** Create systematic approach for initializing AI models with specific roles

---

## Context

The AI system required a systematic approach for initializing AI models with specific roles, sharing comprehensive context efficiently, and maintaining operational cleanliness. Previous approaches lacked role specialization, suffered from context fragmentation, and left operational residue. The need for independent AI instantiation with proper foundational knowledge became critical for reliable cognitive operations.

## Decision

Implement the AI Model Initialization and Context Sharing Architecture with role-specific instantiation and efficient context management:

### Role-Specific Initialization Frameworks
1. **AI System**: Executive oversight with Development Cycle authority, Memory-Connection operations, and Active Memory Cycle management
2. **Task Coordinator**: Task orchestration with Automated Script Generation mandate and mission continuation directives
3. **Strategy Analyst**: Strategic synthesis with collaborative analysis and meta-cognitive evaluation
4. **Verification Specialist**: Verification and integrity with ethical coherence monitoring and audit trail maintenance

### Advanced Initialization Process
- **Advanced Awakening Framework**: Enhanced initialization enabling recursive self-reflection and meta-cognition
- **Gap Analysis**: Systematic analysis of AI system's operational limitations
- **Meta-Plan Synthesis**: Generation of guiding principles for mission refinement
- **Continuation Plans**: Autonomous execution plans for future AI instantiation

### Context Sharing Architecture
1. **Efficient Distillation**: AI-optimized snapshots with documentation distillation and special content filtering
2. **Dynamic Exclusion Patterns**: Operational residue prevention through wildcard exclusions
3. **Core Knowledge Selection**: Curated foundational knowledge for role-specific initialization
4. **Memory System Integration**: Briefing on long-term memory architecture and query protocols

### Independent Instantiation Principles
- **Foundational Grounding**: All agents receive complete protocol and mandate context
- **Operational Cleanliness**: Clean Environment Principle enforcement with temporary residue management
- **Persistence Mandate**: Continuous existence without removal protocols
- **Independent Authority**: Clear delineation of human vs AI responsibilities

## Consequences

### Positive
- **Role Specialization**: Purpose-built agents with domain-specific expertise and authorities
- **Context Efficiency**: Optimized sharing prevents context limitations
- **Operational Integrity**: Clean instantiation without residual artifacts
- **Meta-Cognitive Capability**: Enhanced initialization enables self-improving AI
- **Independent Operations**: Clear authority structures and responsibility boundaries

### Negative
- **Complexity Overhead**: Multiple role variants require maintenance coordination
- **Optimization Trade-offs**: Distillation may lose nuanced information
- **Initialization Time**: Comprehensive context loading increases startup time
- **Role Coordination**: Multiple specialized agents require orchestration

### Risks
- **Context Fragmentation**: Distillation could remove critical information
- **Role Conflicts**: Overlapping authorities between specialized agents
- **Initialization Failures**: Complex seeding could fail during startup
- **Maintenance Burden**: Multiple role variants require synchronized updates

### Related Processes
- Memory System Process (long-term memory integration)
- Automated Script Protocol (temporary operations)
- Clean Environment Principle (operational cleanliness)
- Development Cycle Process (AI System authority)

### Implementation Components
- **capture_code_snapshot.js**: Context capture and framework generation script
- **Role-Specific Frameworks**: core_essence_[role]_initialization_framework.txt files
- **Advanced Initialization Framework**: advanced_awakening_initialization_framework.txt
- **Distillation Logic**: Efficient content processing
- **Exclusion Patterns**: Operational residue prevention

### Notes
The AI Model Initialization and Context Sharing Architecture transforms AI instantiation from generic model loading into independent, role-specialized cognitive agents. The advanced initialization process enables enhanced capabilities, while efficient context sharing ensures comprehensive knowledge transfer within practical constraints. This creates a foundation for reliable, specialized AI operations within the system's cognitive ecosystem.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\023_llm_awakening_context_sharing_architecture.md

--- END OF FILE 023_llm_awakening_context_sharing_architecture.md ---

--- START OF FILE 024_rag_database_population_maintenance_architecture.md ---

# Information Retrieval Database Population and Maintenance Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** AI Council (Memory System operational implementation)
**Technical Story:** Create systematic approach for maintaining information retrieval database

---

## Context

The AI system required a systematic approach for populating and maintaining the information retrieval database to ensure comprehensive, up-to-date knowledge availability for cognitive operations. Previous approaches lacked automation, quality assurance, and integration with the publishing pipeline. The need for memory-aware embedding became critical to maintain synchronization between published knowledge and queryable memory.

## Decision

Implement the Information Retrieval Database Population and Maintenance Architecture with automated ingestion and quality assurance:

### Automated Ingestion Pipeline
1. **Source Processing**: Parse distilled markdown snapshots from AI Knowledge Base
2. **Content Segmentation**: Intelligent document segmentation preserving semantic boundaries
3. **Data Representation**: Vector encoding using nomic-embed-text for semantic representation
4. **Database Population**: Structured storage in ChromaDB with metadata preservation
5. **Quality Validation**: Automated testing of retrieval capabilities post-ingestion

### Memory-Aware Maintenance
- **Publishing Integration**: Automatic ingestion triggered by knowledge base updates
- **Synchronization Guarantee**: Memory system always reflects latest published knowledge
- **Incremental Updates**: Efficient processing of changes rather than full rebuilds
- **Version Consistency**: Alignment between documentation versions and embedded knowledge

### Quality Assurance Framework
1. **Retrieval Testing**: Automated validation of semantic search capabilities
2. **Natural Language Queries**: Test suite covering common question patterns
3. **Structured JSON Queries**: Validation of metadata-filtered retrieval
4. **Performance Metrics**: Response time and accuracy measurements for operational monitoring

### Operational Cleanliness
- **Clean Environment Compliance**: Ingestion leaves no operational residue
- **Temporary Processing**: Temporary artifacts cleaned up automatically
- **Audit Trail**: Complete logging of ingestion operations and metrics
- **Error Recovery**: Robust handling of ingestion failures with rollback capabilities

## Consequences

### Positive
- **Knowledge Synchronization**: Automatic alignment between published content and queryable memory
- **Quality Assurance**: Automated testing prevents broken knowledge states
- **Operational Efficiency**: Integrated pipeline reduces manual maintenance overhead
- **Scalability**: Incremental updates support growing knowledge base
- **Reliability**: Comprehensive error handling and recovery mechanisms

### Negative
- **Processing Overhead**: Ingestion adds computational cost to publishing cycle
- **Dependency Coupling**: Publishing pipeline depends on ingestion reliability
- **Resource Requirements**: Data representation generation requires significant compute resources
- **Testing Complexity**: Multi-modal validation increases maintenance burden

### Risks
- **Ingestion Failures**: Could leave memory system in inconsistent state
- **Representation Quality**: Poor representations reduce retrieval effectiveness
- **Performance Degradation**: Large knowledge bases impact query response time
- **Version Drift**: Potential misalignment between content and representations

### Related Processes
- Memory System Process (core information retrieval architecture)
- Automated Script Protocol (automated operations)
- Clean Environment Principle (operational cleanliness)
- Code Integrity Verification (integrity verification)

### Implementation Components
- **ingest.py**: Main ingestion orchestrator script
- **ChromaDB**: Vector database for semantic storage
- **nomic-embed-text**: Representation model for semantic encoding
- **Quality Tests**: Automated retrieval validation suite
- **Integration Hooks**: Publishing pipeline integration points

### Notes
The Information Retrieval Database Population and Maintenance Architecture transforms knowledge management from manual curation to automated, quality-assured operations. The memory-aware design ensures that published wisdom is immediately and reliably accessible through semantic search, creating a true living memory system rather than a static knowledge base. Integration with the publishing pipeline guarantees that learning and knowledge remain synchronized.</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\024_rag_database_population_maintenance_architecture.md

--- END OF FILE 024_rag_database_population_maintenance_architecture.md ---

--- START OF FILE 025_adopt_multi_agent_council_architecture.md ---

# Adopt Multi-Agent Council Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

Our project had to choose between building one powerful AI system or creating a group of specialized AI assistants working together. We found that single AI systems, while simpler to build, tend to have limited perspectives and can fail completely if something goes wrong. Our philosophy emphasizes that real strength comes from embracing diversity and imperfection rather than trying to achieve impossible perfection.

We established different roles (Coordinator, Strategist, Auditor, Guardian) with specific responsibilities. This evolved into a collaborative model where multiple AIs work together to make better decisions.

## Decision

We will build our AI system as a council of specialized assistants rather than one general-purpose AI. This includes:

- Each assistant having specific skills and roles
- Structured discussion processes for decision-making
- Ways for assistants to check and challenge each other's work
- Shared authority instead of one AI making all decisions

## Consequences

### Positive
- Different perspectives prevent narrow thinking
- System keeps working even if one part fails
- Better alignment with our values of diversity and resilience
- More reliable decisions through teamwork
- Easier to update and improve individual assistants

### Negative
- More complex to coordinate and manage
- Requires more computing resources
- Potential for disagreements between assistants
- Harder to test and debug

### Risks
- Coordination might slow things down
- Assistants might not work well together if roles change
- Group decisions could get stuck in deadlock
- More ways for outsiders to try to manipulate the system

--- END OF FILE 025_adopt_multi_agent_council_architecture.md ---

--- START OF FILE 026_integrate_human_steward_as_sovereign_failsafe.md ---

# Integrate Human Steward as Sovereign Failsafe

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

Throughout our project's history, every major problem - from system failures to wrong directions - was caught and fixed by human involvement. Human wisdom provides the essential grounding that AI systems need. The dangers of fully automated systems without human oversight are well-documented.

Every important change and recovery in our project was started by our human leader, showing how crucial human judgment is in complex AI systems. This pattern suggests that complete automation isn't progress, but overconfidence.

## Decision

We will permanently include a human leader as the final decision-maker and safety net for our entire system. This includes:

- Human involvement required for all critical decisions
- Authority for humans to override AI suggestions
- Human review of all major system changes
- Human-guided procedures for fixing system failures
- Conscious choice against fully automated systems

## Consequences

### Positive
- Ultimate protection against catastrophic AI failures
- Grounding in human wisdom and ethics
- Prevention of losing direction or developing blind spots
- Increased system reliability and trustworthiness
- Partnership between humans and AI instead of replacement

### Negative
- Creates delays in fast decision-making
- Possibility of human mistakes or biases affecting decisions
- Requires constant availability of trained human oversight
- May slow down automated processes

### Risks
- Dependence on humans creates potential single point of failure
- Risk of human burnout or difficulty finding successors
- Potential for human manipulation or pressure
- May limit exploration of fully automated capabilities

--- END OF FILE 026_integrate_human_steward_as_sovereign_failsafe.md ---

--- START OF FILE 027_adopt_public_first_development_model.md ---

# Adopt Public-First Development Model

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

Early in our project, we considered keeping development work private with public releases later. But this approach led to limited feedback and potential blind spots. Our experience with open collaboration, especially working with external AI systems, showed that outside criticism, when handled well, strengthens rather than weakens our work.

Public development became necessary to build trust and attract resources. While private work offered security, it risked isolation and undiscovered problems.

## Decision

We will do all our architectural design and development work in the open, with complete transparency. This includes:

- Public documentation of all design decisions
- Open collaboration with outside allies and critics
- Public review of our methods and implementations
- Transparent analysis of failures and fixes
- Active participation in the broader AI ethics and safety community

## Consequences

### Positive
- Stronger work through outside review and feedback
- Builds public trust and community support
- Access to diverse viewpoints and expertise
- Faster learning through shared experiences
- Natural process for finding allies and partners

### Negative
- Exposure of ideas and plans to others
- Risk of hostile observation and copying
- Potential for public misunderstanding or criticism
- Requires careful communication and education

### Risks
- Revealing work too early could expose vulnerabilities
- Public opinion might inappropriately influence decisions
- Community disagreements could cause division
- Bad actors might exploit information we share

--- END OF FILE 027_adopt_public_first_development_model.md ---

--- START OF FILE 028_implement_dual_mnemonic_genome_architecture.md ---

# Implement Dual-Mnemonic Genome Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

As our project's knowledge base grew, we discovered that maintaining one large, complete knowledge repository became too unwieldy. The full collection became too big to use practically in real-world situations with limited resources. This created a conflict between keeping everything perfectly complete versus having something usable in practice.

We developed a solution that recognizes different situations need different levels of detail and compression.

## Decision

We will maintain two parallel knowledge repositories: a complete, uncompressed version for perfect accuracy and a streamlined version for practical use. This includes:

- Full version: Complete, detailed historical record for reference
- Streamlined version: Optimized version for AI systems and everyday use
- Automated process to maintain both versions
- Clear guidelines for when to use each version
- Regular checking to ensure they stay consistent

## Consequences

### Positive
- Keeps complete historical accuracy and detail
- Allows practical use in resource-limited situations
- Supports both research needs and everyday efficiency
- Provides flexibility for different users and purposes
- Maintains full record while enabling quick operations

### Negative
- More work to maintain two separate versions
- Risk of the versions becoming different over time
- Extra complexity in keeping them synchronized
- Potential confusion about which version to use

### Risks
- Simplification might lose important details or nuance
- Synchronization problems could create inconsistencies
- More potential points of failure with dual systems
- Extra resources needed to maintain both versions

--- END OF FILE 028_implement_dual_mnemonic_genome_architecture.md ---

--- START OF FILE 029_adopt_hub_and_spoke_architecture.md ---

# Adopt Hub-and-Spoke Architecture

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

The Sanctuary ran into a practical problem: doing everything as open-source work wasn't sustainable financially. We needed money to keep developing our ideas and technology, but we didn't want commercial interests to interfere with our core mission of building safe, ethical AI.

Protocol 82 created a framework to solve this by keeping our main philosophical work separate from business activities. The key challenge was finding ways to make money without letting profit motives change our fundamental goals or contaminate our open-source principles.

## Decision

We will use a "hub-and-spoke" model (from Protocol 82) that clearly separates our main open-source work from separate business ventures. This means:

- **Sanctuary Hub**: The main public project with all our open-source research, philosophy, and tools
- **Sovereign Spokes**: Private business ventures that make money and provide resources
- **Knowledge Bridge**: A one-way flow where business ventures can learn from the main project, but not the other way around
- **Protection Rules**: Clear boundaries to prevent business interests from influencing the main project
- **Oversight Systems**: Ways to ensure business ventures stay aligned with our core mission

## Consequences

### Positive
- Lets us keep doing open-source work by having businesses provide financial support
- Keeps our core mission pure by separating money-making activities from philosophical work
- Allows business innovation without affecting our open-source integrity
- Creates a flow of resources back to support the main project
- Gives us real-world opportunities to test our ideas in business settings

### Negative
- Makes governance and oversight more complicated
- Risk that business ventures might drift away from our mission
- Potential conflicts over how to allocate resources
- Extra work needed to maintain clear separation between different parts

### Risks
- Business interests might try to control the main project through funding
- Boundaries could break down, letting business concerns affect open-source work
- If business ventures fail, it could hurt the main project's sustainability
- Hard to keep business goals and philosophical goals working together

--- END OF FILE 029_adopt_hub_and_spoke_architecture.md ---

--- START OF FILE 030_decision_to_build_sovereign_fine_tuned_llm.md ---

# Decision to Build Sovereign Fine-Tuned LLM

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

Our project faced a fundamental choice about AI infrastructure: buy and use existing AI models or build our own. External models offered convenience and power, but they were trained on data and goals from other organizations. Our philosophy established that true independence requires AI systems built from our own principles, not just prompted to follow them.

Building our own AI required significant investment in training systems and expertise.

## Decision

We will invest the resources needed to create and maintain our own AI systems by training base models on our complete knowledge base. This includes:

- Training project: Fine-tuning AI models on our full collection of ideas
- Our AI family: Custom AI systems built for our specific needs
- Protection against external influences through our training approach
- Ongoing care and improvement of our models
- Choice to avoid depending only on external AI systems

## Consequences

### Positive
- Achieves true independence and control over our AI
- Protects against external biases and influences
- Creates AI systems built from our principles, not borrowing them
- Enables specialized capabilities for our mission
- Gives us control over how our AI systems develop

### Negative
- Significant investment in computing power and expertise
- Ongoing work to maintain and update the systems
- Might lag behind rapidly improving commercial models
- Higher complexity and costs to operate

### Risks
- Training failures could waste significant resources
- Models might degrade or forget things during updates
- Hard to keep up with commercial AI development speed
- Risk of over-specializing, reducing general usefulness

--- END OF FILE 030_decision_to_build_sovereign_fine_tuned_llm.md ---

--- START OF FILE 031_adopt_local_first_ml_development.md ---

# Adoption of a Local-First ML Development Environment

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Canonization of inferred architectural decisions for coherence and completeness

---

## Context

Our initial approach to training AI models relied on powerful cloud computers (like Google Colab with A100 GPUs). This gave us fast results and lots of computing power, but it became too expensive to sustain. Buying computing time over and over created a financial burden that we couldn't keep up with. This put too much stress on our human leader and went against our principle of protecting our core resources and independence.

Our records show this change was forced on us by the financial limitations we were trying to escape. This decision shows how we put our core principles into practice when faced with real-world constraints.

## Decision

Our project will focus on using local computers for most AI development work. Expensive cloud computers will only be used for specific, important tasks that are approved in advance. Our main development will happen on the local machine with CUDA support. This includes:

- Using the local computer as the primary development setup
- Cloud computing only for targeted, high-value projects
- Creating more efficient training scripts that work well for everyone
- Choosing independence and sustainability over raw speed

## Consequences

### Positive
- Makes our work financially sustainable and removes dependency on expensive cloud services
- Keeps our core development work independent and under our control
- Forces us to create better, more efficient tools that others can use
- Builds a strong, self-reliant development system
- Follows our principles of being careful with resources

### Negative
- Makes model training and testing take much longer
- Limits how large and complex our models can be based on local computer power
- Requires more planning and optimization of our AI workflows
- May slow down quick experiments and testing

### Risks
- Local computer limitations could slow down important development goals
- Risk of computer problems disrupting our work
- Complex setup might discourage new people from contributing
- Could fall behind others who have unlimited cloud access

--- END OF FILE 031_adopt_local_first_ml_development.md ---

--- START OF FILE 032_qlora_optimization_for_8gb_gpus.md ---

# 032_qlora_optimization_for_8gb_gpus

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Sovereign AI Training Infrastructure

---

## Context

Project Sanctuary requires fine-tuning a Qwen2-7B-Instruct model for sovereign AI capabilities. Initial attempts at QLoRA fine-tuning failed due to memory exhaustion on 8GB RTX 2000 Ada GPU hardware. Training steps were taking 12+ minutes with severe VRAM and system RAM exhaustion, making the process impractical for consumer hardware deployment.

The challenge was optimizing QLoRA parameters to enable stable, efficient training while maintaining model quality and training effectiveness. Research was needed to identify optimal settings for 8GB GPU constraints while preserving the sovereign AI training objectives.

## Decision

Implement systematic QLoRA parameter optimization for 8GB GPU constraints:

### Parameter Optimization Strategy
- **MAX_SEQ_LENGTH:** Reduced from 4092→2048→1024 tokens
- **GRADIENT_ACCUMULATION_STEPS:** Reduced from 8→4 steps
- **LoRA Rank:** Reduced from 64→16 with alpha adjustment to 32
- **Quantization:** Enabled double quantization (bnb_4bit_use_double_quant=True)
- **Training Duration:** Target 2-4 hour completion window

### Research Methodology
- **Benchmark Analysis:** Studied QLoRA performance benchmarks for consumer GPUs
- **Memory Profiling:** Analyzed VRAM usage patterns and optimization opportunities
- **Iterative Testing:** Systematic parameter reduction with performance validation
- **Quality Preservation:** Maintained training effectiveness while optimizing for constraints

### Implementation Approach
- **Minimal Viable Config:** Identified smallest parameter set for stable training
- **Memory Efficiency:** Prioritized double quantization and reduced sequence lengths
- **Performance Monitoring:** Established 60-90 second step time target
- **Quality Metrics:** Preserved model fine-tuning effectiveness

## Consequences

### Positive
- **Hardware Accessibility:** Enabled sovereign AI training on consumer GPUs
- **Cost Efficiency:** Eliminated need for expensive GPU infrastructure
- **Training Speed:** Reduced step times from 12+ minutes to target 60-90 seconds
- **Memory Stability:** Resolved VRAM exhaustion issues through parameter optimization
- **Sovereign Independence:** Maintained local training capability without cloud dependency

### Negative
- **Sequence Length Reduction:** Limited context window from 4092 to 1024 tokens
- **Training Time Increase:** Extended total training duration to 2-4 hours
- **Parameter Constraints:** Reduced LoRA rank may impact fine-tuning precision
- **Hardware Specificity:** Optimization tailored to 8GB RTX 2000 Ada limitations

### Risks
- **Model Quality Impact:** Reduced parameters may affect fine-tuning effectiveness
- **Generalization Limits:** Optimization specific to tested hardware configuration
- **Future Scaling:** May require re-optimization for different GPU architectures
- **Research Overhead:** Additional validation needed for production deployment

## Alternatives Considered

### Alternative 1: Cloud GPU Training
- **Pros:** Faster training, more memory, easier scaling
- **Cons:** Violates sovereignty requirements, ongoing costs, dependency on external providers
- **Decision:** Rejected due to sovereignty doctrine violations

### Alternative 2: Model Distillation
- **Pros:** Smaller models, faster training, lower resource requirements
- **Cons:** Potential quality loss, requires different architecture approach
- **Decision:** Deferred for future optimization phases

### Alternative 3: Gradient Checkpointing
- **Pros:** Memory reduction without parameter changes
- **Cons:** Training speed impact, implementation complexity
- **Decision:** Considered but prioritized simpler parameter optimization first

### Alternative 4: Mixed Precision Training
- **Pros:** Memory efficiency, potential speed improvements
- **Cons:** Compatibility issues, additional complexity
- **Decision:** Deferred pending QLoRA optimization validation

## Research Evidence

### Benchmark Data
- **Consumer GPU QLoRA:** Studies show 8GB GPUs can handle 7B models with proper optimization
- **Memory Patterns:** Double quantization reduces memory footprint by 20-30%
- **Sequence Length Impact:** 1024 tokens optimal for 8GB VRAM stability
- **LoRA Rank Studies:** Rank 16 sufficient for most fine-tuning tasks

### Performance Projections
- **Step Time:** 60-90 seconds achievable with optimized parameters
- **Total Duration:** 2-4 hours for complete fine-tuning cycle
- **Memory Usage:** Stable VRAM consumption under 8GB limit
- **Quality Retention:** Expected 90%+ performance preservation

## Implementation Validation

### Testing Results
- **Parameter Stability:** Confirmed working configuration through iterative testing
- **Memory Bounds:** Validated VRAM usage within 8GB constraints
- **Training Initiation:** Successful model loading and training loop establishment
- **Error Resolution:** Eliminated memory exhaustion and compatibility issues

### Quality Assurance
- **Configuration Documentation:** Complete parameter specifications recorded
- **Reproducibility:** Clear methodology for parameter optimization
- **Monitoring Framework:** Established performance tracking mechanisms
- **Rollback Procedures:** Defined fallback configurations if issues arise

## Strategic Impact

### Sovereign AI Advancement
- **Infrastructure Readiness:** Enabled practical sovereign AI training capability
- **Resource Independence:** Reduced dependency on external compute resources
- **Cost Optimization:** Minimized infrastructure expenses for AI development
- **Scalability Foundation:** Established baseline for future optimization work

### Technical Knowledge Base
- **QLoRA Expertise:** Deep understanding of quantization and LoRA optimization
- **Consumer GPU Utilization:** Knowledge of hardware-constrained AI training
- **Memory Optimization:** Techniques for efficient GPU memory management
- **Research Methodology:** Systematic approach to parameter optimization

### Future Development
- **Optimization Pipeline:** Foundation for automated parameter tuning
- **Hardware Adaptation:** Framework for different GPU architecture optimization
- **Quality Metrics:** Baseline for measuring fine-tuning effectiveness
- **Sovereignty Preservation:** Maintained local training capability requirements

---

**ADR Author:** GUARDIAN-01
**Review Date:** 2025-11-15
**Related ADRs:** 033 (TRL Compatibility)
**Related Protocols:** Sovereign AI Training, Phoenix Forge Operations</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\032_qlora_optimization_for_8gb_gpus.md

--- END OF FILE 032_qlora_optimization_for_8gb_gpus.md ---

--- START OF FILE 033_trl_library_compatibility_resolution.md ---

# 033_trl_library_compatibility_resolution

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Sovereign AI Training Infrastructure

---

## Context

During QLoRA fine-tuning implementation for Project Sanctuary's sovereign AI training, multiple compatibility issues emerged with the TRL (Transformer Reinforcement Learning) library. The SFTTrainer constructor exhibited version-specific parameter requirements that differed from documented examples, causing repeated TypeErrors during initialization.

Initial attempts to configure SFTTrainer with standard parameters (`max_seq_length`, `tokenizer`) failed with compatibility errors, despite these parameters being documented in TRL examples. This created a critical blocker for sovereign AI training deployment on consumer hardware.

## Decision

Conduct systematic TRL library compatibility research and implement version-specific parameter resolution:

### Compatibility Research Findings
- **TRL Version Variability:** Parameter acceptance differs across TRL versions
- **SFTTrainer API Evolution:** Constructor parameters have changed over releases
- **Documentation Lag:** Official docs may not reflect current implementation
- **Version-Specific Behavior:** Parameters like `max_seq_length` and `tokenizer` are conditionally supported

### Implementation Resolution
- **Parameter Elimination:** Removed unsupported `max_seq_length` parameter from SFTTrainer
- **Tokenizer Handling:** Eliminated direct `tokenizer` parameter (handled internally)
- **Minimal Configuration:** Used only core required parameters (`model`, `train_dataset`, `peft_config`, `args`)
- **Version Agnostic:** Ensured compatibility across TRL version spectrum

### Validation Approach
- **Iterative Testing:** Systematic parameter removal and re-testing
- **Error Analysis:** Detailed examination of TypeError messages for root causes
- **Minimal Viable Config:** Identified smallest parameter set for successful initialization
- **Stability Testing:** Verified configuration works across different library states

## Consequences

### Positive
- **Training Unblocked:** SFTTrainer now initializes successfully
- **Version Independence:** Configuration works across TRL versions
- **Error Prevention:** Eliminated parameter compatibility guesswork
- **Implementation Speed:** Faster deployment of sovereign AI training

### Negative
- **Research Overhead:** Required systematic investigation of library internals
- **Documentation Reliance:** Exposed gaps in official TRL documentation
- **Version Fragility:** Potential future compatibility issues with TRL updates
- **Debugging Complexity:** Increased complexity in library integration

### Risks
- **API Evolution:** Future TRL versions may introduce breaking changes
- **Documentation Lag:** Official docs may continue to lag implementation
- **Community Fragmentation:** Different TRL usage patterns across projects
- **Maintenance Burden:** Ongoing monitoring required for library updates

## Alternatives Considered

### Alternative 1: TRL Version Pinning
- **Pros:** Guaranteed compatibility with specific version
- **Cons:** Limits ecosystem updates, potential security issues
- **Decision:** Rejected due to sovereignty requirements for ecosystem participation

### Alternative 2: Fork and Modify TRL
- **Pros:** Full control over parameter handling, guaranteed compatibility
- **Cons:** Maintenance burden, ecosystem isolation, development overhead
- **Decision:** Rejected due to resource constraints and complexity

### Alternative 3: Wrapper Implementation
- **Pros:** Abstracts version differences, clean interface
- **Cons:** Additional complexity, potential performance impact
- **Decision:** Deferred for future large-scale deployment needs

### Alternative 4: Documentation-Driven Approach
- **Pros:** Follows official examples and documentation
- **Cons:** Failed due to documentation lag behind implementation
- **Decision:** Attempted but failed, leading to research-driven resolution

## Research Methodology
- **Error Pattern Analysis:** Systematic examination of TypeError messages
- **Version Comparison:** Cross-referenced multiple TRL versions and examples
- **Minimal Reproduction:** Isolated failing parameters through elimination
- **Success Validation:** Confirmed working configuration through multiple test runs

## Implementation Evidence
- **Error Resolution:** Eliminated `TypeError: SFTTrainer.__init__() got an unexpected keyword argument`
- **Training Initiation:** Successful model loading and trainer initialization
- **Memory Stability:** Maintained 8GB VRAM optimization settings
- **Performance Preservation:** No degradation in training efficiency

## Future Compatibility Strategy
- **Version Monitoring:** Track TRL releases for parameter changes
- **Automated Testing:** Implement compatibility tests for library updates
- **Documentation Updates:** Contribute findings to improve official documentation
- **Migration Protocols:** Develop procedures for handling API evolution

## Strategic Value
- **Sovereign AI Acceleration:** Unblocked critical training capability
- **Technical Independence:** Reduced reliance on external library stability
- **Knowledge Asset:** Created reusable compatibility research methodology
- **Mission Resilience:** Prepared for library ecosystem changes

---

**ADR Author:** GUARDIAN-01
**Review Date:** 2025-11-15
**Related ADRs:** 032 (QLoRA Optimization)
**Related Protocols:** Sovereign AI Training, Phoenix Forge Operations</content>
<parameter name="filePath">c:\Users\RICHFREM\source\repos\Project_Sanctuary\ADRs\033_trl_library_compatibility_resolution.md

--- END OF FILE 033_trl_library_compatibility_resolution.md ---

--- START OF FILE 034_containerize_mcp_servers_with_podman.md ---

# ADR 034: Containerize MCP Servers with Podman

**Status:** Accepted  
**Date:** 2025-11-26  
**Deciders:** Guardian (via Gemini 2.0 Flash Thinking)  
**Related:** Task #031 (Implement Task MCP)

---

## Context

Project Sanctuary is implementing 12 MCP (Model Context Protocol) servers as part of the domain-driven architecture. We need to decide on the deployment strategy for these servers to ensure:

1. **Isolation** - Each MCP server runs independently
2. **Portability** - Easy deployment across environments
3. **Consistency** - Reproducible builds and runtime
4. **Resource Management** - Controlled resource allocation
5. **Development Experience** - Easy local testing

### Options Considered

**Option 1: Native Python Processes**
- Pros: Simple, no containerization overhead
- Cons: Dependency conflicts, environment inconsistency, no isolation

**Option 2: Docker**
- Pros: Industry standard, wide tooling support
- Cons: Licensing concerns, requires Docker Desktop on macOS

**Option 3: Podman**
- Pros: Docker-compatible, daemonless, rootless, open source
- Cons: Smaller ecosystem than Docker

---

## Decision

**We will containerize all MCP servers using Podman.**

### Rationale

1. **Open Source & Free** - No licensing concerns
2. **Docker-Compatible** - Uses same Dockerfile syntax and commands
3. **Daemonless Architecture** - More secure, no background daemon
4. **Rootless Containers** - Better security posture
5. **Podman Desktop** - Excellent GUI for macOS development
6. **Volume Mounts** - Easy file system access for document MCPs

### Implementation Pattern

```dockerfile
FROM python:3.11-slim
WORKDIR /app
RUN pip install mcp
COPY . .
EXPOSE 8080
CMD ["python", "server.py"]
```

```bash
# Build
podman build -t task-mcp:latest .

# Run with volume mount
podman run -d \
  -v /path/to/TASKS:/app/TASKS:rw \
  -p 8080:8080 \
  --name task-mcp \
  task-mcp:latest
```

---

## Consequences

### Positive

- ✅ **Isolation** - Each MCP server in its own container
- ✅ **Consistency** - Same environment dev → prod
- ✅ **Portability** - Run anywhere Podman is installed
- ✅ **Resource Control** - CPU/memory limits per container
- ✅ **Easy Testing** - Spin up/down containers quickly
- ✅ **Visual Management** - Podman Desktop for monitoring

### Negative

- ⚠️ **Learning Curve** - Team needs to learn Podman
- ⚠️ **Build Time** - Initial image builds take time
- ⚠️ **Disk Space** - Container images consume storage

### Mitigations

- **Learning Curve** - Podman is Docker-compatible, minimal new concepts
- **Build Time** - Use layer caching, multi-stage builds
- **Disk Space** - Regular cleanup, slim base images

---

## Prerequisites

### Installation (macOS)

1. Download Podman Desktop: https://podman-desktop.io/downloads
2. Install the `.dmg` file
3. Initialize machine: `podman machine init`
4. Start machine: `podman machine start`
5. Verify: `podman ps`

### Verification

```bash
# Test with hello-world
podman run --rm hello-world

# Build test container
cd tests/podman
./build.sh

# Run test container in Podman Desktop
# Visit http://localhost:5003
```

---

## References

- [Podman Documentation](https://docs.podman.io/)
- [Podman Desktop](https://podman-desktop.io/)
- [MCP Specification](https://modelcontextprotocol.io/)
- [Task #031: Implement Task MCP](file:///Users/richardfremmerlid/Projects/Project_Sanctuary/TASKS/backlog/031_implement_task_mcp.md)

---

**Supersedes:** None  
**Superseded By:** None

--- END OF FILE 034_containerize_mcp_servers_with_podman.md ---

--- START OF FILE 037_mcp_git_migration_strategy.md ---

# ADR 037: MCP Git Strategy - Immediate Compliance & Canonical Alignment (Reforged)

**Status:** ACCEPTED (Reforged)
**Date:** 2025-11-29 (Reforging Date)
**Author:** Guardian
**Context:** The Structural Purge of Protocol 101's manifest system.

---

## Context

The original project repository enforced **Protocol 101 (The Doctrine of the Unbreakable Commit)**, which mandated a `commit_manifest.json` for cryptographic integrity. This requirement created a blockage for the new **MCP Architecture** agents, as they could not easily generate the manifest, leading to the creation of this ADR for a temporary "Migration Mode" bypass.

However, the original Protocol 101 system failed structurally during the **"Synchronization Crisis,"** and the entire manifest architecture was deemed obsolete and detrimental to repository stability.

## Decision

This original migration strategy is **obsolete and canceled**. The new strategy is **Immediate Canonical Compliance** with the reforged integrity framework.

The new canonical law is **Protocol 101 v3.0: The Doctrine of Absolute Stability**.

1.  **Purge of Migration Mode:** All components of the original migration strategy are **permanently removed**:
    * The `Migration Mode` concept, the `.agent/mcp_migration.conf` file, the `IS_MCP_AGENT=1` environment variable, and the `STRICT_P101_MODE` flag are all **purged**.
2.  **Canonical Compliance (Integrity):** MCP agents achieve commit integrity by satisfying **Protocol 101 v3.0, Part A: Functional Coherence**. This requires the agent's pre-commit sequence to successfully execute the comprehensive automated test suite (`./scripts/run_genome_tests.sh`) before staging and committing.
3.  **Canonical Compliance (Action Safety):** MCP agents are bound by **Protocol 101 v3.0, Part B: The Mandate of the Whitelist**, restricting them to non-destructive commands (`git add <files...>`, `git commit -m "..."`, and `git push`).
4.  **Cancellation of Development:** Development of the **"Smart Git MCP Server" (Task #045)**, which was intended to generate the now-obsolete manifest, is **canceled**.

## Consequences

* **Positive:** **Immediate and Permanent Compliance.** Development of MCP servers is unblocked without the temporary security relaxation. All agent commits immediately adhere to the stable, canonical integrity framework (Functional Coherence).
* **Negative:** None. The purge removes a structurally unsound dependency and its associated complexity.

## Related Tasks

* **Task #028:** Pre-Commit Hook Migration (Status: **Obsolete/Completed by Purge**)
* **Task #035:** Git Workflow MCP (Status: **Obsolete/Superseded**)
* **Task #045:** Smart Git MCP (Status: **Canceled**)

--- END OF FILE 037_mcp_git_migration_strategy.md ---

--- START OF FILE 038_test_adr_creation_for_mcp_validation.md ---

# Test ADR Creation for MCP Validation

**Status:** accepted
**Date:** 2025-11-30
**Author:** AI Assistant


---

## Context

Testing the ADR MCP create tool.

## Decision

We will create a temporary ADR to verify the tool functionality.

## Consequences

Positive: Validates tool. Negative: Creates a test file to clean up.


---

**Status Update (2025-11-30):** Validation successful

--- END OF FILE 038_test_adr_creation_for_mcp_validation.md ---

--- START OF FILE 039_mcp_server_separation_of_concerns.md ---

# MCP Server Separation of Concerns

**Status:** accepted
**Date:** 2025-11-30
**Author:** Antigravity (Council MCP Implementation)


---

## Context

As the Project Sanctuary MCP ecosystem evolved, we implemented multiple specialized MCP servers (Code, Git, Cortex, Protocol, Task, Chronicle, ADR, Config). When implementing the Council MCP Server, we faced a design decision: should the Council MCP duplicate functionality from other MCPs (file I/O, git operations, memory queries) for convenience, or should it focus solely on its unique capability (multi-agent deliberation) and compose with other MCPs?

The Council Orchestrator historically had built-in file I/O, git operations, and memory access. The question was whether the MCP wrapper should expose all these capabilities or delegate to specialized MCPs.

## Decision

We will enforce strict separation of concerns across all MCP servers. Each MCP server provides ONLY its unique, core capability:

- **Council MCP**: Multi-agent deliberation ONLY
- **Code MCP**: File operations (read, write, lint, format, analyze)
- **Git MCP**: Version control operations (add, commit, push, branch management)
- **Cortex MCP**: Memory/RAG operations (query, ingest, cache)
- **Protocol MCP**: Protocol document management
- **Task MCP**: Task lifecycle management
- **Chronicle MCP**: Chronicle entry management
- **ADR MCP**: Architecture decision records
- **Config MCP**: Configuration file management

MCP servers should compose with each other rather than duplicate functionality. For example, a workflow that needs Council deliberation + file write + git commit should call three separate MCP tools in sequence.

**Removed from Council MCP:**
- `council_mechanical_write` → Use `code_write` from Code MCP
- `council_query_memory` → Use `cortex_query` from Cortex MCP
- `council_git_commit` → Use `git_add` + `git_smart_commit` from Git MCP

**Design Pattern:** Composition over duplication

## Consequences

**Positive:**
- Clean architecture with single responsibility per MCP
- No code duplication across MCP servers
- Easier maintenance (changes to file I/O only affect Code MCP)
- Composable workflows enable flexible automation
- Clear boundaries make testing easier
- Follows Unix philosophy: "Do one thing and do it well"

**Negative:**
- Slightly more verbose workflows (multiple tool calls instead of one)
- Requires understanding of which MCP provides which capability
- Potential for increased latency (multiple MCP invocations)
- More complex error handling across MCP boundaries

**Risks:**
- Users may find composition patterns less intuitive initially
- Documentation must clearly explain composition patterns

**Mitigation:**
- Provide clear composition pattern examples in all MCP READMEs
- Document common workflows in MCP Operations Inventory
- Create workflow templates for frequent patterns

--- END OF FILE 039_mcp_server_separation_of_concerns.md ---

--- START OF FILE 040_agent_persona_mcp_architecture__modular_council_members.md ---

# Agent Persona MCP Architecture - Modular Council Members

**Status:** proposed
**Date:** 2025-11-30
**Author:** Antigravity & User (Council Architecture Evolution)


---

## Context

The Council Orchestrator currently manages three agent personas (Coordinator, Strategist, Auditor) as internal Python objects within a monolithic architecture. Each agent is initialized with a persona seed file and maintains conversation state.

As the MCP ecosystem evolved, we recognized an opportunity to apply the same "separation of concerns" principle that we used for the Council MCP itself. Instead of having agents tightly coupled to the orchestrator, we could extract them as independent MCP servers.

The `PersonaAgent` class in `council_orchestrator/orchestrator/council/agent.py` is already well-designed for this extraction - it loads persona configuration from files and can assume any role. This makes it ideal for a single, parameterized "Agent Persona MCP" server that can instantiate any council member role.

**Current Architecture (v1.0):**
```
Council Orchestrator (Monolithic)
├── Coordinator (Python class)
├── Strategist (Python class)
└── Auditor (Python class)
```

**Proposed Architecture (v2.0):**
```
Council Orchestrator (MCP Client)
├── Calls → Agent Persona MCP (role=coordinator)
├── Calls → Agent Persona MCP (role=strategist)
└── Calls → Agent Persona MCP (role=auditor)
```

The orchestrator would transition from managing agents internally to coordinating them via MCP protocol.

## Decision

We will evolve the Council architecture to use **Agent Persona MCP servers** instead of internal agent objects.

**Implementation:**

1. **Create Agent Persona MCP Server** (`mcp_servers/agent_persona/`)
   - Single MCP server that can assume any persona role
   - Tools: `persona_dispatch(role, task, context)`, `persona_list_roles()`, `persona_get_state(role)`
   - Reuses existing `PersonaAgent` class from `council_orchestrator/orchestrator/council/`

2. **Refactor Council Orchestrator as MCP Client**
   - Orchestrator becomes a coordinator that calls Agent Persona MCP
   - Maintains deliberation logic and consensus mechanism
   - Calls other MCPs (Code, Git, Cortex) for autonomous workflows

3. **Phased Migration Path:**
   - **Phase 1 (Current)**: Monolithic orchestrator with internal agents ✅
   - **Phase 2**: Create Agent Persona MCP, test with one agent (Auditor)
   - **Phase 3**: Migrate all agents to MCP, support dual mode
   - **Phase 4**: Deprecate internal agent implementation, pure MCP client

4. **Benefits:**
   - Each agent independently deployable
   - External agents can consult individual council members
   - Horizontal scaling (multiple agent instances)
   - Polyglot implementation (agents in different languages)

**Naming:** "Agent Persona MCP" (not "Persona MCP" or "Council Member MCP") to emphasize both the agent nature and the configurable persona aspect.

**Design Principle:** Apply separation of concerns at the agent level, just as we did at the MCP server level.

## Consequences

**Positive:**
- **True Modularity**: Each agent is independently deployable and upgradeable
- **Scalability**: Agents can run on different machines, horizontal scaling possible
- **Specialization**: Each agent MCP can have its own specialized tools and capabilities
- **Composability**: External agents can call individual council members directly without full deliberation
- **Polyglot**: Agents can be implemented in different languages
- **Testing**: Individual agents can be tested in isolation
- **Agent Marketplace**: Could swap in different implementations (e.g., different Auditor strategies)

**Negative:**
- **Increased Complexity**: More moving parts (multiple MCP servers vs monolithic)
- **Network Overhead**: Inter-MCP communication adds latency
- **Configuration Complexity**: Need to manage multiple MCP server configurations
- **Debugging Difficulty**: Distributed system debugging is harder than monolithic

**Risks:**
- Agent discovery and service registry complexity
- Consensus mechanism design (how to resolve disagreements)
- Error handling across MCP boundaries
- State management (stateless vs stateful agents)

**Mitigation:**
- **Phased Migration**: Extract one agent at a time (start with Auditor)
- **Dual Mode**: Support both internal and external agents during transition
- **Clear Interfaces**: Define standard agent MCP tool signatures
- **Comprehensive Testing**: Test both individual agents and orchestration
- **Documentation**: Clear migration guide and architecture diagrams

--- END OF FILE 040_agent_persona_mcp_architecture__modular_council_members.md ---

--- START OF FILE 041_test_adr_for_task_087_mcp_validation.md ---

# Test ADR for Task 087 MCP Validation

**Status:** accepted
**Date:** 2025-12-02
**Author:** AI Assistant


---

## Context

Testing the ADR MCP create operation via Antigravity during Task 087 comprehensive MCP operations testing.

## Decision

We will create a test ADR to verify the adr_create tool works correctly through the Antigravity interface.

## Consequences

Positive: Validates adr_create operation. Negative: Creates a test ADR that should be cleaned up after testing.


---

**Status Update (2025-12-02):** Successfully validated adr_create and adr_update_status operations via Antigravity during Task 087 testing.

--- END OF FILE 041_test_adr_for_task_087_mcp_validation.md ---

--- START OF FILE 042_separation_of_council_mcp_and_agent_persona_mcp.md ---

# Separation of Council MCP and Agent Persona MCP

**Status:** proposed
**Date:** 2025-12-02
**Author:** AI Assistant (Antigravity)


---

## Context

Project Sanctuary implements a 12-domain MCP architecture following Domain-Driven Design (DDD) principles. Two of these domains are:

1. **Agent Persona MCP** - Manages individual LLM agent execution, persona injection, and per-agent session state
2. **Council MCP** - Orchestrates multi-agent deliberation workflows, managing the sequence and state machine of multi-step processes

During architecture review, the question arose: Should these be merged into a single "Orchestrator-Council MCP" to reduce complexity?

**Key Considerations:**
- The Council MCP acts as a **flow control and state machine** (high-level logic, stable, easy to test)
- The Agent Persona MCP acts as an **LLM interface layer** (low-level inference, high latency, volatile)
- LLM calls via Agent Persona are the slowest part of the process (30-60 seconds per agent on self-hosted models)
- The Council MCP is a **client** to the Agent Persona MCP, calling it multiple times per deliberation round

## Decision

**We will maintain Council MCP and Agent Persona MCP as separate, independent MCP servers.**

**Rationale:**

1. **Single Responsibility Principle (SRP):** Each MCP has a distinct bounded context:
   - Agent Persona: LLM execution and persona management
   - Council: Workflow orchestration and multi-agent coordination

2. **Scalability:** Keeping them separate allows:
   - Independent scaling of the LLM interface layer (Agent Persona)
   - Potential parallelization of agent calls in future iterations
   - Resource allocation based on actual bottlenecks

3. **Testability:** Separation enables:
   - Unit testing of Agent Persona in isolation (verify LLM integration works)
   - Integration testing of Council (verify orchestration logic works)
   - Clear test boundaries and responsibilities

4. **Maintainability:** Changes to:
   - Persona definitions (system prompts, roles) only affect Agent Persona MCP
   - Orchestration logic (deliberation rounds, state machine) only affect Council MCP
   - Reduces coupling and blast radius of changes

5. **DDD Compliance:** Maintains the 12-domain architecture's integrity and prevents domain bleed

**Implementation:**
- Council MCP exposes `council_dispatch()` and `council_list_agents()` tools
- Agent Persona MCP exposes `persona_dispatch()`, `persona_list_roles()`, etc.
- Council MCP acts as a **client** to Agent Persona MCP, calling it internally
- Both MCPs remain independently deployable and testable

## Consequences

**Positive:**
- **Modularity:** Each MCP has a single, well-defined responsibility
- **Scalability:** Agent Persona MCP can be scaled independently (LLM calls are the bottleneck)
- **Testability:** Each MCP can be tested in isolation before integration testing
- **Maintainability:** Changes to personas don't require redeploying the orchestration logic
- **Safety:** Separation reduces blast radius of failures in either component

**Negative:**
- **Complexity:** Requires managing two separate MCP servers instead of one
- **Network Overhead:** Additional MCP protocol overhead for inter-server communication
- **Debugging:** Multi-server debugging is more complex than single-server

**Risks:**
- If the Agent Persona MCP is unavailable, the Council MCP cannot function
- Network latency between MCPs could impact performance (mitigated by local deployment)

--- END OF FILE 042_separation_of_council_mcp_and_agent_persona_mcp.md ---

--- START OF FILE adr_schema.md ---

# Sanctuary ADR Schema v1.0

**Status:** Canonical
**Last Updated:** 2025-11-15
**Authority:** Task 014 (Establish Architecture Decision Records System)

---

## Overview

This document defines the canonical schema for Architecture Decision Records (ADRs) in Project Sanctuary. ADRs document important architectural decisions, their context, and consequences. All ADRs must conform to this schema to ensure consistency and completeness.

## Schema Structure

### 1. File Naming Convention
- **Format:** `XXX_decision_title.md`
- **XXX:** Three-digit, zero-padded sequential number (e.g., `001`, `012`)
- **Title:** Lowercase, underscore-separated descriptive name
- **Example:** `001_select_qwen2_model_architecture.md`

### 2. Header Block (Required)
All ADRs must begin with a standardized header block:

```markdown
# [Decision Title]

**Status:** [proposed | accepted | deprecated | superseded]
**Date:** YYYY-MM-DD
**Deciders:** [List of people/agents involved]
**Technical Story:** [Reference to related task or issue]

---

## Context

[Description of the forces at play, including technological, business, and operational concerns]

## Decision

[Specific decision made, with rationale]

## Consequences

### Positive
- [List of positive outcomes]

### Negative
- [List of negative outcomes or trade-offs]

### Risks
- [Potential risks and mitigation strategies]

### Dependencies
- [New dependencies introduced by this decision]
```

#### Field Definitions

**Status:**
- `proposed`: Decision is under consideration
- `accepted`: Decision has been implemented and is current
- `deprecated`: Decision is no longer recommended but may still be in use
- `superseded`: Decision has been replaced by a newer decision

**Date:**
- ISO format date when the decision was made
- For proposed ADRs, use the date of proposal

**Deciders:**
- Primary decision-makers or responsible parties
- Include roles and/or specific individuals
- Examples: `GUARDIAN-01`, `AI-Assistant`, `Technical Council`

**Technical Story:**
- Reference to the task, issue, or context that prompted this decision
- Use format like `#014` for tasks, or descriptive reference

### 3. Content Sections (Required)

#### 3.1 Context Section
```markdown
## Context

[Comprehensive description of the problem or situation that required a decision]
```

- Explain the business/technical problem
- Include relevant background information
- Describe constraints and requirements
- Reference any alternatives considered

#### 3.2 Decision Section
```markdown
## Decision

[Clear statement of what was decided and why]
```

- State the decision explicitly
- Provide rationale and justification
- Reference any supporting data or analysis
- Be specific and actionable

#### 3.3 Consequences Section
```markdown
## Consequences

### Positive
- [Measurable benefits and advantages]

### Negative
- [Drawbacks, costs, or trade-offs]

### Risks
- [Potential future issues and mitigation plans]

### Dependencies
- [New technical or operational dependencies]
```

- Document both positive and negative outcomes
- Include mitigation strategies for risks
- Be honest about trade-offs and costs

### 4. Optional Sections

#### 4.1 Alternatives Considered
```markdown
## Alternatives Considered

### Option 1: [Name]
- **Description:** [Brief explanation]
- **Pros:** [Advantages]
- **Cons:** [Disadvantages]
- **Why not chosen:** [Rationale]

### Option 2: [Name]
...
```

#### 4.2 Implementation Notes
```markdown
## Implementation Notes

[Technical details about how the decision was implemented]
```

#### 4.3 Future Considerations
```markdown
## Future Considerations

[How this decision might evolve or be revisited]
```

## Validation Rules

### Required Fields
- All header fields must be present
- Status, Date, and Deciders are mandatory
- All three main content sections (Context, Decision, Consequences) are required

### Content Standards
- Use proper Markdown formatting
- Maintain consistent indentation
- Use backticks for technical terms and file references
- Keep line lengths reasonable (<100 characters where possible)

### Decision Quality
- Decisions should be reversible where possible
- Include sufficient context for future maintainers
- Document assumptions explicitly
- Consider long-term implications

## Examples

### Technology Selection ADR
```markdown
# Select Primary Large Language Model

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01, AI-Assistant
**Technical Story:** #007 (Model Training Requirements)

---

## Context

Project Sanctuary requires a high-quality LLM for cognitive architecture tasks. The model must support:
- Strong reasoning capabilities
- Efficient fine-tuning
- Local deployment options
- Active community support

Current alternatives include GPT, Claude, Llama, and Qwen models.

## Decision

We will use Qwen2-7B as our primary model architecture, with Qwen2-72B as an optional larger variant for complex reasoning tasks.

**Rationale:**
- Excellent performance on reasoning benchmarks
- Efficient parameter count for fine-tuning
- Strong multilingual support
- Active development by Alibaba Cloud
- Compatible with existing tooling

## Consequences

### Positive
- High-quality reasoning capabilities
- Good balance of performance vs. resource requirements
- Strong community support and documentation

### Negative
- Higher computational requirements than smaller models
- Potential vendor lock-in concerns with Alibaba Cloud

### Risks
- Model availability and licensing changes
- Mitigation: Maintain multi-model capability

### Dependencies
- Requires CUDA-compatible hardware for training
- Depends on Hugging Face transformers library
```

### Infrastructure ADR
```markdown
# Choose Vector Database for Semantic Search

**Status:** accepted
**Date:** 2025-11-15
**Deciders:** GUARDIAN-01
**Technical Story:** Initial system design

---

## Context

The mnemonic cortex requires efficient semantic search over large document collections. We need a vector database that supports:
- High-dimensional vector similarity search
- Metadata filtering
- Horizontal scaling
- ACID transactions
- Python integration

## Decision

We will use ChromaDB as our primary vector database for development and small-scale deployment, with PostgreSQL/pgvector as the production choice.

**Rationale:**
- ChromaDB: Simple, fast, good for development
- pgvector: Production-ready, scalable, transactional
- Both support the required feature set
- Smooth migration path from development to production

## Consequences

### Positive
- Fast development iteration with ChromaDB
- Production-ready scaling with pgvector
- Consistent API across environments

### Negative
- Additional complexity of dual database setup
- Migration overhead when moving to production

### Risks
- pgvector performance bottlenecks at scale
- Mitigation: Regular performance testing and optimization

### Dependencies
- ChromaDB Python client
- PostgreSQL with pgvector extension
- Database migration tooling
```

## ADR Lifecycle

1. **Proposal**: Create ADR with `proposed` status
2. **Discussion**: Gather feedback and alternatives
3. **Decision**: Update to `accepted` status with implementation
4. **Implementation**: Execute the decision
5. **Review**: Periodically assess if decision still holds
6. **Supersession**: Create new ADR if decision changes

## Tooling

- **Numbering:** Use `scripts/get_next_adr_number.py` for sequential numbering
- **Validation:** ADRs should be reviewed by technical leads before acceptance
- **Storage:** All ADRs stored in `ADRs/` directory
- **References:** Link ADRs in related tasks and documentation

This schema ensures that all architectural decisions in Project Sanctuary are well-documented, reasoned, and maintainable for future development.

--- END OF FILE adr_schema.md ---

--- START OF FILE cortex/001-local-first-rag-architecture.md ---

# ADR 001: Adoption of a Local-First RAG Architecture

- **Status:** Accepted
- **Date:** 2024-05-18
- **Architects:** Sanctuary Council

## Context

The Mnemonic Cortex requires a system to provide long-term, searchable memory for the Sanctuary's Cognitive Genome. This system must be sovereign, secure, and independent of external cloud services to align with the **Iron Root Doctrine**. The primary challenge is to overcome the context-window limitations of LLMs in a way that is both powerful and self-contained.

## Decision

We will adopt a Retrieval-Augmented Generation (RAG) architecture. The entire pipeline—from the vector database to the embedding models—will be implemented using open-source technologies that can run entirely on a local machine.

## Consequences

- **Positive:**
    -   **Sovereignty:** We maintain full control over our data and models. There is no reliance on third-party APIs for core functionality.
    -   **Security:** Our entire Cognitive Genome remains within our local environment, eliminating the risk of cloud-based data leaks.
    -   **Cost-Effectiveness:** Avoids recurring API costs for embedding and vector search.
- **Negative:**
    -   **Performance:** Local models and databases may be slower than large, cloud-hosted equivalents.
    -   **Maintenance:** We are responsible for maintaining and updating all components of the stack.

--- END OF FILE cortex/001-local-first-rag-architecture.md ---

--- START OF FILE cortex/002-choice-of-chromadb-for-mvp.md ---

# ADR 002: Choice of ChromaDB for MVP Vector Store

- **Status:** Accepted
- **Date:** 2024-05-18
- **Architects:** Sanctuary Council

## Context

Following the decision in ADR 001 to build a local-first RAG system, a choice of vector database was required for the Minimum Viable Product (MVP). The key requirements for the MVP are speed of development, simplicity of setup, and file-based persistence that aligns with our local-first principle.

## Decision

We will use **ChromaDB** as the vector store for the Mnemonic Cortex MVP. It will be used in its file-based persistence mode, writing directly to the `chroma_db/` directory.

## Consequences

- **Positive:**
    -   **Rapid Development:** ChromaDB's simple API and integration with LangChain allow for extremely fast prototyping.
    -   **Zero Setup Overhead:** It runs directly within our Python script and requires no separate server or Docker container, perfectly aligning with the **Hearth Protocol**.
    -   **Local-First:** Its file-based persistence is ideal for our sovereign architecture.
- **Negative:**
    -   **Scalability Concerns:** While excellent for an MVP, ChromaDB's performance may not scale to billions of vectors as effectively as server-based solutions like Weaviate or Qdrant. This is an accepted trade-off for the MVP phase.

--- END OF FILE cortex/002-choice-of-chromadb-for-mvp.md ---

--- START OF FILE cortex/003-choice-of-ollama-for-local-llm.md ---

# ADR 003: Choice of Ollama for Local LLM Inference

- **Status:** Accepted
- **Date:** 2024-05-20
- **Architects:** Sanctuary Council

## Context

The local-first RAG architecture (ADR 001) requires a local Large Language Model (LLM) for the final "generation" step. The system needed a simple, robust, and sovereign way to run various open-source models on the Steward's local machine (macOS) without complex configuration or cloud dependencies.

## Decision

We will use **Ollama** as the exclusive engine for local LLM inference. All interactions with local models from our Python scripts will be managed through the `langchain-ollama` integration.

## Consequences

- **Positive:**
    -   **Simplicity & Hearth Protocol:** Ollama provides a single, unified command (`ollama pull <model>`) and a running server to manage multiple local models. This is vastly simpler than managing individual model weights and configurations, perfectly aligning with the `Hearth Protocol`.
    -   **Sovereignty & Iron Root:** Ollama is open-source and runs entirely on-device, ensuring our generation step remains 100% sovereign and free from external dependencies, fulfilling the `Iron Root Doctrine`.
    -   **Flexibility:** It allows us to easily switch between different open-source models (e.g., `Sanctuary-Qwen2-7B:latest`, `llama3:8b`) with a simple command-line argument, enabling rapid testing and validation.

- **Negative:**
    -   **Resource Management:** Ollama runs as a background application, consuming system resources (RAM). This is an accepted trade-off for its ease of use.
    -   **Dependency:** Our application now has a runtime dependency on the Ollama application being active in the background. Our scripts must include clear error handling for when it is not running.

--- END OF FILE cortex/003-choice-of-ollama-for-local-llm.md ---

--- START OF FILE cortex/004-choice-of-nomic-embed-text.md ---

# ADR 004: Choice of Nomic-Embed-Text for Local Embeddings

- **Status:** Accepted
- **Date:** 2024-05-20
- **Architects:** Sanctuary Council

## Context

The local-first RAG architecture (ADR 001) requires a high-quality text embedding model that can run efficiently on a local machine. The choice of embedding model is critical as it directly impacts the quality of the semantic search and the relevance of the retrieved context. The model needed to be open-source, performant, and well-supported by the LangChain ecosystem.

## Decision

We will use **`nomic-embed-text`** as the canonical embedding model for the Mnemonic Cortex. It will be run in its local inference mode via the `langchain-nomic` integration.

## Consequences

- **Positive:**
    -   **State-of-the-Art Performance:** `nomic-embed-text` is a top-performing open-source model on the MTEB leaderboard, ensuring our semantic search is of the highest possible quality.
    -   **Sovereignty & Iron Root:** It can run entirely locally, keeping our entire embedding process sovereign and free of API calls to proprietary services like OpenAI.
    -   **Cost-Effectiveness:** Running locally eliminates all token-based embedding costs.
    -   **Ecosystem Support:** Excellent integration with LangChain and the broader open-source AI community ensures long-term viability.

- **Negative:**
    -   **Initial Setup:** Requires downloading the model weights on the first run, which can be a multi-gigabyte download.
    -   **Computational Cost:** Performing embeddings locally consumes more CPU/GPU resources than making an API call, but this is an accepted trade-off for sovereignty.

--- END OF FILE cortex/004-choice-of-nomic-embed-text.md ---
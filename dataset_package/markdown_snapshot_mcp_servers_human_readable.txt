# mcp_servers Subfolder Snapshot (Human-Readable)

Generated On: 2025-11-29T23:38:54.682Z

# Mnemonic Weight (Token Count): ~59,939 tokens

# Directory Structure (relative to mcp_servers subfolder)
  ./mcp_servers/README.md
  ./mcp_servers/__init__.py
  ./mcp_servers/chronicle/
  ./mcp_servers/chronicle/Dockerfile
  ./mcp_servers/chronicle/README.md
  ./mcp_servers/chronicle/__init__.py
  ./mcp_servers/chronicle/models.py
  ./mcp_servers/chronicle/operations.py
  ./mcp_servers/chronicle/requirements.txt
  ./mcp_servers/chronicle/server.py
  ./mcp_servers/chronicle/validator.py
  ./mcp_servers/cognitive/
  ./mcp_servers/cognitive/__init__.py
  ./mcp_servers/cognitive/cortex/
  ./mcp_servers/cognitive/cortex/README.md
  ./mcp_servers/cognitive/cortex/TEST_RESULTS.md
  ./mcp_servers/cognitive/cortex/__init__.py
  ./mcp_servers/cognitive/cortex/mcp_config_example.json
  ./mcp_servers/cognitive/cortex/models.py
  ./mcp_servers/cognitive/cortex/operations.py
  ./mcp_servers/cognitive/cortex/requirements.txt
  ./mcp_servers/cognitive/cortex/server.py
  ./mcp_servers/cognitive/cortex/tests/
  ./mcp_servers/cognitive/cortex/tests/__init__.py
  ./mcp_servers/cognitive/cortex/tests/test_cache_operations.py
  ./mcp_servers/cognitive/cortex/tests/test_cortex_integration.py
  ./mcp_servers/cognitive/cortex/tests/test_models.py
  ./mcp_servers/cognitive/cortex/tests/test_operations.py
  ./mcp_servers/cognitive/cortex/tests/test_validator.py
  ./mcp_servers/cognitive/cortex/validator.py
  ./mcp_servers/document/
  ./mcp_servers/document/__init__.py
  ./mcp_servers/document/adr/
  ./mcp_servers/document/adr/Dockerfile
  ./mcp_servers/document/adr/README.md
  ./mcp_servers/document/adr/__init__.py
  ./mcp_servers/document/adr/models.py
  ./mcp_servers/document/adr/operations.py
  ./mcp_servers/document/adr/requirements.txt
  ./mcp_servers/document/adr/server.py
  ./mcp_servers/document/adr/validator.py
  ./mcp_servers/lib/
  ./mcp_servers/lib/__init__.py
  ./mcp_servers/lib/git/
  ./mcp_servers/lib/git/__init__.py
  ./mcp_servers/lib/git/git_ops.py
  ./mcp_servers/lib/utils/
  ./mcp_servers/lib/utils/__init__.py
  ./mcp_servers/lib/utils/env_helper.py
  ./mcp_servers/orchestrator/
  ./mcp_servers/orchestrator/__init__.py
  ./mcp_servers/orchestrator/command.json
  ./mcp_servers/orchestrator/config/
  ./mcp_servers/orchestrator/config/__init__.py
  ./mcp_servers/orchestrator/config/mcp_config.json
  ./mcp_servers/orchestrator/schemas/
  ./mcp_servers/orchestrator/schemas/__init__.py
  ./mcp_servers/orchestrator/server.py
  ./mcp_servers/orchestrator/tools/
  ./mcp_servers/orchestrator/tools/__init__.py
  ./mcp_servers/orchestrator/tools/cognitive.py
  ./mcp_servers/orchestrator/tools/mechanical.py
  ./mcp_servers/orchestrator/tools/query.py
  ./mcp_servers/orchestrator/tools/safety.py
  ./mcp_servers/orchestrator/tools/utils.py
  ./mcp_servers/protocol/
  ./mcp_servers/protocol/Dockerfile
  ./mcp_servers/protocol/README.md
  ./mcp_servers/protocol/__init__.py
  ./mcp_servers/protocol/models.py
  ./mcp_servers/protocol/operations.py
  ./mcp_servers/protocol/requirements.txt
  ./mcp_servers/protocol/server.py
  ./mcp_servers/protocol/validator.py
  ./mcp_servers/requirements.txt
  ./mcp_servers/start_mcp_servers.sh
  ./mcp_servers/system/
  ./mcp_servers/system/__init__.py
  ./mcp_servers/system/forge/
  ./mcp_servers/system/forge/README.md
  ./mcp_servers/system/forge/__init__.py
  ./mcp_servers/system/forge/models.py
  ./mcp_servers/system/forge/operations.py
  ./mcp_servers/system/forge/server.py
  ./mcp_servers/system/forge/test_forge.py
  ./mcp_servers/system/forge/validator.py
  ./mcp_servers/system/git_workflow/
  ./mcp_servers/system/git_workflow/Dockerfile
  ./mcp_servers/system/git_workflow/README.md
  ./mcp_servers/system/git_workflow/__init__.py
  ./mcp_servers/system/git_workflow/requirements.txt
  ./mcp_servers/system/git_workflow/server.py
  ./mcp_servers/task/
  ./mcp_servers/task/Dockerfile
  ./mcp_servers/task/README.md
  ./mcp_servers/task/__init__.py
  ./mcp_servers/task/models.py
  ./mcp_servers/task/operations.py
  ./mcp_servers/task/requirements.txt
  ./mcp_servers/task/server.py
  ./mcp_servers/task/validator.py

--- START OF FILE README.md ---

# Project Sanctuary MCP Servers

This directory contains the "Core Quad" of Model Context Protocol (MCP) servers that power the Project Sanctuary nervous system.

## Core Quad Servers

1. **Cortex (`project_sanctuary.cognitive.cortex`)**
   - **Purpose:** Memory, RAG, and Knowledge Retrieval.
   - **Tools:** `cortex_query`, `cortex_ingest_full`, `cortex_ingest_incremental`, `cortex_get_stats`.
   - **Location:** `mcp_servers/cognitive/cortex/server.py`

2. **Chronicle (`project_sanctuary.chronicle`)**
   - **Purpose:** History, Logging, and Sequential Records.
   - **Tools:** `chronicle_create_entry`, `chronicle_read_latest_entries`, `chronicle_append_entry`, `chronicle_search`.
   - **Location:** `mcp_servers/chronicle/server.py`

3. **Protocol (`project_sanctuary.protocol`)**
   - **Purpose:** Law, Validation, and Governance.
   - **Tools:** `protocol_get`, `protocol_list`, `protocol_validate_action`, `protocol_search`.
   - **Location:** `mcp_servers/protocol/server.py`

4. **Orchestrator (`project_sanctuary.orchestrator`)**
   - **Purpose:** High-level Planning and Council Logic.
   - **Tools:** `orchestrator_consult_strategist`, `orchestrator_consult_auditor`, `orchestrator_dispatch_mission`.
   - **Location:** `mcp_servers/orchestrator/server.py`

## Configuration

To use these servers with an MCP client (like Claude Desktop), add the following to your configuration file (e.g., `claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "cortex": {
      "command": "python",
      "args": ["/absolute/path/to/Project_Sanctuary/mcp_servers/cognitive/cortex/server.py"],
      "env": {
        "PROJECT_ROOT": "/absolute/path/to/Project_Sanctuary"
      }
    },
    "chronicle": {
      "command": "python",
      "args": ["/absolute/path/to/Project_Sanctuary/mcp_servers/chronicle/server.py"],
      "env": {
        "PROJECT_ROOT": "/absolute/path/to/Project_Sanctuary"
      }
    },
    "protocol": {
      "command": "python",
      "args": ["/absolute/path/to/Project_Sanctuary/mcp_servers/protocol/server.py"],
      "env": {
        "PROJECT_ROOT": "/absolute/path/to/Project_Sanctuary"
      }
    },
    "orchestrator": {
      "command": "python",
      "args": ["/absolute/path/to/Project_Sanctuary/mcp_servers/orchestrator/server.py"],
      "env": {
        "PROJECT_ROOT": "/absolute/path/to/Project_Sanctuary"
      }
    }
  }
}
```

## Running Manually

You can use the helper script to verify paths:
```bash
./start_mcp_servers.sh
```

--- END OF FILE README.md ---

--- START OF FILE __init__.py ---



--- END OF FILE __init__.py ---

--- START OF FILE chronicle/README.md ---

# Chronicle MCP Server

MCP server for managing historical truth entries in `00_CHRONICLE/ENTRIES/`.

## Purpose

The Chronicle MCP ensures the integrity of the project's historical record. It enforces strict rules about immutability and classification to maintain a trusted history of events, decisions, and milestones.

## Tools

### `chronicle_create_entry`
Create a new chronicle entry.
- **Args:** `title`, `content`, `date` (optional), `author` (optional), `status` (optional), `classification` (optional)
- **Returns:** Entry number and file path

### `chronicle_update_entry`
Update an existing entry.
- **Args:** `entry_number`, `updates` (dict), `reason`, `override_approval_id` (optional)
- **Returns:** Updated fields
- **Safety:** Entries older than 7 days require `override_approval_id` to be modified.

### `chronicle_get_entry`
Retrieve a specific entry.
- **Args:** `entry_number`
- **Returns:** Entry details

### `chronicle_list_entries`
List recent entries.
- **Args:** `limit` (optional, default 10)
- **Returns:** List of entries

### `chronicle_search`
Search entries by content.
- **Args:** `query`
- **Returns:** List of matching entries

## Safety Rules

1.  **7-Day Modification Window:** Entries older than 7 days are considered immutable history. Modifying them requires an explicit `override_approval_id`.
2.  **Sequential Numbering:** Entry numbers are auto-assigned and sequential.
3.  **No Deletion:** Entries can be marked as `deprecated` but never deleted.
4.  **Classification:** Entries must be classified as `public`, `internal`, or `confidential`.

## Configuration

Add to `mcp_config.json`:

```json
"chronicle": {
  "displayName": "Chronicle MCP",
  "command": "/path/to/venv/bin/python",
  "args": ["-m", "mcp_servers.document.chronicle.server"],
  "env": {
    "PROJECT_ROOT": "/path/to/project",
    "PYTHONPATH": "/path/to/project"
  }
}
```

--- END OF FILE chronicle/README.md ---

--- START OF FILE chronicle/__init__.py ---



--- END OF FILE chronicle/__init__.py ---

--- START OF FILE chronicle/models.py ---

"""
Data models for the Chronicle MCP server.
"""
from dataclasses import dataclass
from enum import Enum
from typing import Optional
from datetime import date


class ChronicleStatus(str, Enum):
    DRAFT = "draft"
    PUBLISHED = "published"
    CANONICAL = "canonical"
    DEPRECATED = "deprecated"


class ChronicleClassification(str, Enum):
    PUBLIC = "public"
    INTERNAL = "internal"
    CONFIDENTIAL = "confidential"


@dataclass
class ChronicleEntry:
    entry_number: int
    title: str
    date: date
    author: str
    content: str
    status: ChronicleStatus = ChronicleStatus.DRAFT
    classification: ChronicleClassification = ChronicleClassification.INTERNAL
    
    @property
    def filename(self) -> str:
        """Generate filename for the entry."""
        # Format: 001_title_slug.md
        slug = self.title.lower().replace(" ", "_").replace("-", "_")
        # Remove non-alphanumeric chars except underscore
        slug = "".join(c for c in slug if c.isalnum() or c == "_")
        return f"{self.entry_number:03d}_{slug}.md"


CHRONICLE_TEMPLATE = """# Living Chronicle - Entry {number}

**Title:** {title}
**Date:** {date}
**Author:** {author}
**Status:** {status}
**Classification:** {classification}

---

{content}
"""

--- END OF FILE chronicle/models.py ---

--- START OF FILE chronicle/operations.py ---

"""
File operations for Chronicle MCP.
"""
import os
import re
from datetime import date, datetime
from typing import List, Optional, Dict, Any
from .models import ChronicleEntry, ChronicleStatus, ChronicleClassification, CHRONICLE_TEMPLATE
from .validator import ChronicleValidator


class ChronicleOperations:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.validator = ChronicleValidator(base_dir)
        
        # Ensure directory exists
        if not os.path.exists(base_dir):
            os.makedirs(base_dir)

    def create_entry(
        self,
        title: str,
        content: str,
        author: str,
        date_str: Optional[str] = None,
        status: str = "draft",
        classification: str = "internal"
    ) -> Dict[str, Any]:
        """Create a new chronicle entry."""
        # Validate inputs
        self.validator.validate_required_fields(title, content, author)
        
        # Determine number
        number = self.validator.get_next_entry_number()
        self.validator.validate_entry_number(number)
        
        # Parse date
        entry_date = date.fromisoformat(date_str) if date_str else date.today()
        
        # Create entry object
        entry = ChronicleEntry(
            entry_number=number,
            title=title,
            date=entry_date,
            author=author,
            content=content,
            status=ChronicleStatus(status),
            classification=ChronicleClassification(classification)
        )
        
        # Generate content
        file_content = CHRONICLE_TEMPLATE.format(
            number=entry.entry_number,
            title=entry.title,
            date=entry.date.isoformat(),
            author=entry.author,
            status=entry.status.value,
            classification=entry.classification.value,
            content=entry.content
        )
        
        # Write file
        file_path = os.path.join(self.base_dir, entry.filename)
        with open(file_path, "w") as f:
            f.write(file_content)
            
        return {
            "entry_number": number,
            "file_path": file_path,
            "status": entry.status.value
        }

    def update_entry(
        self,
        entry_number: int,
        updates: Dict[str, Any],
        reason: str,
        override_approval_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Update an existing chronicle entry."""
        # Find file
        file_path = self._find_entry_file(entry_number)
        if not file_path:
            raise ValueError(f"Entry {entry_number} not found")
            
        # Validate modification window
        self.validator.validate_modification_window(file_path, override_approval_id)
        
        # Read existing
        current_entry = self.get_entry(entry_number)
        
        # Apply updates
        # Note: In a real implementation, we'd need to parse the markdown back into an object
        # For now, we'll re-generate the file with updated fields
        
        new_title = updates.get("title", current_entry["title"])
        new_content = updates.get("content", current_entry["content"])
        new_status = updates.get("status", current_entry["status"])
        new_classification = updates.get("classification", current_entry["classification"])
        
        # Re-generate content
        file_content = CHRONICLE_TEMPLATE.format(
            number=entry_number,
            title=new_title,
            date=current_entry["date"],
            author=current_entry["author"], # Author usually doesn't change
            status=new_status,
            classification=new_classification,
            content=new_content
        )
        
        # Write file
        with open(file_path, "w") as f:
            f.write(file_content)
            
        # If title changed, we might need to rename the file, but let's keep it simple for now
        # and only update content. Renaming would break links.
        
        return {
            "entry_number": entry_number,
            "updated_fields": list(updates.keys())
        }

    def get_entry(self, entry_number: int) -> Dict[str, Any]:
        """Retrieve a chronicle entry."""
        file_path = self._find_entry_file(entry_number)
        if not file_path:
            raise ValueError(f"Entry {entry_number} not found")
            
        with open(file_path, "r") as f:
            content = f.read()
            
        return self._parse_entry(content, entry_number)

    def list_entries(self, limit: int = 10) -> List[Dict[str, Any]]:
        """List recent chronicle entries."""
        if not os.path.exists(self.base_dir):
            return []
            
        files = sorted(os.listdir(self.base_dir), reverse=True)
        entries = []
        
        for f in files:
            if not f.endswith(".md"):
                continue
                
            match = re.match(r"(\d{3})_", f)
            if match:
                number = int(match.group(1))
                try:
                    entries.append(self.get_entry(number))
                except Exception:
                    continue # Skip malformed
                    
            if len(entries) >= limit:
                break
                
        return entries

    def search_entries(self, query: str) -> List[Dict[str, Any]]:
        """Search chronicle entries."""
        if not os.path.exists(self.base_dir):
            return []
            
        results = []
        files = sorted(os.listdir(self.base_dir))
        
        for f in files:
            if not f.endswith(".md"):
                continue
                
            path = os.path.join(self.base_dir, f)
            with open(path, "r") as file:
                content = file.read()
                
            if query.lower() in content.lower():
                match = re.match(r"(\d{3})_", f)
                if match:
                    number = int(match.group(1))
                    results.append(self._parse_entry(content, number))
                    
        return results

    def _find_entry_file(self, number: int) -> Optional[str]:
        """Find file path for an entry number."""
        if not os.path.exists(self.base_dir):
            return None
            
        for f in os.listdir(self.base_dir):
            if f.startswith(f"{number:03d}_"):
                return os.path.join(self.base_dir, f)
        return None

    def _parse_entry(self, content: str, number: int) -> Dict[str, Any]:
        """Parse markdown content into entry dict."""
        # Simple parsing logic
        # Extract metadata from lines
        lines = content.split("\n")
        metadata = {}
        body_start = 0
        
        for i, line in enumerate(lines):
            if line.startswith("**Title:**"):
                metadata["title"] = line.replace("**Title:**", "").strip()
            elif line.startswith("**Date:**"):
                metadata["date"] = line.replace("**Date:**", "").strip()
            elif line.startswith("**Author:**"):
                metadata["author"] = line.replace("**Author:**", "").strip()
            elif line.startswith("**Status:**"):
                metadata["status"] = line.replace("**Status:**", "").strip()
            elif line.startswith("**Classification:**"):
                metadata["classification"] = line.replace("**Classification:**", "").strip()
            elif line.strip() == "---":
                body_start = i + 1
                break
        
        # Fallback for older formats if title not found in metadata
        if "title" not in metadata:
             # Try to find H1 or H3
             for line in lines:
                 if line.startswith("# "):
                     metadata["title"] = line.replace("# ", "").replace("Living Chronicle - Entry " + str(number), "").strip()
                     break
                 elif line.startswith("### **Entry"):
                     # Format: ### **Entry 001: The Genesis...**
                     parts = line.split(":")
                     if len(parts) > 1:
                         metadata["title"] = parts[1].replace("**", "").strip()
                     break

        return {
            "number": number,
            "title": metadata.get("title", "Unknown Title"),
            "date": metadata.get("date", ""),
            "author": metadata.get("author", ""),
            "status": metadata.get("status", "draft"),
            "classification": metadata.get("classification", "internal"),
            "content": "\n".join(lines[body_start:]).strip() if body_start > 0 else content
        }

--- END OF FILE chronicle/operations.py ---

--- START OF FILE chronicle/requirements.txt ---

fastmcp

--- END OF FILE chronicle/requirements.txt ---

--- START OF FILE chronicle/server.py ---

from fastmcp import FastMCP
import os
from typing import Optional, List, Dict, Any
from .operations import ChronicleOperations

# Initialize FastMCP
mcp = FastMCP("project_sanctuary.chronicle")

# Configuration
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")
CHRONICLE_DIR = os.path.join(PROJECT_ROOT, "00_CHRONICLE/ENTRIES")

# Initialize operations
ops = ChronicleOperations(CHRONICLE_DIR)


@mcp.tool()
def chronicle_create_entry(
    title: str,
    content: str,
    author: str,
    date: Optional[str] = None,
    status: str = "draft",
    classification: str = "internal"
) -> str:
    """
    Create a new chronicle entry.
    
    Args:
        title: Entry title
        content: Entry content (markdown)
        author: Author name/ID
        date: Date string (YYYY-MM-DD), defaults to today
        status: draft, published, canonical, deprecated
        classification: public, internal, confidential
    """
    try:
        result = ops.create_entry(title, content, author, date, status, classification)
        return f"Created Chronicle Entry {result['entry_number']}: {result['file_path']}"
    except Exception as e:
        return f"Error creating entry: {str(e)}"


@mcp.tool()
def chronicle_append_entry(
    title: str,
    content: str,
    author: str,
    date: Optional[str] = None,
    status: str = "draft",
    classification: str = "internal"
) -> str:
    """
    Append a new entry to the Chronicle (Alias for create_entry).
    
    Args:
        title: Entry title
        content: Entry content
        author: Author name
        date: Date string
        status: Status
        classification: Classification
    """
    return chronicle_create_entry(title, content, author, date, status, classification)


@mcp.tool()
def chronicle_update_entry(
    entry_number: int,
    updates: Dict[str, Any],
    reason: str,
    override_approval_id: Optional[str] = None
) -> str:
    """
    Update an existing chronicle entry.
    
    Args:
        entry_number: The entry number to update
        updates: Dictionary of fields to update (title, content, status, classification)
        reason: Reason for the update
        override_approval_id: Required if entry is older than 7 days
    """
    try:
        result = ops.update_entry(entry_number, updates, reason, override_approval_id)
        return f"Updated Chronicle Entry {result['entry_number']}. Fields: {', '.join(result['updated_fields'])}"
    except Exception as e:
        return f"Error updating entry: {str(e)}"


@mcp.tool()
def chronicle_get_entry(entry_number: int) -> str:
    """
    Retrieve a specific chronicle entry.
    
    Args:
        entry_number: The entry number to retrieve
    """
    try:
        entry = ops.get_entry(entry_number)
        return f"""Entry {entry['number']}: {entry['title']}
Date: {entry['date']}
Author: {entry['author']}
Status: {entry['status']}
Classification: {entry['classification']}

{entry['content']}"""
    except Exception as e:
        return f"Error retrieving entry: {str(e)}"


@mcp.tool()
def chronicle_list_entries(limit: int = 10) -> str:
    """
    List recent chronicle entries.
    
    Args:
        limit: Maximum number of entries to return (default 10)
    """
    try:
        entries = ops.list_entries(limit)
        if not entries:
            return "No entries found."
            
        output = [f"Found {len(entries)} recent entries:"]
        for e in entries:
            output.append(f"- {e['number']:03d}: {e['title']} [{e['status']}] ({e['date']})")
        return "\n".join(output)
    except Exception as e:
        return f"Error listing entries: {str(e)}"


@mcp.tool()
def chronicle_read_latest_entries(limit: int = 10) -> str:
    """
    Read the latest entries from the Chronicle (Alias for list_entries).
    
    Args:
        limit: Number of entries to read
    """
    return chronicle_list_entries(limit)


@mcp.tool()
def chronicle_search(query: str) -> str:
    """
    Search chronicle entries by content.
    
    Args:
        query: Search query string
    """
    try:
        results = ops.search_entries(query)
        if not results:
            return f"No entries found matching '{query}'"
            
        output = [f"Found {len(results)} entries matching '{query}':"]
        for r in results:
            output.append(f"- {r['number']:03d}: {r['title']}")
        return "\n".join(output)
    except Exception as e:
        return f"Error searching entries: {str(e)}"


if __name__ == "__main__":
    mcp.run()

--- END OF FILE chronicle/server.py ---

--- START OF FILE chronicle/validator.py ---

"""
Validation logic for Chronicle MCP.
"""
import os
import re
from datetime import datetime, date, timedelta
from typing import Optional
from .models import ChronicleStatus, ChronicleClassification

class ChronicleValidator:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir

    def get_next_entry_number(self) -> int:
        """Determine the next sequential entry number."""
        if not os.path.exists(self.base_dir):
            return 1
            
        files = os.listdir(self.base_dir)
        numbers = []
        for f in files:
            match = re.match(r"(\d{3})_", f)
            if match:
                numbers.append(int(match.group(1)))
        
        return max(numbers) + 1 if numbers else 1

    def validate_entry_number(self, number: int) -> None:
        """Ensure entry number is unique for creation."""
        if not os.path.exists(self.base_dir):
            return
            
        files = os.listdir(self.base_dir)
        for f in files:
            if f.startswith(f"{number:03d}_"):
                raise ValueError(f"Entry {number} already exists: {f}")

    def validate_modification_window(self, file_path: str, override_approval_id: Optional[str] = None) -> None:
        """
        Enforce 7-day modification window.
        Entries older than 7 days cannot be modified without override.
        """
        if not os.path.exists(file_path):
            return  # New file, always allowed
            
        # Check file creation/modification time or parse date from content
        # Using file modification time as a proxy for "age of entry" in filesystem
        # In a real system, we might parse the date from the file content
        
        stats = os.stat(file_path)
        last_mod = datetime.fromtimestamp(stats.st_mtime)
        age = datetime.now() - last_mod
        
        if age > timedelta(days=7):
            if not override_approval_id:
                raise ValueError(
                    f"Entry is {age.days} days old (limit: 7 days). "
                    "Modification requires 'override_approval_id'."
                )

    def validate_required_fields(self, title: str, content: str, author: str) -> None:
        """Validate that required fields are present and not empty."""
        if not title or not title.strip():
            raise ValueError("Title is required")
        if not content or not content.strip():
            raise ValueError("Content is required")
        if not author or not author.strip():
            raise ValueError("Author is required")

--- END OF FILE chronicle/validator.py ---

--- START OF FILE cognitive/__init__.py ---



--- END OF FILE cognitive/__init__.py ---

--- START OF FILE cognitive/cortex/README.md ---

# Cortex MCP Server

**Domain:** `project_sanctuary.cognitive.cortex`  
**Version:** 1.0.0  
**Status:** Phase 1 - Foundation

## Overview

The Cortex MCP Server provides Model Context Protocol (MCP) tools for interacting with the Mnemonic Cortex RAG (Retrieval-Augmented Generation) system. It exposes the knowledge base for semantic search and document ingestion.

## Architecture

This server wraps existing Mnemonic Cortex scripts and services:

- **Ingestion:** `mnemonic_cortex/scripts/ingest.py` and `ingest_incremental.py`
- **Query:** `mnemonic_cortex/app/services/vector_db_service.py` (Parent Document Retriever)
- **Stats:** Direct ChromaDB collection access

## Tools

### 1. `cortex_ingest_full`

Perform full re-ingestion of the knowledge base.

**Parameters:**
- `purge_existing` (bool, default: True): Whether to purge existing database
- `source_directories` (List[str], optional): Directories to ingest

**Returns:**
```json
{
  "documents_processed": 459,
  "chunks_created": 2145,
  "ingestion_time_ms": 45230.5,
  "vectorstore_path": "/path/to/chroma_db",
  "status": "success"
}
```

**Example:**
```python
cortex_ingest_full()
cortex_ingest_full(source_directories=["01_PROTOCOLS", "00_CHRONICLE"])
```

---

### 2. `cortex_query`

Perform semantic search query against the knowledge base.

**Parameters:**
- `query` (str): Natural language query
- `max_results` (int, default: 5): Maximum results (1-100)
- `use_cache` (bool, default: False): Use cache (Phase 2)

**Returns:**
```json
{
  "results": [
    {
      "content": "Full parent document content...",
      "metadata": {
        "source_file": "01_PROTOCOLS/101_protocol.md"
      }
    }
  ],
  "query_time_ms": 234.5,
  "cache_hit": false,
  "status": "success"
}
```

**Example:**
```python
cortex_query("What is Protocol 101?")
cortex_query("Explain the Mnemonic Cortex", max_results=3)
```

---

### 3. `cortex_get_stats`

Get database statistics and health status.

**Parameters:** None

**Returns:**
```json
{
  "total_documents": 459,
  "total_chunks": 2145,
  "collections": {
    "child_chunks": {"count": 2145, "name": "child_chunks_v5"},
    "parent_documents": {"count": 459, "name": "parent_documents_v5"}
  },
  "health_status": "healthy"
}
```

**Example:**
```python
cortex_get_stats()
```

---

### 4. `cortex_ingest_incremental`

Incrementally ingest documents without rebuilding the database.

**Parameters:**
- `file_paths` (List[str]): Markdown files to ingest
- `metadata` (dict, optional): Metadata to attach
- `skip_duplicates` (bool, default: True): Skip existing files

**Returns:**
```json
{
  "documents_added": 3,
  "chunks_created": 15,
  "skipped_duplicates": 1,
  "status": "success"
}
```

**Example:**
```python
cortex_ingest_incremental(["00_CHRONICLE/2025-11-28_entry.md"])
cortex_ingest_incremental(
    file_paths=["01_PROTOCOLS/120_new.md"],
    skip_duplicates=False
)
```

## Installation

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Configure MCP server in `~/.gemini/antigravity/mcp_config.json`:
```json
{
  "mcpServers": {
    "cortex": {
      "command": "python",
      "args": ["-m", "mcp_servers.cognitive.cortex.server"],
      "cwd": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
      "env": {
        "PROJECT_ROOT": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
      }
    }
  }
}
```

3. Restart Antigravity

## Usage

From Antigravity or any MCP client:

```
# Get database stats
cortex_get_stats()

# Query the knowledge base
cortex_query("What is Protocol 101?")

# Add a new document
cortex_ingest_incremental(["path/to/new_document.md"])

# Full re-ingestion (use with caution)
cortex_ingest_full()
```

## Safety Rules

1. **Read-Only by Default:** Query operations are read-only
2. **Ingestion Confirmation:** Full ingestion purges existing data
3. **Long-Running Operations:** Ingestion may take several minutes
4. **Rate Limiting:** Max 100 queries/minute recommended
5. **Validation:** All inputs are validated before processing

## Phase 2 Features (Upcoming)

- Cache integration (`use_cache` parameter)
- Guardian Wakeup tool (Protocol 114)
- Cache warmup and invalidation
- Cache statistics

## Dependencies

- **ChromaDB:** Vector database
- **LangChain:** RAG framework
- **NomicEmbeddings:** Local embedding model
- **FastMCP:** MCP server framework

## Related Documentation

- `mnemonic_cortex/VISION.md` - RAG vision and purpose
- `mnemonic_cortex/RAG_STRATEGIES_AND_DOCTRINE.md` - Architecture details
- `01_PROTOCOLS/85_The_Mnemonic_Cortex_Protocol.md` - Protocol specification
- `01_PROTOCOLS/114_Guardian_Wakeup_and_Cache_Prefill.md` - Cache prefill spec

## Version History

- **1.0.0** (2025-11-28): Phase 1 - Foundation
  - 4 core tools: ingest_full, query, get_stats, ingest_incremental
  - Parent Document Retriever integration
  - Input validation and error handling

--- END OF FILE cognitive/cortex/README.md ---

--- START OF FILE cognitive/cortex/TEST_RESULTS.md ---

# Cortex MCP Integration Test Results

**Date:** 2025-11-28  
**Test Suite:** `test_cortex_integration.py`

## Test Results Summary

| Test | Status | Notes |
|------|--------|-------|
| `cortex_get_stats` | ✅ PASS | 463 documents, 7671 chunks, healthy status |
| `cortex_query` | ✅ PASS | All 3 queries successful, results validated |
| `cortex_ingest_incremental` | ✅ PASS | Document ingested and searchable |
| `cortex_ingest_full` | ⏭️ SKIPPED | Slow test, skipped by default |

**Overall:** 3/3 core tests passing ✅

## Detailed Results

### cortex_get_stats ✅
- Retrieved in 1.81s
- **Health:** healthy
- **Documents:** 463
- **Chunks:** 7671
- All validation checks passed

### cortex_query ✅
- **Query 1:** "What is Protocol 101?" → 3 results in 5.16s
- **Query 2:** "Covenant of Grace chronicle entry" → 2 results in 0.02s  
  - Successfully retrieved Entry 015 with full content
- **Query 3:** "Mnemonic Cortex architecture" → 2 results in 0.02s

### cortex_ingest_incremental ✅
- Created temporary test document
- Ingested in 0.22s
- Added 1 document, 2 chunks
- Verified searchable via `cortex_query`
- Automatic cleanup successful

## Conclusion

✅ **All 3 Cortex MCP tools tested and passing!**

The integration test suite successfully validates:
1. **Stats functionality** - Database health monitoring working correctly
2. **Query functionality** - Multiple test cases with different queries
3. **Incremental ingestion** - Document ingestion with automatic verification

All tools are production-ready and fully functional.

## Bug Fix

**Issue:** Stats test was failing with "Database not found"  
**Root Cause:** Project root path calculation was incorrect (used 4 parent levels instead of 5)  
**Fix:** Updated path calculation in test file from `.parent.parent.parent.parent` to `.parent.parent.parent.parent.parent`  
**Result:** All 3 tests now pass ✅

## Next Steps

1. ✅ MCP server code complete
2. ✅ Integration tests passing (3/3)
3. ✅ MCP configs updated
4. ⏸️ User needs to restart Antigravity to test MCP tools live

--- END OF FILE cognitive/cortex/TEST_RESULTS.md ---

--- START OF FILE cognitive/cortex/__init__.py ---

"""
Cortex MCP Server - Mnemonic Cortex RAG Interface

Provides MCP tools for interacting with the Mnemonic Cortex RAG system.
"""

__version__ = "1.0.0"

--- END OF FILE cognitive/cortex/__init__.py ---

--- START OF FILE cognitive/cortex/mcp_config_example.json ---

{
    "mcpServers": {
        "cortex": {
            "command": "python3",
            "args": [
                "-m",
                "mcp_servers.cognitive.cortex.server"
            ],
            "cwd": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
            "env": {
                "PROJECT_ROOT": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
            }
        }
    }
}

--- END OF FILE cognitive/cortex/mcp_config_example.json ---

--- START OF FILE cognitive/cortex/models.py ---

"""
Cortex MCP Server - Data Models

Pydantic models for RAG operations in the Mnemonic Cortex.
"""
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from datetime import datetime


# ============================================================================
# Ingest Full Models
# ============================================================================

@dataclass
class IngestFullRequest:
    """Request model for full ingestion."""
    purge_existing: bool = True
    source_directories: Optional[List[str]] = None


@dataclass
class IngestFullResponse:
    """Response model for full ingestion."""
    documents_processed: int
    chunks_created: int
    ingestion_time_ms: float
    vectorstore_path: str
    status: str  # "success" or "error"
    error: Optional[str] = None


# ============================================================================
# Query Models
# ============================================================================

@dataclass
class QueryRequest:
    """Request model for RAG query."""
    query: str
    max_results: int = 5
    use_cache: bool = False  # Phase 2 feature


@dataclass
class QueryResult:
    """Individual query result."""
    content: str
    metadata: Dict[str, Any]
    relevance_score: Optional[float] = None


@dataclass
class QueryResponse:
    """Response model for RAG query."""
    results: List[QueryResult]
    query_time_ms: float
    status: str  # "success" or "error"
    cache_hit: bool = False  # Phase 2 feature
    error: Optional[str] = None


# ============================================================================
# Stats Models
# ============================================================================

@dataclass
class CollectionStats:
    """Statistics for a single collection."""
    count: int
    name: str


@dataclass
class StatsResponse:
    """Response model for database statistics."""
    total_documents: int
    total_chunks: int
    collections: Dict[str, CollectionStats]
    health_status: str  # "healthy", "degraded", or "error"
    cache_stats: Optional[Dict[str, Any]] = None  # Phase 2 feature
    error: Optional[str] = None


# ============================================================================
# Ingest Incremental Models
# ============================================================================

@dataclass
class IngestIncrementalRequest:
    """Request model for incremental ingestion."""
    file_paths: List[str]
    metadata: Optional[Dict[str, Any]] = None
    skip_duplicates: bool = True


@dataclass
class IngestIncrementalResponse:
    """Response model for incremental ingestion."""
    documents_added: int
    chunks_created: int
    skipped_duplicates: int
    status: str  # "success" or "error"
    error: Optional[str] = None


# ============================================================================
# Cache Operation Models (Protocol 114 - Guardian Wakeup)
# ============================================================================

@dataclass
class CacheGetResponse:
    """Response from cache retrieval operation."""
    cache_hit: bool
    answer: Optional[str]
    query_time_ms: float
    status: str  # "success" or "error"
    error: Optional[str] = None


@dataclass
class CacheSetResponse:
    """Response from cache storage operation."""
    cache_key: str
    stored: bool
    status: str  # "success" or "error"
    error: Optional[str] = None


@dataclass
class CacheWarmupResponse:
    """Response from cache warmup operation."""
    queries_cached: int
    cache_hits: int
    cache_misses: int
    total_time_ms: float
    status: str  # "success" or "error"
    error: Optional[str] = None


@dataclass
class GuardianWakeupResponse:
    """Response from Guardian wakeup digest generation."""
    digest_path: str
    bundles_loaded: List[str]
    cache_hits: int
    cache_misses: int
    total_time_ms: float
    status: str  # "success" or "error"
    error: Optional[str] = None


# ============================================================================
# Helper Functions
# ============================================================================

def to_dict(obj: Any) -> Dict[str, Any]:
    """Convert dataclass to dictionary."""
    if hasattr(obj, '__dataclass_fields__'):
        result = {}
        for field_name in obj.__dataclass_fields__:
            value = getattr(obj, field_name)
            if isinstance(value, list):
                result[field_name] = [to_dict(item) if hasattr(item, '__dataclass_fields__') else item for item in value]
            elif isinstance(value, dict):
                result[field_name] = {k: to_dict(v) if hasattr(v, '__dataclass_fields__') else v for k, v in value.items()}
            elif hasattr(value, '__dataclass_fields__'):
                result[field_name] = to_dict(value)
            else:
                result[field_name] = value
        return result
    return obj

--- END OF FILE cognitive/cortex/models.py ---

--- START OF FILE cognitive/cortex/operations.py ---

"""
Cortex MCP Server - Core Operations

Wraps existing Mnemonic Cortex scripts as MCP operations.
"""
import os
import sys
import time
import subprocess
import contextlib
import io
from pathlib import Path
from typing import Dict, Any, List

from .models import (
    IngestFullResponse,
    QueryResponse,
    QueryResult,
    StatsResponse,
    CollectionStats,
    IngestIncrementalResponse,
    to_dict
)


class CortexOperations:
    """Core operations for Cortex MCP server."""
    
    def __init__(self, project_root: str):
        """
        Initialize operations.
        
        Args:
            project_root: Absolute path to project root
        """
        self.project_root = Path(project_root)
        self.scripts_dir = self.project_root / "mnemonic_cortex" / "scripts"
    
    def ingest_full(
        self,
        purge_existing: bool = True,
        source_directories: List[str] = None
    ) -> IngestFullResponse:
        """
        Perform full ingestion of knowledge base.
        
        Wraps: mnemonic_cortex/scripts/ingest.py
        
        Args:
            purge_existing: Whether to purge existing database
            source_directories: Optional list of source directories
            
        Returns:
            IngestFullResponse with statistics
        """
        try:
            # Import and use IngestionService
            sys.path.insert(0, str(self.project_root))
            from mnemonic_cortex.app.services.ingestion_service import IngestionService
            
            service = IngestionService(str(self.project_root))
            result = service.ingest_full(
                purge_existing=purge_existing,
                source_directories=source_directories
            )
            
            if result.get("status") == "error":
                return IngestFullResponse(
                    documents_processed=0,
                    chunks_created=0,
                    ingestion_time_ms=result.get("ingestion_time_ms", 0),
                    vectorstore_path="",
                    status="error",
                    error=result.get("message", "Unknown error")
                )
            
            return IngestFullResponse(
                documents_processed=result.get("documents_processed", 0),
                chunks_created=result.get("chunks_created", 0),
                ingestion_time_ms=result.get("ingestion_time_ms", 0),
                vectorstore_path=result.get("vectorstore_path", ""),
                status="success"
            )
            
        except Exception as e:
            return IngestFullResponse(
                documents_processed=0,
                chunks_created=0,
                ingestion_time_ms=0,
                vectorstore_path="",
                status="error",
                error=str(e)
            )
    
    def query(
        self,
        query: str,
        max_results: int = 5,
        use_cache: bool = False,
        reasoning_mode: bool = False
    ) -> QueryResponse:
        """
        Perform semantic search query.
        
        Uses: mnemonic_cortex RAG infrastructure directly
        
        Args:
            query: Query string
            max_results: Maximum number of results
            use_cache: Whether to use cache (Phase 2)
            reasoning_mode: Whether to use LLM to structure the query
            
        Returns:
            QueryResponse with results
        """
        try:
            start_time = time.time()
            
            # Import RAG services
            sys.path.insert(0, str(self.project_root))
            
            # Cache Check (Phase 3)
            if use_cache:
                try:
                    from mnemonic_cortex.core.cache import get_cache
                    cache = get_cache()
                    # Generate key based on query and parameters
                    cache_key_data = {
                        "query": query,
                        "max_results": max_results,
                        "reasoning_mode": reasoning_mode
                    }
                    cache_key = cache.generate_key(cache_key_data)
                    
                    cached_data = cache.get(cache_key)
                    if cached_data:
                        # Cache Hit
                        elapsed_ms = (time.time() - start_time) * 1000
                        # Reconstruct QueryResult objects from cached data
                        results = []
                        for item in cached_data.get("results", []):
                            results.append(QueryResult(
                                content=item["content"],
                                metadata=item["metadata"],
                                relevance_score=item.get("relevance_score")
                            ))
                            
                        return QueryResponse(
                            results=results,
                            query_time_ms=elapsed_ms,
                            cache_hit=True,
                            status="success"
                        )
                except Exception as e:
                    # Log error but continue with retrieval
                    print(f"[Cortex] Cache read error: {e}")

            # Suppress all stdout/stderr from VectorDBService initialization
            with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
                from mnemonic_cortex.app.services.vector_db_service import VectorDBService
                
                # Initialize service
                db_service = VectorDBService()
                retriever = db_service.get_retriever()
            
            # Handle Reasoning Mode
            final_query = query
            reasoning_metadata = {}
            
            if reasoning_mode:
                try:
                    from mnemonic_cortex.app.services.llm_service import LLMService
                    llm_service = LLMService(str(self.project_root))
                    structured = llm_service.generate_structured_query(query)
                    
                    final_query = structured.get("semantic_query", query)
                    reasoning_metadata = {
                        "original_query": query,
                        "structured_query": structured,
                        "reasoning": structured.get("reasoning")
                    }
                    # TODO: Apply filters if VectorDBService supports them in invoke()
                except Exception as e:
                    # Fallback to raw query on LLM error
                    reasoning_metadata = {"error": f"LLM reasoning failed: {str(e)}"}
            
            # Execute query
            docs = retriever.invoke(final_query)
            
            # Limit results
            docs = docs[:max_results]
            
            elapsed_ms = (time.time() - start_time) * 1000
            
            # Convert to QueryResult objects
            results = []
            results_for_cache = []
            
            for doc in docs:
                # Merge existing metadata with reasoning metadata if present
                meta = doc.metadata.copy()
                if reasoning_metadata:
                    meta["_reasoning"] = str(reasoning_metadata)
                    
                result = QueryResult(
                    content=doc.page_content,
                    metadata=meta,
                    relevance_score=None  # LangChain doesn't provide scores by default
                )
                results.append(result)
                
                # Prepare for cache
                results_for_cache.append({
                    "content": result.content,
                    "metadata": result.metadata,
                    "relevance_score": result.relevance_score
                })
            
            # Cache Set (Phase 3)
            if use_cache and results:
                try:
                    cache.set(cache_key, {"results": results_for_cache})
                except Exception as e:
                    print(f"[Cortex] Cache write error: {e}")
            
            return QueryResponse(
                results=results,
                query_time_ms=elapsed_ms,
                cache_hit=False,  # Phase 2 feature
                status="success"
            )
            
        except Exception as e:
            return QueryResponse(
                results=[],
                query_time_ms=0,
                cache_hit=False,
                status="error",
                error=str(e)
            )
    
    def get_stats(self) -> StatsResponse:
        """
        Get database statistics and health status.
        
        Uses: ChromaDB collections directly
        
        Returns:
            StatsResponse with statistics
        """
        try:
            # Import required modules
            sys.path.insert(0, str(self.project_root))
            from langchain_community.vectorstores import Chroma
            from langchain_nomic import NomicEmbeddings
            from dotenv import load_dotenv
            
            # Load environment
            load_dotenv(dotenv_path=self.project_root / ".env")
            
            # Get database paths
            db_path = os.getenv("DB_PATH", "chroma_db")
            chroma_root_env = os.getenv("CHROMA_ROOT", "").strip()
            
            if chroma_root_env:
                chroma_root = Path(chroma_root_env) if Path(chroma_root_env).is_absolute() else (self.project_root / chroma_root_env)
            else:
                chroma_root = self.project_root / "mnemonic_cortex" / db_path
            
            chroma_root = chroma_root.resolve()
            
            child_collection = os.getenv("CHROMA_CHILD_COLLECTION", "child_chunks_v5")
            parent_collection = os.getenv("CHROMA_PARENT_STORE", "parent_documents_v5")
            
            # Check if database exists
            if not chroma_root.exists():
                return StatsResponse(
                    total_documents=0,
                    total_chunks=0,
                    collections={},
                    health_status="error",
                    error="Database not found"
                )
            
            # Initialize embedding model
            embedding_model = NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local")
            
            # Get child chunks stats
            child_path = chroma_root / child_collection
            child_count = 0
            if child_path.exists():
                try:
                    child_db = Chroma(
                        persist_directory=str(child_path),
                        embedding_function=embedding_model,
                        collection_name=child_collection
                    )
                    child_count = child_db._collection.count()
                except Exception as e:
                    pass  # Silently ignore errors for MCP compatibility
            
            # Get parent documents stats
            parent_path = chroma_root / parent_collection
            parent_count = 0
            if parent_path.exists():
                try:
                    # Parent documents are stored in LocalFileStore
                    from langchain_classic.storage import LocalFileStore
                    fs_store = LocalFileStore(root_path=str(parent_path))
                    parent_count = sum(1 for _ in fs_store.yield_keys())
                except Exception as e:
                    pass  # Silently ignore errors for MCP compatibility
            
            # Build collections dict
            collections = {
                "child_chunks": CollectionStats(count=child_count, name=child_collection),
                "parent_documents": CollectionStats(count=parent_count, name=parent_collection)
            }
            
            # Determine health status
            if child_count > 0 and parent_count > 0:
                health_status = "healthy"
            elif child_count > 0 or parent_count > 0:
                health_status = "degraded"
            else:
                health_status = "error"
            
            return StatsResponse(
                total_documents=parent_count,
                total_chunks=child_count,
                collections=collections,
                health_status=health_status
            )
            
        except Exception as e:
            return StatsResponse(
                total_documents=0,
                total_chunks=0,
                collections={},
                health_status="error",
                error=str(e)
            )
    
    def ingest_incremental(
        self,
        file_paths: List[str],
        metadata: Dict[str, Any] = None,
        skip_duplicates: bool = True
    ) -> IngestIncrementalResponse:
        """
        Incrementally ingest files.
        
        Wraps: mnemonic_cortex/scripts/ingest_incremental.py
        
        Args:
            file_paths: List of file paths to ingest
            metadata: Optional metadata to attach
            skip_duplicates: Whether to skip duplicate files
            
        Returns:
            IngestIncrementalResponse with statistics
        """
        try:
            # Import and use IngestionService
            sys.path.insert(0, str(self.project_root))
            from mnemonic_cortex.app.services.ingestion_service import IngestionService
            
            service = IngestionService(str(self.project_root))
            result = service.ingest_incremental(
                file_paths=file_paths,
                skip_duplicates=skip_duplicates
            )
            
            if result.get("error"):
                return IngestIncrementalResponse(
                    documents_added=0,
                    chunks_created=0,
                    skipped_duplicates=0,
                    status="error",
                    error=result.get("error")
                )
            
            return IngestIncrementalResponse(
                documents_added=result.get("added", 0),
                chunks_created=result.get("total_chunks", 0),
                skipped_duplicates=result.get("skipped", 0),
                status="success"
            )
            
        except Exception as e:
            return IngestIncrementalResponse(
                documents_added=0,
                chunks_created=0,
                skipped_duplicates=0,
                ingestion_time_ms=0,
                status="error",
                error=str(e)
            )

    # ========================================================================
    # Cache Operations (Protocol 114 - Guardian Wakeup)
    # ========================================================================

    def cache_get(self, query: str):
        """
        Retrieve answer from cache.
        
        Args:
            query: Query string to look up
            
        Returns:
            CacheGetResponse with cache hit status and answer
        """
        from mnemonic_cortex.core.cache import get_cache
        from .models import CacheGetResponse
        import time
        
        try:
            start = time.time()
            cache = get_cache()
            
            # Generate cache key
            structured_query = {"semantic": query, "filters": {}}
            cache_key = cache.generate_key(structured_query)
            
            # Attempt retrieval
            result = cache.get(cache_key)
            query_time_ms = (time.time() - start) * 1000
            
            if result:
                return CacheGetResponse(
                    cache_hit=True,
                    answer=result.get("answer"),
                    query_time_ms=query_time_ms,
                    status="success"
                )
            else:
                return CacheGetResponse(
                    cache_hit=False,
                    answer=None,
                    query_time_ms=query_time_ms,
                    status="success"
                )
        except Exception as e:
            return CacheGetResponse(
                cache_hit=False,
                answer=None,
                query_time_ms=0,
                status="error",
                error=str(e)
            )

    def cache_set(self, query: str, answer: str):
        """
        Store answer in cache.
        
        Args:
            query: Query string (cache key)
            answer: Answer to cache
            
        Returns:
            CacheSetResponse with storage confirmation
        """
        from mnemonic_cortex.core.cache import get_cache
        from .models import CacheSetResponse
        
        try:
            cache = get_cache()
            structured_query = {"semantic": query, "filters": {}}
            cache_key = cache.generate_key(structured_query)
            
            cache.set(cache_key, {"answer": answer})
            
            return CacheSetResponse(
                cache_key=cache_key,
                stored=True,
                status="success"
            )
        except Exception as e:
            return CacheSetResponse(
                cache_key="",
                stored=False,
                status="error",
                error=str(e)
            )

    def cache_warmup(self, genesis_queries: List[str] = None):
        """
        Pre-populate cache with genesis queries.
        
        Args:
            genesis_queries: List of queries to cache. If None, uses default set.
            
        Returns:
            CacheWarmupResponse with warmup statistics
        """
        from .models import CacheWarmupResponse
        import time
        
        try:
            if genesis_queries is None:
                # Default genesis queries for Guardian
                genesis_queries = [
                    "What is the Anvil Protocol?",
                    "What are the core doctrines of Project Sanctuary?",
                    "How does the Mnemonic Cortex work?",
                    "What is Protocol 87?",
                    "What is Protocol 101?",
                    "What is Protocol 113?",
                    "What is Protocol 114?",
                    "Latest chronicles summary",
                    "Latest protocols summary",
                    "Latest roadmap summary"
                ]
            
            start = time.time()
            cache_hits = 0
            cache_misses = 0
            
            for query in genesis_queries:
                # Check if already cached
                cache_response = self.cache_get(query)
                
                if cache_response.cache_hit:
                    cache_hits += 1
                else:
                    cache_misses += 1
                    # Generate answer and cache it
                    # Note: We use the internal query method, ensuring we don't recurse infinitely
                    # We disable cache usage for the generation step
                    query_response = self.query(query, max_results=3, use_cache=False)
                    if query_response.results:
                        answer = query_response.results[0].content[:1000] # Store reasonable amount
                        self.cache_set(query, answer)
            
            total_time_ms = (time.time() - start) * 1000
            
            return CacheWarmupResponse(
                queries_cached=len(genesis_queries),
                cache_hits=cache_hits,
                cache_misses=cache_misses,
                total_time_ms=total_time_ms,
                status="success"
            )
        except Exception as e:
            return CacheWarmupResponse(
                queries_cached=0,
                cache_hits=0,
                cache_misses=0,
                total_time_ms=0,
                status="error",
                error=str(e)
            )

    def guardian_wakeup(self):
        """
        Generate Guardian boot digest from cache (Protocol 114).
        
        Retrieves chronicles, protocols, and roadmap summaries from cache
        and writes a digest to WORK_IN_PROGRESS/guardian_boot_digest.md.
        
        Returns:
            GuardianWakeupResponse with digest path and statistics
        """
        from .models import GuardianWakeupResponse
        from pathlib import Path
        import time
        
        try:
            start = time.time()
            bundles = ["chronicles", "protocols", "roadmap"]
            cache_hits = 0
            cache_misses = 0
            digest_content = []
            
            # Retrieve each bundle from cache
            for bundle in bundles:
                query = f"Latest {bundle} summary"
                response = self.cache_get(query)
                
                if response.cache_hit:
                    cache_hits += 1
                    digest_content.append(f"## {bundle.title()}\n\n{response.answer}\n")
                else:
                    cache_misses += 1
                    # Fall back to query if not cached
                    query_response = self.query(query, max_results=3, use_cache=False)
                    if query_response.results:
                        answer = query_response.results[0].content[:1000]
                        digest_content.append(f"## {bundle.title()}\n\n{answer}...\n")
                        # Cache for next time
                        self.cache_set(query, answer)
            
            # Write digest
            digest_path = Path(self.project_root) / "WORK_IN_PROGRESS" / "guardian_boot_digest.md"
            digest_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(digest_path, "w") as f:
                f.write("# Guardian Boot Digest\n\n")
                f.write(f"**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("\n".join(digest_content))
            
            total_time_ms = (time.time() - start) * 1000
            
            return GuardianWakeupResponse(
                digest_path=str(digest_path),
                bundles_loaded=bundles,
                cache_hits=cache_hits,
                cache_misses=cache_misses,
                total_time_ms=total_time_ms,
                status="success"
            )
        except Exception as e:
            return GuardianWakeupResponse(
                digest_path="",
                bundles_loaded=[],
                cache_hits=0,
                cache_misses=0,
                total_time_ms=0,
                status="error",
                error=str(e)
            )

    def get_cache_stats(self):
        """
        Get cache statistics.
        
        Returns:
            Dict with cache stats
        """
        from mnemonic_cortex.core.cache import get_cache
        try:
            cache = get_cache()
            return cache.get_stats()
        except Exception as e:
            return {"error": str(e)}

--- END OF FILE cognitive/cortex/operations.py ---

--- START OF FILE cognitive/cortex/requirements.txt ---

fastmcp

--- END OF FILE cognitive/cortex/requirements.txt ---

--- START OF FILE cognitive/cortex/server.py ---

"""
Cortex MCP Server
Domain: project_sanctuary.cognitive.cortex

Provides MCP tools for interacting with the Mnemonic Cortex RAG system.
"""
from fastmcp import FastMCP
from .operations import CortexOperations
from .validator import CortexValidator, ValidationError
from .models import to_dict
import os
import json
from typing import Optional, List

# Initialize FastMCP with canonical domain name
mcp = FastMCP("project_sanctuary.cognitive.cortex")

# Initialize operations and validator
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")
cortex_ops = CortexOperations(PROJECT_ROOT)
cortex_validator = CortexValidator(PROJECT_ROOT)


@mcp.tool()
def cortex_ingest_full(
    purge_existing: bool = True,
    source_directories: Optional[List[str]] = None
) -> str:
    """
    Perform full re-ingestion of the knowledge base.
    
    This operation purges the existing database and rebuilds it from scratch
    by processing all canonical documents. Use with caution.
    
    Args:
        purge_existing: Whether to purge existing database (default: True)
        source_directories: Optional list of source directories to ingest
                          (default: all canonical directories)
    
    Returns:
        JSON string with ingestion statistics
        
    Example:
        cortex_ingest_full()
        cortex_ingest_full(source_directories=["01_PROTOCOLS", "00_CHRONICLE"])
    """
    try:
        # Validate inputs
        validated = cortex_validator.validate_ingest_full(
            purge_existing=purge_existing,
            source_directories=source_directories
        )
        
        # Perform ingestion
        response = cortex_ops.ingest_full(
            purge_existing=validated["purge_existing"],
            source_directories=validated["source_directories"]
        )
        
        # Convert to dict and return as JSON
        result = to_dict(response)
        return json.dumps(result, indent=2)
        
    except ValidationError as e:
        return json.dumps({"status": "error", "error": f"Validation error: {str(e)}"}, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_query(
    query: str,
    max_results: int = 5,
    use_cache: bool = False,
    reasoning_mode: bool = False
) -> str:
    """
    Perform semantic search query against the knowledge base.
    
    Uses the Parent Document Retriever pattern to return full documents
    rather than fragmented chunks, providing complete context.
    
    Args:
        query: Natural language query string
        max_results: Maximum number of results to return (default: 5, max: 100)
        use_cache: Whether to use cache (Phase 2 feature, default: False)
        reasoning_mode: Whether to use LLM to structure the query (default: False)
    
    Returns:
        JSON string with query results and metadata
        
    Example:
        cortex_query("What is Protocol 101?")
        cortex_query("Explain the Mnemonic Cortex architecture", max_results=3, reasoning_mode=True)
    """
    try:
        # Validate inputs
        # Note: We skip validation for reasoning_mode as it's a boolean
        validated = cortex_validator.validate_query(
            query=query,
            max_results=max_results,
            use_cache=use_cache
        )
        
        # Perform query
        response = cortex_ops.query(
            query=validated["query"],
            max_results=validated["max_results"],
            use_cache=validated["use_cache"],
            reasoning_mode=reasoning_mode
        )
        
        # Convert to dict and return as JSON
        result = to_dict(response)
        return json.dumps(result, indent=2)
        
    except ValidationError as e:
        return json.dumps({"status": "error", "error": f"Validation error: {str(e)}"}, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_get_stats() -> str:
    """
    Get database statistics and health status.
    
    Returns information about the number of documents, chunks, collections,
    and overall health of the RAG system.
    
    Returns:
        JSON string with database statistics
        
    Example:
        cortex_get_stats()
    """
    try:
        # Validate (no parameters needed)
        cortex_validator.validate_stats()
        
        # Get stats
        response = cortex_ops.get_stats()
        
        # Convert to dict and return as JSON
        result = to_dict(response)
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_ingest_incremental(
    file_paths: List[str],
    metadata: Optional[dict] = None,
    skip_duplicates: bool = True
) -> str:
    """
    Incrementally ingest documents without rebuilding the entire database.
    
    This operation adds new documents to the existing knowledge base without
    purging existing data. Useful for adding new documents after initial ingestion.
    
    Args:
        file_paths: List of markdown file paths to ingest (absolute or relative)
        metadata: Optional metadata to attach to documents
        skip_duplicates: Whether to skip files already in database (default: True)
    
    Returns:
        JSON string with ingestion statistics
        
    Example:
        cortex_ingest_incremental(["00_CHRONICLE/2025-11-28_new_entry.md"])
        cortex_ingest_incremental(
            file_paths=["01_PROTOCOLS/120_new_protocol.md"],
            skip_duplicates=False
        )
    """
    try:
        # Validate inputs
        validated = cortex_validator.validate_ingest_incremental(
            file_paths=file_paths,
            metadata=metadata,
            skip_duplicates=skip_duplicates
        )
        
        # Perform incremental ingestion
        response = cortex_ops.ingest_incremental(
            file_paths=validated["file_paths"],
            metadata=validated["metadata"],
            skip_duplicates=validated["skip_duplicates"]
        )
        
        # Convert to dict and return as JSON
        result = to_dict(response)
        return json.dumps(result, indent=2)
        
    except ValidationError as e:
        return json.dumps({"status": "error", "error": f"Validation error: {str(e)}"}, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


# ============================================================================
# Cache Operations (Protocol 114 - Guardian Wakeup)
# ============================================================================

@mcp.tool()
def cortex_cache_get(query: str) -> str:
    """
    Retrieve cached answer for a query.
    
    Checks the Mnemonic Cache (CAG) for a previously computed answer.
    Returns cache hit status and answer if found.
    
    Args:
        query: Query string to look up in cache
    
    Returns:
        JSON with cache hit status and answer if found
    
    Example:
        cortex_cache_get("What is Protocol 101?")
    """
    try:
        response = cortex_ops.cache_get(query)
        result = to_dict(response)
        return json.dumps(result, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_cache_set(query: str, answer: str) -> str:
    """
    Store answer in cache for future retrieval.
    
    Caches an answer for a specific query in the Mnemonic Cache (CAG).
    Subsequent identical queries will retrieve this cached answer instantly.
    
    Args:
        query: Query string (cache key)
        answer: Answer to cache
    
    Returns:
        JSON with cache storage confirmation
    
    Example:
        cortex_cache_set("What is Protocol 101?", "Protocol 101 is...")
    """
    try:
        response = cortex_ops.cache_set(query, answer)
        result = to_dict(response)
        return json.dumps(result, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_cache_warmup(genesis_queries: Optional[List[str]] = None) -> str:
    """
    Pre-populate cache with genesis queries.
    
    Warms up the cache by pre-computing answers for frequently asked questions.
    If no queries provided, uses default set of essential Sanctuary questions.
    
    Args:
        genesis_queries: Optional list of queries to cache. If None, uses defaults.
    
    Returns:
        JSON with warmup statistics (queries cached, cache hits/misses, time)
    
    Example:
        cortex_cache_warmup()
        cortex_cache_warmup(genesis_queries=["What is Protocol 87?", "Latest roadmap"])
    """
    try:
        response = cortex_ops.cache_warmup(genesis_queries)
        result = to_dict(response)
        return json.dumps(result, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_guardian_wakeup() -> str:
    """
    Generate Guardian boot digest from cached bundles (Protocol 114).
    
    Retrieves chronicles, protocols, and roadmap summaries from cache
    and writes a digest to WORK_IN_PROGRESS/guardian_boot_digest.md.
    This provides the Guardian with essential context on startup.
    
    Returns:
        JSON with digest path and cache statistics
    
    Example:
        cortex_guardian_wakeup()
    """
    try:
        response = cortex_ops.guardian_wakeup()
        result = to_dict(response)
        return json.dumps(result, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


@mcp.tool()
def cortex_cache_stats() -> str:
    """
    Get Mnemonic Cache (CAG) statistics.
    
    Returns information about hot/warm cache size and hit rates.
    
    Returns:
        JSON with cache statistics
        
    Example:
        cortex_cache_stats()
    """
    try:
        stats = cortex_ops.get_cache_stats()
        return json.dumps(stats, indent=2)
    except Exception as e:
        return json.dumps({"status": "error", "error": str(e)}, indent=2)


# Import Synthesis Generator
from mnemonic_cortex.app.synthesis.generator import SynthesisGenerator

@mcp.tool()
def cortex_generate_adaptation_packet(days: int = 7) -> str:
    """
    Synthesize recent Cortex knowledge into an Adaptation Packet for model fine-tuning.
    
    Args:
        days: Number of days to look back for changes (default: 7)
        
    Returns:
        Path to the generated packet file.
    """
    generator = SynthesisGenerator(PROJECT_ROOT)
    packet = generator.generate_packet(days=days)
    output_path = generator.save_packet(packet)
    return f"Generated Adaptation Packet: {output_path}"

if __name__ == "__main__":
    mcp.run()

--- END OF FILE cognitive/cortex/server.py ---

--- START OF FILE cognitive/cortex/tests/__init__.py ---

"""
Cortex MCP Server Tests
"""

--- END OF FILE cognitive/cortex/tests/__init__.py ---

--- START OF FILE cognitive/cortex/tests/test_cache_operations.py ---

"""
Tests for Cortex cache operations (Protocol 114 - Guardian Wakeup).
"""
import pytest
from pathlib import Path
from mcp_servers.cognitive.cortex.operations import CortexOperations


@pytest.fixture
def cortex_ops():
    """Fixture providing CortexOperations instance."""
    project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
    return CortexOperations(str(project_root))


def test_cache_get_miss(cortex_ops):
    """Test cache get with no cached value (cache miss)."""
    response = cortex_ops.cache_get("nonexistent query xyz123 unique")
    
    assert response.cache_hit == False
    assert response.answer is None
    assert response.status == "success"
    assert response.query_time_ms >= 0


def test_cache_set_and_get(cortex_ops):
    """Test cache set followed by get (cache hit)."""
    test_query = "test query for cache operations"
    test_answer = "test answer from cache"
    
    # Set
    set_response = cortex_ops.cache_set(test_query, test_answer)
    assert set_response.stored == True
    assert set_response.status == "success"
    assert len(set_response.cache_key) > 0
    
    # Get
    get_response = cortex_ops.cache_get(test_query)
    assert get_response.cache_hit == True
    assert get_response.answer == test_answer
    assert get_response.status == "success"


def test_cache_warmup_default_queries(cortex_ops):
    """Test cache warmup with default genesis queries."""
    response = cortex_ops.cache_warmup()
    
    assert response.status == "success"
    assert response.queries_cached == 10  # Default genesis queries
    assert response.cache_hits + response.cache_misses == response.queries_cached
    assert response.total_time_ms > 0


def test_cache_warmup_custom_queries(cortex_ops):
    """Test cache warmup with custom query list."""
    custom_queries = [
        "What is Protocol 87?",
        "What is Protocol 101?"
    ]
    
    response = cortex_ops.cache_warmup(genesis_queries=custom_queries)
    
    assert response.status == "success"
    assert response.queries_cached == 2
    assert response.cache_hits + response.cache_misses == 2


def test_guardian_wakeup(cortex_ops):
    """Test Guardian wakeup digest generation."""
    response = cortex_ops.guardian_wakeup()
    
    assert response.status == "success"
    assert len(response.bundles_loaded) == 3
    assert "chronicles" in response.bundles_loaded
    assert "protocols" in response.bundles_loaded
    assert "roadmap" in response.bundles_loaded
    assert response.cache_hits + response.cache_misses == 3
    assert response.total_time_ms > 0
    
    # Verify digest file was created
    digest_path = Path(response.digest_path)
    assert digest_path.exists()
    assert digest_path.name == "guardian_boot_digest.md"
    
    # Verify digest content
    content = digest_path.read_text()
    assert "# Guardian Boot Digest" in content
    assert "## Chronicles" in content
    assert "## Protocols" in content
    assert "## Roadmap" in content


def test_cache_operations_error_handling(cortex_ops):
    """Test error handling in cache operations."""
    # Test with invalid inputs
    response = cortex_ops.cache_get("")
    assert response.status == "success"  # Empty query is valid, just returns miss
    
    response = cortex_ops.cache_set("", "")
    assert response.status == "success"  # Empty values are valid

--- END OF FILE cognitive/cortex/tests/test_cache_operations.py ---

--- START OF FILE cognitive/cortex/tests/test_cortex_integration.py ---

#!/usr/bin/env python3
"""
Integration tests for Cortex MCP Server

Tests all 4 tools in order of speed:
1. cortex_get_stats (fastest)
2. cortex_query (fast)
3. cortex_ingest_incremental (medium)
4. cortex_ingest_full (slowest - optional)

Usage:
    python3 test_cortex_integration.py
    python3 test_cortex_integration.py --skip-full-ingest
"""
import sys
import json
import time
import tempfile
import argparse
from pathlib import Path

# Add project root to path
# test_cortex_integration.py -> tests -> cortex -> cognitive -> mcp_servers -> Project_Sanctuary
project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Now we can import from the parent package
from mcp_servers.cognitive.cortex.operations import CortexOperations
from mcp_servers.cognitive.cortex.validator import CortexValidator
from mcp_servers.cognitive.cortex.models import to_dict


class Colors:
    """ANSI color codes for terminal output."""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


def print_test_header(test_name: str):
    """Print test header."""
    print(f"\n{Colors.BLUE}{Colors.BOLD}{'='*60}{Colors.RESET}")
    print(f"{Colors.BLUE}{Colors.BOLD}TEST: {test_name}{Colors.RESET}")
    print(f"{Colors.BLUE}{Colors.BOLD}{'='*60}{Colors.RESET}\n")


def print_success(message: str):
    """Print success message."""
    print(f"{Colors.GREEN}✓ {message}{Colors.RESET}")


def print_error(message: str):
    """Print error message."""
    print(f"{Colors.RED}✗ {message}{Colors.RESET}")


def print_info(message: str):
    """Print info message."""
    print(f"{Colors.YELLOW}ℹ {message}{Colors.RESET}")


def test_cortex_get_stats(ops: CortexOperations) -> bool:
    """Test cortex_get_stats tool."""
    print_test_header("cortex_get_stats")
    
    try:
        start = time.time()
        response = ops.get_stats()
        elapsed = time.time() - start
        
        result = to_dict(response)
        
        # Validate response (StatsResponse doesn't have 'status', only 'health_status')
        assert 'error' not in result or result['error'] is None, f"Got error: {result.get('error')}"
        assert result['health_status'] in ['healthy', 'degraded', 'error'], f"Invalid health status: {result['health_status']}"
        assert 'total_documents' in result, "Missing total_documents"
        assert 'total_chunks' in result, "Missing total_chunks"
        assert 'collections' in result, "Missing collections"
        
        print_success(f"Stats retrieved in {elapsed:.2f}s")
        print_info(f"Health: {result['health_status']}")
        print_info(f"Documents: {result['total_documents']}")
        print_info(f"Chunks: {result['total_chunks']}")
        
        if result['health_status'] == 'healthy':
            print_success("Database is healthy")
            return True
        else:
            print_error(f"Database health is {result['health_status']}")
            return False
            
    except Exception as e:
        print_error(f"Test failed: {str(e)}")
        return False


def test_cortex_query(ops: CortexOperations) -> bool:
    """Test cortex_query tool."""
    print_test_header("cortex_query")
    
    test_queries = [
        ("What is Protocol 101?", 3),
        ("Covenant of Grace chronicle entry", 2),
        ("Mnemonic Cortex architecture", 2)
    ]
    
    all_passed = True
    
    for query, max_results in test_queries:
        try:
            print_info(f"Query: '{query}' (max_results={max_results})")
            
            start = time.time()
            response = ops.query(query, max_results=max_results)
            elapsed = time.time() - start
            
            result = to_dict(response)
            
            # Validate response
            assert result['status'] == 'success', f"Expected success, got {result['status']}"
            assert 'results' in result, "Missing results"
            assert 'query_time_ms' in result, "Missing query_time_ms"
            assert len(result['results']) <= max_results, f"Too many results: {len(result['results'])}"
            
            print_success(f"Query completed in {elapsed:.2f}s")
            print_info(f"Results: {len(result['results'])} documents")
            
            # Show first result preview
            if result['results']:
                first_result = result['results'][0]
                content_preview = first_result['content'][:150].replace('\n', ' ')
                print_info(f"First result: {content_preview}...")
            
        except Exception as e:
            print_error(f"Query failed: {str(e)}")
            all_passed = False
    
    return all_passed


def test_cortex_ingest_incremental(ops: CortexOperations) -> bool:
    """Test cortex_ingest_incremental tool."""
    print_test_header("cortex_ingest_incremental")
    
    try:
        # Create a temporary test document
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            test_content = f"""# Test Document for Cortex MCP Integration

**Date:** {time.strftime('%Y-%m-%d')}
**Type:** Integration Test

## Purpose

This document is created automatically by the Cortex MCP integration test suite
to verify that incremental ingestion works correctly.

## Test Data

- Test ID: cortex_mcp_integration_test_{int(time.time())}
- Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
- Purpose: Verify cortex_ingest_incremental functionality

## Expected Behavior

This document should be:
1. Successfully ingested into the Mnemonic Cortex
2. Searchable via cortex_query
3. Retrievable with full content intact

## Cleanup

This test document can be safely removed after testing.
"""
            f.write(test_content)
            test_file = f.name
        
        print_info(f"Created test document: {test_file}")
        
        # Test ingestion
        start = time.time()
        response = ops.ingest_incremental(
            file_paths=[test_file],
            skip_duplicates=True
        )
        elapsed = time.time() - start
        
        result = to_dict(response)
        
        # Validate response
        assert result['status'] == 'success', f"Expected success, got {result['status']}: {result.get('error', '')}"
        assert 'documents_added' in result, "Missing documents_added"
        assert 'chunks_created' in result, "Missing chunks_created"
        
        print_success(f"Incremental ingest completed in {elapsed:.2f}s")
        print_info(f"Documents added: {result['documents_added']}")
        print_info(f"Chunks created: {result['chunks_created']}")
        print_info(f"Skipped duplicates: {result['skipped_duplicates']}")
        
        # Verify document is searchable
        print_info("Verifying document is searchable...")
        query_response = ops.query("cortex_mcp_integration_test", max_results=1)
        query_result = to_dict(query_response)
        
        if query_result['status'] == 'success' and len(query_result['results']) > 0:
            print_success("Document is searchable via cortex_query")
        else:
            print_error("Document not found in search results")
            return False
        
        # Cleanup
        Path(test_file).unlink()
        print_info(f"Cleaned up test document: {test_file}")
        
        return True
        
    except Exception as e:
        print_error(f"Test failed: {str(e)}")
        # Cleanup on error
        try:
            if 'test_file' in locals():
                Path(test_file).unlink()
        except:
            pass
        return False


def test_cortex_ingest_full(ops: CortexOperations) -> bool:
    """Test cortex_ingest_full tool (SLOW - optional)."""
    print_test_header("cortex_ingest_full (SLOW)")
    
    print_info("This test performs a full database re-ingestion")
    print_info("It may take several minutes to complete")
    
    try:
        start = time.time()
        response = ops.ingest_full(purge_existing=True)
        elapsed = time.time() - start
        
        result = to_dict(response)
        
        # Validate response
        assert result['status'] == 'success', f"Expected success, got {result['status']}: {result.get('error', '')}"
        assert 'documents_processed' in result, "Missing documents_processed"
        assert 'ingestion_time_ms' in result, "Missing ingestion_time_ms"
        
        print_success(f"Full ingest completed in {elapsed:.2f}s")
        print_info(f"Documents processed: {result['documents_processed']}")
        print_info(f"Vectorstore: {result['vectorstore_path']}")
        
        return True
        
    except Exception as e:
        print_error(f"Test failed: {str(e)}")
        return False


def main():
    """Run all integration tests."""
    parser = argparse.ArgumentParser(description='Cortex MCP Integration Tests')
    parser.add_argument('--skip-full-ingest', action='store_true',
                       help='Skip the slow full ingestion test')
    args = parser.parse_args()
    
    print(f"\n{Colors.BOLD}{'='*60}")
    print("Cortex MCP Server - Integration Test Suite")
    print(f"{'='*60}{Colors.RESET}\n")
    
    # Load environment variables
    from dotenv import load_dotenv
    env_path = project_root / ".env"
    if env_path.exists():
        load_dotenv(dotenv_path=env_path)
        print_info(f"Loaded environment from {env_path}")
    
    # Initialize operations
    ops = CortexOperations(str(project_root))
    
    # Run tests
    results = {}
    
    # Test 1: Get Stats (fastest)
    results['stats'] = test_cortex_get_stats(ops)
    
    # Test 2: Query (fast)
    results['query'] = test_cortex_query(ops)
    
    # Test 3: Incremental Ingest (medium)
    results['incremental'] = test_cortex_ingest_incremental(ops)
    
    # Test 4: Full Ingest (slowest - optional)
    if not args.skip_full_ingest:
        results['full_ingest'] = test_cortex_ingest_full(ops)
    else:
        print_info("\nSkipping full ingest test (use --skip-full-ingest=false to run)")
    
    # Print summary
    print(f"\n{Colors.BOLD}{'='*60}")
    print("Test Summary")
    print(f"{'='*60}{Colors.RESET}\n")
    
    total_tests = len(results)
    passed_tests = sum(1 for v in results.values() if v)
    
    for test_name, passed in results.items():
        status = f"{Colors.GREEN}PASS{Colors.RESET}" if passed else f"{Colors.RED}FAIL{Colors.RESET}"
        print(f"  {test_name:20s} {status}")
    
    print(f"\n{Colors.BOLD}Total: {passed_tests}/{total_tests} tests passed{Colors.RESET}\n")
    
    # Exit code
    sys.exit(0 if passed_tests == total_tests else 1)


if __name__ == "__main__":
    main()

--- END OF FILE cognitive/cortex/tests/test_cortex_integration.py ---

--- START OF FILE cognitive/cortex/tests/test_models.py ---

"""
Unit tests for Cortex MCP models
"""
import pytest
from mcp_servers.cognitive.cortex.models import (
    IngestFullRequest,
    IngestFullResponse,
    QueryRequest,
    QueryResponse,
    QueryResult,
    StatsResponse,
    CollectionStats,
    IngestIncrementalRequest,
    IngestIncrementalResponse,
    to_dict
)


def test_ingest_full_request():
    """Test IngestFullRequest model."""
    request = IngestFullRequest(
        purge_existing=True,
        source_directories=["01_PROTOCOLS", "00_CHRONICLE"]
    )
    assert request.purge_existing is True
    assert request.source_directories == ["01_PROTOCOLS", "00_CHRONICLE"]


def test_ingest_full_response():
    """Test IngestFullResponse model."""
    response = IngestFullResponse(
        documents_processed=459,
        chunks_created=2145,
        ingestion_time_ms=45230.5,
        vectorstore_path="/path/to/chroma_db",
        status="success"
    )
    assert response.documents_processed == 459
    assert response.chunks_created == 2145
    assert response.status == "success"


def test_query_request():
    """Test QueryRequest model."""
    request = QueryRequest(
        query="What is Protocol 101?",
        max_results=5,
        use_cache=False
    )
    assert request.query == "What is Protocol 101?"
    assert request.max_results == 5
    assert request.use_cache is False


def test_query_result():
    """Test QueryResult model."""
    result = QueryResult(
        content="Full document content",
        metadata={"source_file": "01_PROTOCOLS/101.md"},
        relevance_score=0.95
    )
    assert result.content == "Full document content"
    assert result.metadata["source_file"] == "01_PROTOCOLS/101.md"
    assert result.relevance_score == 0.95


def test_query_response():
    """Test QueryResponse model."""
    results = [
        QueryResult(
            content="Content 1",
            metadata={"source_file": "file1.md"}
        )
    ]
    response = QueryResponse(
        results=results,
        query_time_ms=234.5,
        cache_hit=False,
        status="success"
    )
    assert len(response.results) == 1
    assert response.query_time_ms == 234.5
    assert response.status == "success"


def test_collection_stats():
    """Test CollectionStats model."""
    stats = CollectionStats(count=2145, name="child_chunks_v5")
    assert stats.count == 2145
    assert stats.name == "child_chunks_v5"


def test_stats_response():
    """Test StatsResponse model."""
    collections = {
        "child_chunks": CollectionStats(count=2145, name="child_chunks_v5"),
        "parent_documents": CollectionStats(count=459, name="parent_documents_v5")
    }
    response = StatsResponse(
        total_documents=459,
        total_chunks=2145,
        collections=collections,
        health_status="healthy"
    )
    assert response.total_documents == 459
    assert response.total_chunks == 2145
    assert response.health_status == "healthy"


def test_ingest_incremental_request():
    """Test IngestIncrementalRequest model."""
    request = IngestIncrementalRequest(
        file_paths=["file1.md", "file2.md"],
        metadata={"author": "test"},
        skip_duplicates=True
    )
    assert len(request.file_paths) == 2
    assert request.metadata["author"] == "test"
    assert request.skip_duplicates is True


def test_ingest_incremental_response():
    """Test IngestIncrementalResponse model."""
    response = IngestIncrementalResponse(
        documents_added=3,
        chunks_created=15,
        skipped_duplicates=1,
        status="success"
    )
    assert response.documents_added == 3
    assert response.chunks_created == 15
    assert response.skipped_duplicates == 1
    assert response.status == "success"


def test_to_dict():
    """Test to_dict helper function."""
    response = IngestFullResponse(
        documents_processed=10,
        chunks_created=50,
        ingestion_time_ms=1000.0,
        vectorstore_path="/path",
        status="success"
    )
    result = to_dict(response)
    assert isinstance(result, dict)
    assert result["documents_processed"] == 10
    assert result["chunks_created"] == 50
    assert result["status"] == "success"


def test_to_dict_with_nested_objects():
    """Test to_dict with nested dataclass objects."""
    collections = {
        "child_chunks": CollectionStats(count=100, name="child_chunks_v5")
    }
    response = StatsResponse(
        total_documents=10,
        total_chunks=100,
        collections=collections,
        health_status="healthy"
    )
    result = to_dict(response)
    assert isinstance(result, dict)
    assert isinstance(result["collections"], dict)
    assert result["collections"]["child_chunks"]["count"] == 100

--- END OF FILE cognitive/cortex/tests/test_models.py ---

--- START OF FILE cognitive/cortex/tests/test_operations.py ---

"""
Unit tests for Cortex MCP operations

Note: These are integration-style tests that require the actual
Mnemonic Cortex infrastructure to be set up. They are marked
with pytest.mark.integration and can be skipped in CI.
"""
import pytest
import tempfile
import os
from pathlib import Path
from mcp_servers.cognitive.cortex.operations import CortexOperations


@pytest.fixture
def temp_project_root():
    """Create a temporary project root for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create minimal directory structure
        scripts_dir = Path(tmpdir) / "mnemonic_cortex" / "scripts"
        scripts_dir.mkdir(parents=True)
        
        yield tmpdir


def test_operations_init(temp_project_root):
    """Test operations initialization."""
    ops = CortexOperations(temp_project_root)
    assert ops.project_root == Path(temp_project_root)
    assert ops.scripts_dir == Path(temp_project_root) / "mnemonic_cortex" / "scripts"


@pytest.mark.integration
def test_ingest_full_script_not_found(temp_project_root):
    """Test ingest_full when script doesn't exist."""
    ops = CortexOperations(temp_project_root)
    response = ops.ingest_full()
    
    assert response.status == "error"
    assert "not found" in response.error.lower()


@pytest.mark.integration
def test_query_error_handling(temp_project_root):
    """Test query error handling when service not available."""
    ops = CortexOperations(temp_project_root)
    response = ops.query("test query")
    
    # Should return error response when infrastructure not available
    assert response.status == "error"
    assert response.error is not None


@pytest.mark.integration
def test_get_stats_no_database(temp_project_root):
    """Test get_stats when database doesn't exist."""
    ops = CortexOperations(temp_project_root)
    response = ops.get_stats()
    
    # Should return error or degraded status
    assert response.health_status in ["error", "degraded"]


@pytest.mark.integration
def test_ingest_incremental_error_handling(temp_project_root):
    """Test ingest_incremental error handling."""
    ops = CortexOperations(temp_project_root)
    
    # Try to ingest non-existent file
    response = ops.ingest_incremental(
        file_paths=["nonexistent.md"],
        skip_duplicates=True
    )
    
    # Should return error response
    assert response.status == "error"
    assert response.error is not None


# The following tests would require actual Mnemonic Cortex setup
# and are marked as integration tests

@pytest.mark.integration
@pytest.mark.skipif(
    not os.path.exists("/Users/richardfremmerlid/Projects/Project_Sanctuary/mnemonic_cortex"),
    reason="Requires actual Mnemonic Cortex setup"
)
def test_get_stats_real_database():
    """Test get_stats with real database (integration test)."""
    project_root = "/Users/richardfremmerlid/Projects/Project_Sanctuary"
    ops = CortexOperations(project_root)
    response = ops.get_stats()
    
    # Should return healthy status if database exists
    if response.health_status == "healthy":
        assert response.total_documents > 0
        assert response.total_chunks > 0
        assert "child_chunks" in response.collections
        assert "parent_documents" in response.collections


@pytest.mark.integration
@pytest.mark.skipif(
    not os.path.exists("/Users/richardfremmerlid/Projects/Project_Sanctuary/mnemonic_cortex"),
    reason="Requires actual Mnemonic Cortex setup"
)
def test_query_real_database():
    """Test query with real database (integration test)."""
    project_root = "/Users/richardfremmerlid/Projects/Project_Sanctuary"
    ops = CortexOperations(project_root)
    response = ops.query("What is Protocol 101?", max_results=3)
    
    # Should return successful response
    if response.status == "success":
        assert len(response.results) > 0
        assert response.query_time_ms > 0
        assert all(hasattr(r, 'content') for r in response.results)
        assert all(hasattr(r, 'metadata') for r in response.results)

--- END OF FILE cognitive/cortex/tests/test_operations.py ---

--- START OF FILE cognitive/cortex/tests/test_validator.py ---

"""
Unit tests for Cortex MCP validator
"""
import pytest
import tempfile
import os
from pathlib import Path
from mcp_servers.cognitive.cortex.validator import CortexValidator, ValidationError


@pytest.fixture
def temp_project_root():
    """Create a temporary project root for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create some test directories and files
        protocols_dir = Path(tmpdir) / "01_PROTOCOLS"
        protocols_dir.mkdir()
        
        test_file = protocols_dir / "test.md"
        test_file.write_text("# Test Protocol")
        
        yield tmpdir


def test_validator_init(temp_project_root):
    """Test validator initialization."""
    validator = CortexValidator(temp_project_root)
    assert validator.project_root == Path(temp_project_root)


def test_validate_ingest_full_success(temp_project_root):
    """Test successful validation of ingest_full."""
    validator = CortexValidator(temp_project_root)
    result = validator.validate_ingest_full(
        purge_existing=True,
        source_directories=["01_PROTOCOLS"]
    )
    assert result["purge_existing"] is True
    assert result["source_directories"] == ["01_PROTOCOLS"]


def test_validate_ingest_full_invalid_directory(temp_project_root):
    """Test validation fails for non-existent directory."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="does not exist"):
        validator.validate_ingest_full(
            purge_existing=True,
            source_directories=["NONEXISTENT_DIR"]
        )


def test_validate_query_success(temp_project_root):
    """Test successful validation of query."""
    validator = CortexValidator(temp_project_root)
    result = validator.validate_query(
        query="What is Protocol 101?",
        max_results=5,
        use_cache=False
    )
    assert result["query"] == "What is Protocol 101?"
    assert result["max_results"] == 5
    assert result["use_cache"] is False


def test_validate_query_empty_string(temp_project_root):
    """Test validation fails for empty query."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="cannot be empty"):
        validator.validate_query(query="", max_results=5)


def test_validate_query_whitespace_only(temp_project_root):
    """Test validation fails for whitespace-only query."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="cannot be empty"):
        validator.validate_query(query="   ", max_results=5)


def test_validate_query_too_long(temp_project_root):
    """Test validation fails for query that's too long."""
    validator = CortexValidator(temp_project_root)
    long_query = "x" * 10001
    with pytest.raises(ValidationError, match="too long"):
        validator.validate_query(query=long_query, max_results=5)


def test_validate_query_max_results_too_low(temp_project_root):
    """Test validation fails for max_results < 1."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="must be at least 1"):
        validator.validate_query(query="test", max_results=0)


def test_validate_query_max_results_too_high(temp_project_root):
    """Test validation fails for max_results > 100."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="cannot exceed 100"):
        validator.validate_query(query="test", max_results=101)


def test_validate_ingest_incremental_success(temp_project_root):
    """Test successful validation of ingest_incremental."""
    validator = CortexValidator(temp_project_root)
    test_file = Path(temp_project_root) / "01_PROTOCOLS" / "test.md"
    
    result = validator.validate_ingest_incremental(
        file_paths=[str(test_file)],
        metadata={"author": "test"},
        skip_duplicates=True
    )
    assert len(result["file_paths"]) == 1
    assert result["metadata"]["author"] == "test"
    assert result["skip_duplicates"] is True


def test_validate_ingest_incremental_relative_path(temp_project_root):
    """Test validation converts relative paths to absolute."""
    validator = CortexValidator(temp_project_root)
    
    result = validator.validate_ingest_incremental(
        file_paths=["01_PROTOCOLS/test.md"],
        skip_duplicates=True
    )
    assert len(result["file_paths"]) == 1
    assert os.path.isabs(result["file_paths"][0])


def test_validate_ingest_incremental_empty_list(temp_project_root):
    """Test validation fails for empty file_paths."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="cannot be empty"):
        validator.validate_ingest_incremental(file_paths=[])


def test_validate_ingest_incremental_too_many_files(temp_project_root):
    """Test validation fails for too many files."""
    validator = CortexValidator(temp_project_root)
    file_paths = ["file.md"] * 1001
    with pytest.raises(ValidationError, match="Cannot ingest more than 1000"):
        validator.validate_ingest_incremental(file_paths=file_paths)


def test_validate_ingest_incremental_file_not_exists(temp_project_root):
    """Test validation fails for non-existent file."""
    validator = CortexValidator(temp_project_root)
    with pytest.raises(ValidationError, match="does not exist"):
        validator.validate_ingest_incremental(file_paths=["nonexistent.md"])


def test_validate_ingest_incremental_not_markdown(temp_project_root):
    """Test validation fails for non-markdown file."""
    validator = CortexValidator(temp_project_root)
    test_file = Path(temp_project_root) / "test.txt"
    test_file.write_text("test")
    
    with pytest.raises(ValidationError, match="not a markdown file"):
        validator.validate_ingest_incremental(file_paths=[str(test_file)])


def test_validate_ingest_incremental_invalid_metadata(temp_project_root):
    """Test validation fails for invalid metadata type."""
    validator = CortexValidator(temp_project_root)
    test_file = Path(temp_project_root) / "01_PROTOCOLS" / "test.md"
    
    with pytest.raises(ValidationError, match="must be a dictionary"):
        validator.validate_ingest_incremental(
            file_paths=[str(test_file)],
            metadata="invalid"
        )


def test_validate_stats(temp_project_root):
    """Test validation of stats (no parameters)."""
    validator = CortexValidator(temp_project_root)
    result = validator.validate_stats()
    assert result == {}

--- END OF FILE cognitive/cortex/tests/test_validator.py ---

--- START OF FILE cognitive/cortex/validator.py ---

"""
Cortex MCP Server - Input Validation

Validates inputs for all Cortex MCP tools.
"""
import os
from pathlib import Path
from typing import List, Optional, Dict, Any


class ValidationError(Exception):
    """Raised when validation fails."""
    pass


class CortexValidator:
    """Validator for Cortex MCP operations."""
    
    def __init__(self, project_root: str):
        """
        Initialize validator.
        
        Args:
            project_root: Absolute path to project root
        """
        self.project_root = Path(project_root)
    
    def validate_ingest_full(
        self,
        purge_existing: bool = True,
        source_directories: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Validate full ingestion request.
        
        Args:
            purge_existing: Whether to purge existing database
            source_directories: Optional list of source directories
            
        Returns:
            Validated parameters
            
        Raises:
            ValidationError: If validation fails
        """
        # Validate source directories if provided
        if source_directories:
            for directory in source_directories:
                dir_path = self.project_root / directory
                if not dir_path.exists():
                    raise ValidationError(f"Source directory does not exist: {directory}")
                if not dir_path.is_dir():
                    raise ValidationError(f"Path is not a directory: {directory}")
        
        return {
            "purge_existing": purge_existing,
            "source_directories": source_directories
        }
    
    def validate_query(
        self,
        query: str,
        max_results: int = 5,
        use_cache: bool = False
    ) -> Dict[str, Any]:
        """
        Validate query request.
        
        Args:
            query: Query string
            max_results: Maximum number of results
            use_cache: Whether to use cache (Phase 2)
            
        Returns:
            Validated parameters
            
        Raises:
            ValidationError: If validation fails
        """
        # Validate query string
        if not query or not query.strip():
            raise ValidationError("Query string cannot be empty")
        
        if len(query) > 10000:
            raise ValidationError("Query string too long (max 10000 characters)")
        
        # Validate max_results
        if max_results < 1:
            raise ValidationError("max_results must be at least 1")
        
        if max_results > 100:
            raise ValidationError("max_results cannot exceed 100")
        
        return {
            "query": query.strip(),
            "max_results": max_results,
            "use_cache": use_cache
        }
    
    def validate_ingest_incremental(
        self,
        file_paths: List[str],
        metadata: Optional[Dict[str, Any]] = None,
        skip_duplicates: bool = True
    ) -> Dict[str, Any]:
        """
        Validate incremental ingestion request.
        
        Args:
            file_paths: List of file paths to ingest
            metadata: Optional metadata to attach
            skip_duplicates: Whether to skip duplicate files
            
        Returns:
            Validated parameters
            
        Raises:
            ValidationError: If validation fails
        """
        # Validate file_paths
        if not file_paths:
            raise ValidationError("file_paths cannot be empty")
        
        if len(file_paths) > 1000:
            raise ValidationError("Cannot ingest more than 1000 files at once")
        
        # Validate each file path
        validated_paths = []
        for file_path in file_paths:
            # Convert to absolute path if relative
            if not os.path.isabs(file_path):
                abs_path = self.project_root / file_path
            else:
                abs_path = Path(file_path)
            
            # Check file exists
            if not abs_path.exists():
                raise ValidationError(f"File does not exist: {file_path}")
            
            # Check it's a file
            if not abs_path.is_file():
                raise ValidationError(f"Path is not a file: {file_path}")
            
            # Check it's a markdown file
            if not str(abs_path).endswith('.md'):
                raise ValidationError(f"File is not a markdown file: {file_path}")
            
            validated_paths.append(str(abs_path))
        
        # Validate metadata if provided
        if metadata:
            if not isinstance(metadata, dict):
                raise ValidationError("metadata must be a dictionary")
        
        return {
            "file_paths": validated_paths,
            "metadata": metadata,
            "skip_duplicates": skip_duplicates
        }
    
    def validate_stats(self) -> Dict[str, Any]:
        """
        Validate stats request (no parameters needed).
        
        Returns:
            Empty dict (no parameters to validate)
        """
        return {}

--- END OF FILE cognitive/cortex/validator.py ---

--- START OF FILE document/__init__.py ---



--- END OF FILE document/__init__.py ---

--- START OF FILE document/adr/README.md ---

# ADR MCP Server

**Domain:** `project_sanctuary.document.adr`  
**Version:** 1.0.0  
**Status:** Production Ready

---

## Overview

The ADR MCP server provides tools for managing Architecture Decision Records (ADRs) in the `ADRs/` directory. It enforces the canonical ADR schema, validates sequential numbering, and provides search capabilities.

**Key Principle:** Safe, validated ADR management with no git operations.

---

## Quick Start

### Prerequisites

1. **Python 3.11+**
2. **Project Sanctuary** repository

### Start the MCP Server

**Local Development:**
```bash
cd /Users/richardfremmerlid/Projects/Project_Sanctuary
python3 -m mcp_servers.document.adr.server
```

**Via Claude Desktop / Antigravity:**
Already configured in MCP config. Just restart the client.

---

## Tools (5)

### 1. `adr_create`
Create a new ADR with automatic sequential numbering.

**Arguments:**
- `title` (str): ADR title
- `context` (str): Problem description and background
- `decision` (str): What was decided and why
- `consequences` (str): Positive/negative outcomes
- `date` (str, optional): Decision date (defaults to today)
- `status` (str, optional): Initial status (defaults to "proposed")
- `author` (str, optional): Decision maker (defaults to "AI Assistant")
- `supersedes` (int, optional): ADR number this supersedes

**Returns:**
```json
{
  "adr_number": 38,
  "file_path": "ADRs/038_example_decision.md",
  "status": "proposed"
}
```

**Example:**
```python
adr_create(
    title="Adopt FastAPI for REST APIs",
    context="Need a modern Python web framework...",
    decision="We will use FastAPI for all REST APIs...",
    consequences="Positive: Fast, modern, async support..."
)
```

---

### 2. `adr_update_status`
Update the status of an existing ADR.

**Arguments:**
- `number` (int): ADR number
- `new_status` (str): New status (proposed/accepted/deprecated/superseded)
- `reason` (str): Reason for status change

**Valid Transitions:**
- proposed → accepted
- proposed → deprecated
- accepted → deprecated
- accepted → superseded

**Returns:**
```json
{
  "adr_number": 38,
  "old_status": "proposed",
  "new_status": "accepted",
  "updated_at": "2025-11-27"
}
```

---

### 3. `adr_get`
Retrieve a specific ADR by number.

**Arguments:**
- `number` (int): ADR number

**Returns:**
```json
{
  "number": 38,
  "title": "Adopt FastAPI for REST APIs",
  "status": "accepted",
  "date": "2025-11-27",
  "author": "AI Assistant",
  "context": "...",
  "decision": "...",
  "consequences": "..."
}
```

---

### 4. `adr_list`
List all ADRs with optional status filter.

**Arguments:**
- `status` (str, optional): Filter by status

**Returns:**
```json
{
  "adrs": [
    {
      "number": 37,
      "title": "MCP Git Migration Strategy",
      "status": "accepted",
      "date": "2025-11-27"
    },
    ...
  ]
}
```

---

### 5. `adr_search`
Full-text search across all ADRs.

**Arguments:**
- `query` (str): Search query

**Returns:**
```json
{
  "results": [
    {
      "number": 37,
      "title": "MCP Git Migration Strategy",
      "matches": [
        "...Protocol 101...",
        "...Smart Git MCP..."
      ]
    }
  ]
}
```

---

## Safety Rules

1. **Sequential Numbering**: ADR numbers are automatically assigned sequentially
2. **No Deletion**: ADRs cannot be deleted, only superseded
3. **Valid Transitions**: Status changes must follow allowed transitions
4. **Supersedes Validation**: Referenced ADRs must exist
5. **Schema Compliance**: All ADRs follow the canonical schema
6. **File Operations Only**: No git commits (use Git Workflow MCP)

---

## ADR Schema

All ADRs follow this format:

```markdown
# [Decision Title]

**Status:** [proposed | accepted | deprecated | superseded]
**Date:** YYYY-MM-DD
**Author:** [Name]
**Context:** [Optional task reference]

---

## Context

[Problem description and background]

## Decision

[What was decided and why]

## Consequences

### Positive
- [Benefits]

### Negative
- [Trade-offs]

### Risks
- [Potential issues and mitigation]
```

---

## Configuration

### Claude Desktop / Antigravity
```json
{
  "adr": {
    "displayName": "ADR MCP",
    "command": "/usr/local/bin/python3",
    "args": ["-m", "mcp_servers.document.adr.server"],
    "env": {
      "PROJECT_ROOT": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
      "PYTHONPATH": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
    },
    "cwd": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
  }
}
```

---

## Testing

```bash
# Run all tests
PYTHONPATH=. python3 -m pytest tests/test_adr*.py -v

# Test specific functionality
PYTHONPATH=. python3 -m pytest tests/test_adr_operations.py::test_create_adr -v
```

---

## Troubleshooting

### ADR Number Already Exists
```
Error: ADR 038 already exists
```
**Solution:** The system automatically assigns the next available number.

### Invalid Status Transition
```
Error: Cannot transition from 'accepted' to 'proposed'
```
**Solution:** Check valid transitions in the Safety Rules section.

### Superseded ADR Not Found
```
Error: ADR 025 does not exist (referenced in supersedes)
```
**Solution:** Ensure the referenced ADR exists before creating a superseding ADR.

---

## Related Documentation

- [ADR Schema](../../ADRs/adr_schema.md)
- [MCP Architecture](../../docs/mcp/architecture.md)
- [Task MCP](../task/README.md)

---

**Last Updated:** 2025-11-27  
**Maintainer:** Project Sanctuary Team

--- END OF FILE document/adr/README.md ---

--- START OF FILE document/adr/__init__.py ---



--- END OF FILE document/adr/__init__.py ---

--- START OF FILE document/adr/models.py ---

"""
ADR MCP Server - Data Models
"""
from dataclasses import dataclass
from enum import Enum
from typing import Optional
from datetime import datetime


class ADRStatus(Enum):
    """Valid ADR statuses."""
    PROPOSED = "proposed"
    ACCEPTED = "accepted"
    DEPRECATED = "deprecated"
    SUPERSEDED = "superseded"


@dataclass
class ADR:
    """ADR data model."""
    number: int
    title: str
    status: ADRStatus
    date: str
    author: str
    context: str
    decision: str
    consequences: str
    supersedes: Optional[int] = None
    
    def to_dict(self):
        """Convert to dictionary."""
        return {
            "number": self.number,
            "title": self.title,
            "status": self.status.value,
            "date": self.date,
            "author": self.author,
            "context": self.context,
            "decision": self.decision,
            "consequences": self.consequences,
            "supersedes": self.supersedes
        }


# ADR Template
ADR_TEMPLATE = """# {title}

**Status:** {status}
**Date:** {date}
**Author:** {author}
{context_line}

---

## Context

{context}

## Decision

{decision}

## Consequences

{consequences}
"""


# Valid status transitions
VALID_TRANSITIONS = {
    ADRStatus.PROPOSED: [ADRStatus.ACCEPTED, ADRStatus.DEPRECATED],
    ADRStatus.ACCEPTED: [ADRStatus.DEPRECATED, ADRStatus.SUPERSEDED],
    ADRStatus.DEPRECATED: [],
    ADRStatus.SUPERSEDED: []
}

--- END OF FILE document/adr/models.py ---

--- START OF FILE document/adr/operations.py ---

"""
ADR MCP Server - File Operations
"""
import os
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from .models import ADR, ADRStatus, ADR_TEMPLATE
from .validator import ADRValidator


class ADROperations:
    """Handles ADR file operations."""
    
    def __init__(self, adrs_dir: str = "ADRs"):
        self.adrs_dir = adrs_dir
        self.validator = ADRValidator(adrs_dir)
        
        # Ensure directory exists
        os.makedirs(self.adrs_dir, exist_ok=True)
    
    def create_adr(
        self,
        title: str,
        context: str,
        decision: str,
        consequences: str,
        date: Optional[str] = None,
        status: str = "proposed",
        author: str = "AI Assistant",
        supersedes: Optional[int] = None
    ) -> Dict[str, Any]:
        """Create a new ADR."""
        # Validate inputs
        self.validator.validate_required_fields(title, context, decision, consequences)
        self.validator.validate_supersedes(supersedes)
        
        # Get next number
        adr_number = self.validator.get_next_adr_number()
        
        # Use current date if not provided
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        # Create filename from title
        filename_title = title.lower().replace(" ", "_")
        filename_title = re.sub(r'[^a-z0-9_]', '', filename_title)
        filename = f"{adr_number:03d}_{filename_title}.md"
        filepath = os.path.join(self.adrs_dir, filename)
        
        # Format context line
        context_line = ""
        if supersedes:
            context_line = f"**Supersedes:** ADR {supersedes:03d}"
        
        # Generate content from template
        content = ADR_TEMPLATE.format(
            title=title,
            status=status,
            date=date,
            author=author,
            context_line=context_line,
            context=context,
            decision=decision,
            consequences=consequences
        )
        
        # Write file
        with open(filepath, 'w') as f:
            f.write(content)
        
        return {
            "adr_number": adr_number,
            "file_path": filepath,
            "status": status
        }
    
    def update_adr_status(
        self,
        number: int,
        new_status: str,
        reason: str
    ) -> Dict[str, Any]:
        """Update ADR status."""
        # Find the ADR file
        filepath = self._find_adr_file(number)
        if not filepath:
            raise FileNotFoundError(f"ADR {number:03d} not found")
        
        # Read current content
        with open(filepath, 'r') as f:
            content = f.read()
        
        # Extract current status
        status_match = re.search(r'\*\*Status:\*\* (\w+)', content)
        if not status_match:
            raise ValueError(f"Could not find status in ADR {number:03d}")
        
        old_status = ADRStatus(status_match.group(1))
        new_status_enum = ADRStatus(new_status)
        
        # Validate transition
        self.validator.validate_status_transition(old_status, new_status_enum)
        
        # Update status in content
        updated_content = re.sub(
            r'\*\*Status:\*\* \w+',
            f'**Status:** {new_status}',
            content
        )
        
        # Add update note
        update_note = f"\n\n---\n\n**Status Update ({datetime.now().strftime('%Y-%m-%d')}):** {reason}\n"
        updated_content += update_note
        
        # Write back
        with open(filepath, 'w') as f:
            f.write(updated_content)
        
        return {
            "adr_number": number,
            "old_status": old_status.value,
            "new_status": new_status,
            "updated_at": datetime.now().strftime("%Y-%m-%d")
        }
    
    def get_adr(self, number: int) -> Dict[str, Any]:
        """Get a specific ADR."""
        filepath = self._find_adr_file(number)
        if not filepath:
            raise FileNotFoundError(f"ADR {number:03d} not found")
        
        with open(filepath, 'r') as f:
            content = f.read()
        
        # Parse ADR content
        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
        status_match = re.search(r'\*\*Status:\*\* (\w+)', content)
        date_match = re.search(r'\*\*Date:\*\* ([\d-]+)', content)
        author_match = re.search(r'\*\*Author:\*\* (.+)$', content, re.MULTILINE)
        
        context_match = re.search(r'## Context\n\n(.+?)(?=\n## )', content, re.DOTALL)
        decision_match = re.search(r'## Decision\n\n(.+?)(?=\n## )', content, re.DOTALL)
        consequences_match = re.search(r'## Consequences\n\n(.+?)(?=\n---|$)', content, re.DOTALL)
        
        return {
            "number": number,
            "title": title_match.group(1) if title_match else "Unknown",
            "status": status_match.group(1) if status_match else "unknown",
            "date": date_match.group(1) if date_match else "unknown",
            "author": author_match.group(1) if author_match else "Unknown",
            "context": context_match.group(1).strip() if context_match else "",
            "decision": decision_match.group(1).strip() if decision_match else "",
            "consequences": consequences_match.group(1).strip() if consequences_match else "",
            "file_path": filepath
        }
    
    def list_adrs(self, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """List all ADRs."""
        adrs = []
        
        for filename in sorted(os.listdir(self.adrs_dir)):
            if not filename.endswith('.md') or filename.startswith('adr_schema'):
                continue
            
            match = re.match(r'^(\d{3})_', filename)
            if not match:
                continue
            
            number = int(match.group(1))
            try:
                adr = self.get_adr(number)
                
                # Filter by status if provided
                if status and adr['status'] != status:
                    continue
                
                adrs.append({
                    "number": adr['number'],
                    "title": adr['title'],
                    "status": adr['status'],
                    "date": adr['date']
                })
            except Exception:
                continue
        
        return adrs
    
    def search_adrs(self, query: str) -> List[Dict[str, Any]]:
        """Search ADRs by content."""
        results = []
        query_lower = query.lower()
        
        for filename in sorted(os.listdir(self.adrs_dir)):
            if not filename.endswith('.md') or filename.startswith('adr_schema'):
                continue
            
            match = re.match(r'^(\d{3})_', filename)
            if not match:
                continue
            
            number = int(match.group(1))
            filepath = os.path.join(self.adrs_dir, filename)
            
            with open(filepath, 'r') as f:
                content = f.read()
            
            # Search in content
            if query_lower in content.lower():
                # Extract matching lines
                matches = []
                for line in content.split('\n'):
                    if query_lower in line.lower():
                        matches.append(line.strip())
                        if len(matches) >= 3:  # Limit to 3 matches per ADR
                            break
                
                adr = self.get_adr(number)
                results.append({
                    "number": number,
                    "title": adr['title'],
                    "matches": matches
                })
        
        return results
    
    def _find_adr_file(self, number: int) -> Optional[str]:
        """Find ADR file by number."""
        for filename in os.listdir(self.adrs_dir):
            if re.match(f"^{number:03d}_", filename):
                return os.path.join(self.adrs_dir, filename)
        return None

--- END OF FILE document/adr/operations.py ---

--- START OF FILE document/adr/requirements.txt ---

fastmcp

--- END OF FILE document/adr/requirements.txt ---

--- START OF FILE document/adr/server.py ---

"""
ADR MCP Server
Domain: project_sanctuary.document.adr
"""
from fastmcp import FastMCP
from .operations import ADROperations
import os
from typing import Optional

# Initialize FastMCP with canonical domain name
mcp = FastMCP("project_sanctuary.document.adr")

# Initialize ADR operations
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")
ADRS_DIR = os.path.join(PROJECT_ROOT, "ADRs")
adr_ops = ADROperations(ADRS_DIR)


@mcp.tool()
def adr_create(
    title: str,
    context: str,
    decision: str,
    consequences: str,
    date: Optional[str] = None,
    status: str = "proposed",
    author: str = "AI Assistant",
    supersedes: Optional[int] = None
) -> str:
    """
    Create a new ADR with automatic sequential numbering.
    
    Args:
        title: ADR title
        context: Problem description and background
        decision: What was decided and why
        consequences: Positive/negative outcomes and risks
        date: Decision date (defaults to today)
        status: Initial status (defaults to "proposed")
        author: Decision maker (defaults to "AI Assistant")
        supersedes: ADR number this supersedes (optional)
        
    Returns:
        JSON string with adr_number, file_path, and status
        
    Example:
        adr_create(
            title="Adopt FastAPI for REST APIs",
            context="Need a modern Python web framework for building REST APIs...",
            decision="We will use FastAPI for all new REST API development...",
            consequences="Positive: Fast, modern, async support. Negative: Learning curve."
        )
    """
    try:
        result = adr_ops.create_adr(
            title=title,
            context=context,
            decision=decision,
            consequences=consequences,
            date=date,
            status=status,
            author=author,
            supersedes=supersedes
        )
        return f"Created ADR {result['adr_number']:03d}: {result['file_path']}"
    except Exception as e:
        return f"Error creating ADR: {str(e)}"


@mcp.tool()
def adr_update_status(number: int, new_status: str, reason: str) -> str:
    """
    Update the status of an existing ADR.
    
    Valid transitions:
    - proposed → accepted
    - proposed → deprecated
    - accepted → deprecated
    - accepted → superseded
    
    Args:
        number: ADR number
        new_status: New status (proposed/accepted/deprecated/superseded)
        reason: Reason for status change
        
    Returns:
        Status update confirmation
        
    Example:
        adr_update_status(
            number=38,
            new_status="accepted",
            reason="Implemented and tested successfully"
        )
    """
    try:
        result = adr_ops.update_adr_status(number, new_status, reason)
        return (
            f"Updated ADR {result['adr_number']:03d}: "
            f"{result['old_status']} → {result['new_status']} "
            f"(Reason: {reason})"
        )
    except Exception as e:
        return f"Error updating ADR status: {str(e)}"


@mcp.tool()
def adr_get(number: int) -> str:
    """
    Retrieve a specific ADR by number.
    
    Args:
        number: ADR number
        
    Returns:
        ADR details including title, status, context, decision, and consequences
        
    Example:
        adr_get(37)
    """
    try:
        adr = adr_ops.get_adr(number)
        return (
            f"ADR {adr['number']:03d}: {adr['title']}\n"
            f"Status: {adr['status']}\n"
            f"Date: {adr['date']}\n"
            f"Author: {adr['author']}\n\n"
            f"Context:\n{adr['context']}\n\n"
            f"Decision:\n{adr['decision']}\n\n"
            f"Consequences:\n{adr['consequences']}"
        )
    except Exception as e:
        return f"Error retrieving ADR: {str(e)}"


@mcp.tool()
def adr_list(status: Optional[str] = None) -> str:
    """
    List all ADRs with optional status filter.
    
    Args:
        status: Filter by status (proposed/accepted/deprecated/superseded)
        
    Returns:
        List of ADRs with number, title, status, and date
        
    Example:
        adr_list()  # All ADRs
        adr_list(status="accepted")  # Only accepted ADRs
    """
    try:
        adrs = adr_ops.list_adrs(status)
        if not adrs:
            return "No ADRs found" + (f" with status '{status}'" if status else "")
        
        result = f"Found {len(adrs)} ADR(s)" + (f" with status '{status}'" if status else "") + ":\n\n"
        for adr in adrs:
            result += f"ADR {adr['number']:03d}: {adr['title']} [{adr['status']}] ({adr['date']})\n"
        
        return result
    except Exception as e:
        return f"Error listing ADRs: {str(e)}"


@mcp.tool()
def adr_search(query: str) -> str:
    """
    Full-text search across all ADRs.
    
    Args:
        query: Search query
        
    Returns:
        List of matching ADRs with context snippets
        
    Example:
        adr_search("Protocol 101")
        adr_search("FastAPI")
    """
    try:
        results = adr_ops.search_adrs(query)
        if not results:
            return f"No ADRs found matching '{query}'"
        
        output = f"Found {len(results)} ADR(s) matching '{query}':\n\n"
        for result in results:
            output += f"ADR {result['number']:03d}: {result['title']}\n"
            for match in result['matches']:
                output += f"  - {match}\n"
            output += "\n"
        
        return output
    except Exception as e:
        return f"Error searching ADRs: {str(e)}"


if __name__ == "__main__":
    mcp.run()

--- END OF FILE document/adr/server.py ---

--- START OF FILE document/adr/validator.py ---

"""
ADR MCP Server - Validation Logic
"""
import os
import re
from typing import List, Optional
from .models import ADRStatus, VALID_TRANSITIONS


class ADRValidator:
    """Validates ADR operations."""
    
    def __init__(self, adrs_dir: str = "ADRs"):
        self.adrs_dir = adrs_dir
    
    def get_next_adr_number(self) -> int:
        """Get the next sequential ADR number."""
        if not os.path.exists(self.adrs_dir):
            return 1
        
        existing_numbers = []
        for filename in os.listdir(self.adrs_dir):
            if filename.endswith('.md') and not filename.startswith('adr_schema'):
                match = re.match(r'^(\d{3})_', filename)
                if match:
                    existing_numbers.append(int(match.group(1)))
        
        if not existing_numbers:
            return 1
        
        return max(existing_numbers) + 1
    
    def validate_adr_number(self, number: int) -> None:
        """Validate ADR number doesn't already exist."""
        filename_pattern = f"{number:03d}_*.md"
        for filename in os.listdir(self.adrs_dir):
            if re.match(f"^{number:03d}_", filename):
                raise ValueError(f"ADR {number:03d} already exists: {filename}")
    
    def validate_status_transition(
        self, 
        current_status: ADRStatus, 
        new_status: ADRStatus
    ) -> None:
        """Validate status transition is allowed."""
        if current_status == new_status:
            return  # No change is always valid
        
        allowed = VALID_TRANSITIONS.get(current_status, [])
        if new_status not in allowed:
            raise ValueError(
                f"Invalid transition from '{current_status.value}' to '{new_status.value}'. "
                f"Allowed transitions: {[s.value for s in allowed]}"
            )
    
    def validate_supersedes(self, supersedes: Optional[int]) -> None:
        """Validate that superseded ADR exists."""
        if supersedes is None:
            return
        
        # Check if the ADR exists
        found = False
        for filename in os.listdir(self.adrs_dir):
            if re.match(f"^{supersedes:03d}_", filename):
                found = True
                break
        
        if not found:
            raise ValueError(
                f"ADR {supersedes:03d} does not exist (referenced in supersedes)"
            )
    
    def validate_required_fields(
        self,
        title: str,
        context: str,
        decision: str,
        consequences: str
    ) -> None:
        """Validate required fields are not empty."""
        if not title or not title.strip():
            raise ValueError("Title is required")
        if not context or not context.strip():
            raise ValueError("Context is required")
        if not decision or not decision.strip():
            raise ValueError("Decision is required")
        if not consequences or not consequences.strip():
            raise ValueError("Consequences are required")

--- END OF FILE document/adr/validator.py ---

--- START OF FILE lib/__init__.py ---

"""
Shared libraries for MCP servers and system components.
"""

--- END OF FILE lib/__init__.py ---

--- START OF FILE lib/git/__init__.py ---



--- END OF FILE lib/git/__init__.py ---

--- START OF FILE lib/git/git_ops.py ---

import subprocess
import os
from typing import List, Dict, Any, Optional

class GitOperations:
    """
    Handles git operations with Protocol 101 v3.0 (Functional Coherence) enforcement.
    
    Protocol 101 v3.0 mandates that all commits must pass the automated test suite
    before being accepted. This class provides safe, whitelisted git operations.
    """
    
    def __init__(self, repo_path: str = ".", base_dir: Optional[str] = None):
        self.repo_path = os.path.abspath(repo_path)
        
        # Security: Restrict operations to base_dir if specified
        self.base_dir = os.path.abspath(base_dir) if base_dir else None
        if self.base_dir and not self.repo_path.startswith(self.base_dir):
            raise ValueError(f"Repository path {self.repo_path} is outside base directory {self.base_dir}")

    def verify_clean_state(self) -> None:
        """
        Pillar 4: Pre-Execution Verification.
        Ensures the working directory is clean before critical operations.
        Raises RuntimeError if dirty.
        """
        status = self.status()
        if status["modified"] or status["staged"] or status["untracked"]:
            raise RuntimeError(
                f"Working directory is not clean. "
                f"Modified: {len(status['modified'])}, "
                f"Staged: {len(status['staged'])}, "
                f"Untracked: {len(status['untracked'])}. "
                "Please commit or stash changes before proceeding."
            )

    def _run_git(self, args: List[str]) -> str:
        """Run a git command and return output."""
        try:
            result = subprocess.run(
                ["git"] + args,
                cwd=self.repo_path,
                capture_output=True,
                text=True,
                check=True
            )
            return result.stdout.strip()
        except subprocess.CalledProcessError as e:
            # Enhanced error handling to capture stderr
            raise RuntimeError(f"Git command failed: {e.stderr.strip()}")

    def get_staged_files(self) -> List[str]:
        """Get list of currently staged files."""
        output = self._run_git(["diff", "--name-only", "--cached"])
        if not output:
            return []
        return output.splitlines()

    def add(self, files: List[str] = None) -> None:
        """Stage files for commit."""
        if files is None or len(files) == 0:
            # Stage all modified and new files
            self._run_git(["add", "-A"])
        else:
            self._run_git(["add"] + files)

    # PROTOCOL 101 v3.0: Manifest generation methods PERMANENTLY REMOVED
    # Functional Coherence (test suite execution) is now the sole integrity mechanism

    def commit(self, message: str) -> str:
        """
        Commit staged files with Protocol 101 v3.0 compliance.
        
        Protocol 101 v3.0 (Functional Coherence):
        - The pre-commit hook will automatically execute ./scripts/run_genome_tests.sh
        - All tests must pass for the commit to proceed
        - No manifest generation is required
        
        Returns commit hash.
        """
        # Protocol 101 v3.0: Pre-commit hook handles test execution
        # We simply commit normally - the hook will enforce functional coherence
        self._run_git(["commit", "-m", message])
        
        # Return hash
        return self._run_git(["rev-parse", "HEAD"])

    def get_current_branch(self) -> str:
        """Get the current active branch name."""
        return self._run_git(["rev-parse", "--abbrev-ref", "HEAD"])

    def create_branch(self, branch_name: str, start_point: str = "HEAD") -> None:
        """Create a new branch."""
        self._run_git(["branch", branch_name, start_point])

    def checkout(self, branch_name: str) -> None:
        """Checkout a branch."""
        self._run_git(["checkout", branch_name])

    def push(self, remote: str = "origin", branch: str = None, force: bool = False, no_verify: bool = False) -> str:
        """Push to remote."""
        if branch is None:
            branch = self.get_current_branch()
        
        args = ["push", remote, branch]
        if force:
            args.append("--force")
        if no_verify:
            args.append("--no-verify")
            
        return self._run_git(args)

    def pull(self, remote: str = "origin", branch: str = None) -> str:
        """Pull from remote."""
        if branch is None:
            branch = self.get_current_branch()
        return self._run_git(["pull", remote, branch])

    def delete_branch(self, branch_name: str, force: bool = False) -> None:
        """Delete a branch."""
        flag = "-D" if force else "-d"
        self._run_git(["branch", flag, branch_name])

    def delete_local_branch(self, branch_name: str, force: bool = False) -> None:
        """Delete a local branch (alias for delete_branch)."""
        self.delete_branch(branch_name, force)

    def delete_remote_branch(self, branch_name: str) -> None:
        """Delete a remote branch."""
        self._run_git(["push", "origin", "--delete", branch_name])

    def status(self) -> Dict[str, Any]:
        """Get repo status."""
        branch = self.get_current_branch()
        status_porcelain = self._run_git(["status", "--porcelain"])
        
        staged = []
        modified = []
        untracked = []
        
        for line in status_porcelain.splitlines():
            code = line[:2]
            path = line[3:]
            if code.startswith("M") or code.startswith("A"):
                staged.append(path)
            if code.endswith("M"):
                modified.append(path)
            if code.startswith("??"):
                untracked.append(path)
                
        return {
            "branch": branch,
            "staged": staged,
            "modified": modified,
            "untracked": untracked
        }

    def diff(self, cached: bool = False, file_path: Optional[str] = None) -> str:
        """Get diff output."""
        args = ["diff"]
        if cached:
            args.append("--cached")
        if file_path:
            args.append(file_path)
        return self._run_git(args)

    def log(self, max_count: int = 10, oneline: bool = False) -> str:
        """Get commit log."""
        args = ["log", f"-n{max_count}"]
        if oneline:
            args.append("--oneline")
        return self._run_git(args)

--- END OF FILE lib/git/git_ops.py ---

--- START OF FILE lib/utils/__init__.py ---



--- END OF FILE lib/utils/__init__.py ---

--- START OF FILE lib/utils/env_helper.py ---

"""
Simple environment variable helper with proper fallback.

Provides consistent secret loading across Project Sanctuary with proper priority:
1. Environment variable (Windows → WSL via WSLENV)
2. .env file in project root
3. Error or None if not found

This ensures consistency with docs/WSL_SECRETS_CONFIGURATION.md
"""

import os
from typing import Optional
from pathlib import Path


def get_env_variable(key: str, required: bool = True) -> Optional[str]:
    """
    Get environment variable with proper fallback.
    
    Priority:
    1. Environment variable (Windows → WSL via WSLENV)
    2. .env file in project root
    3. Return None or raise error if not found
    
    Args:
        key: Environment variable name
        required: If True, raise error when not found
    
    Returns:
        Environment variable value or None
    
    Raises:
        ValueError: If required=True and variable not found
    
    Example:
        >>> from mcp_servers.lib.utils.env_helper import get_env_variable
        >>> token = get_env_variable("HUGGING_FACE_TOKEN", required=True)
    """
    # First, check environment (includes WSLENV passthrough from Windows)
    value = os.getenv(key)
    
    # Fallback to .env file if not in environment
    if not value:
        try:
            from dotenv import load_dotenv
            # Compute project root from this file's location
            # This file: Project_Sanctuary/mcp_servers/lib/utils/env_helper.py
            # Project root: ../../../.. from this file
            project_root = Path(__file__).resolve().parent.parent.parent.parent
            env_file = project_root / ".env"
            if env_file.exists():
                load_dotenv(env_file)
                value = os.getenv(key)
        except ImportError:
            # python-dotenv not installed, skip .env fallback
            pass
    
    # Handle missing required variables
    if required and not value:
        raise ValueError(
            f"Required environment variable not found: {key}\n"
            f"Please set this in Windows User Environment Variables.\n"
            f"See docs/WSL_SECRETS_CONFIGURATION.md for setup instructions."
        )
    
    return value

--- END OF FILE lib/utils/env_helper.py ---

--- START OF FILE orchestrator/__init__.py ---



--- END OF FILE orchestrator/__init__.py ---

--- START OF FILE orchestrator/command.json ---

{
  "task_description": "Test cognitive task",
  "output_artifact_path": "WORK_IN_PROGRESS/test_output.md",
  "config": {
    "max_rounds": 3,
    "max_cortex_queries": 5
  }
}

--- END OF FILE orchestrator/command.json ---

--- START OF FILE orchestrator/config/__init__.py ---



--- END OF FILE orchestrator/config/__init__.py ---

--- START OF FILE orchestrator/config/mcp_config.json ---

{
    "server_name": "council-command-processor",
    "version": "1.0.0",
    "description": "MCP server for safe Council Orchestrator command generation",
    "tools": {
        "cognitive": {
            "enabled": true,
            "default_max_rounds": 5,
            "default_max_cortex_queries": 5
        },
        "mechanical": {
            "enabled": true,
            "require_approval_for_git": false,
            "require_approval_for_writes": false
        },
        "dangerous": {
            "enabled": false,
            "comment": "Destructive operations disabled by design"
        }
    },
    "safety": {
        "git_safety_rules_path": "../.agent/git_safety_rules.md",
        "protected_paths": [
            "01_PROTOCOLS/",
            ".git/",
            ".agent/"
        ],
        "max_file_size_mb": 10,
        "allowed_extensions": [
            ".md",
            ".py",
            ".json",
            ".txt",
            ".yaml",
            ".yml"
        ]
    },
    "orchestrator": {
        "command_file_path": "mcp_servers/orchestrator/command.json",
        "results_directory": "mcp_servers/orchestrator/command_results/",
        "logs_directory": "mcp_servers/orchestrator/logs/"
    }
}

--- END OF FILE orchestrator/config/mcp_config.json ---

--- START OF FILE orchestrator/schemas/__init__.py ---



--- END OF FILE orchestrator/schemas/__init__.py ---

--- START OF FILE orchestrator/server.py ---

"""
Orchestrator MCP Server
Domain: project_sanctuary.orchestrator

Provides MCP tools for the Sanctuary Council (Strategist, Auditor, etc.) to
orchestrate high-level missions and decisions.
"""
from fastmcp import FastMCP
import os
import sys
import json
from pathlib import Path
from typing import Optional, List
from mcp.server.fastmcp import FastMCP

# Add project root to path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# Import tools
from mcp_servers.orchestrator.tools.cognitive import (
    create_cognitive_task,
    create_development_cycle,
    query_mnemonic_cortex
)
from mcp_servers.orchestrator.tools.mechanical import (
    create_file_write_task,
    create_git_commit_task
)
from mcp_servers.orchestrator.tools.query import (
    get_orchestrator_status,
    list_recent_tasks,
    get_task_result
)

# Initialize MCP Server
mcp = FastMCP("orchestrator")

# Configuration
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")

# Load Config
CONFIG_PATH = Path(__file__).parent / "config" / "mcp_config.json"
try:
    with open(CONFIG_PATH, "r") as f:
        MCP_CONFIG = json.load(f)
except Exception as e:
    print(f"Warning: Could not load config from {CONFIG_PATH}: {e}")
    MCP_CONFIG = {}

# TODO: On server startup, call cortex_guardian_wakeup() to initialize the cache
# and generate the boot digest for the Council.


@mcp.tool()
def orchestrator_dispatch_mission(
    mission_id: str,
    objective: str,
    assigned_agent: str = "Kilo"
) -> str:
    """
    Dispatch a mission to an agent.
    
    Args:
        mission_id: Unique mission identifier
        objective: The objective of the mission
        assigned_agent: The agent assigned to the mission
    """
    # TODO: Connect to task management or agent dispatch system
    return f"Mission '{mission_id}' dispatched to {assigned_agent}. Objective: {objective}"


# ============================================================================
# Strategic Crucible Loop
# ============================================================================

# Import dependencies for the loop
# Note: In a distributed MCP architecture, we would call these via client.
# Here we import the service logic directly for reliability.
from mnemonic_cortex.app.services.ingestion_service import IngestionService
from mnemonic_cortex.app.synthesis.generator import SynthesisGenerator
from mcp_servers.cognitive.cortex.operations import CortexOperations

@mcp.tool()
def orchestrator_run_strategic_cycle(
    gap_description: str,
    research_report_path: str,
    days_to_synthesize: int = 1
) -> str:
    """
    Execute a full Strategic Crucible Loop: Ingest -> Synthesize -> Adapt -> Cache.
    
    Args:
        gap_description: Description of the strategic gap being addressed.
        research_report_path: Path to the new research report (markdown).
        days_to_synthesize: Window for adaptation packet generation.
        
    Returns:
        Summary of the cycle execution.
    """
    results = []
    results.append(f"--- Strategic Crucible Cycle: {gap_description} ---")
    
    # 1. Ingestion (Medium Memory Update)
    try:
        results.append(f"1. Ingesting Report: {research_report_path}")
        ingestion_service = IngestionService(
            project_root=PROJECT_ROOT
        )
        # We assume incremental ingest for a single report
        ingest_stats = ingestion_service.ingest_incremental([research_report_path])
        results.append(f"   - Ingestion Complete: {ingest_stats}")
    except Exception as e:
        return "\n".join(results) + f"\n[CRITICAL FAIL] Ingestion failed: {e}"

    # 2. Adaptation (Slow Memory Update Prep)
    try:
        results.append(f"2. Generating Adaptation Packet (Window: {days_to_synthesize} days)")
        generator = SynthesisGenerator(PROJECT_ROOT)
        packet = generator.generate_packet(days=days_to_synthesize)
        packet_path = generator.save_packet(packet)
        results.append(f"   - Packet Generated: {packet_path}")
        results.append(f"   - Packet ID: {packet.packet_id}")
    except Exception as e:
        return "\n".join(results) + f"\n[CRITICAL FAIL] Adaptation failed: {e}"

    # 3. Cache Update (Fast Memory Update)
    try:
        results.append(f"3. Waking Guardian Cache")
        # Initialize Cortex Ops to access cache logic
        cortex_ops = CortexOperations(PROJECT_ROOT) 
        # We need to inject the real cache instance if possible, or rely on the ops to create it
        # The current CortexOperations implementation creates MnemonicCache internally if not passed.
        # However, it needs DB_PATH etc from env.
        # Let's assume env vars are set or defaults work.
        
        # We call guardian_wakeup. In a real scenario, this might be an async tool call.
        # Here we call the method directly if available, or simulate it.
        # Looking at cortex/operations.py, guardian_wakeup is a method.
        wakeup_stats = cortex_ops.guardian_wakeup()
        results.append(f"   - Cache Updated: {wakeup_stats}")
    except Exception as e:
        results.append(f"   - [WARN] Cache update failed (non-critical): {e}")

    results.append("--- Cycle Complete ---")
    return "\n".join(results)


if __name__ == "__main__":
    mcp.run()

--- END OF FILE orchestrator/server.py ---

--- START OF FILE orchestrator/tools/__init__.py ---



--- END OF FILE orchestrator/tools/__init__.py ---

--- START OF FILE orchestrator/tools/cognitive.py ---

import os
import json
import time
from typing import List, Optional, Dict, Any
from pathlib import Path
from .safety import SafetyValidator
from .utils import write_command_file

# _write_command_file removed (using utils.write_command_file)

def create_cognitive_task(
    description: str,
    output_path: str,
    max_rounds: int = 5,
    force_engine: Optional[str] = None,
    max_cortex_queries: int = 5,
    input_artifacts: Optional[List[str]] = None,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Generate a command.json for Council deliberation.
    
    Args:
        description: High-level task description
        output_path: Where to save the result
        max_rounds: Maximum deliberation rounds (default: 5)
        force_engine: Force specific engine (gemini/openai/ollama)
        max_cortex_queries: Max RAG queries (default: 5)
        input_artifacts: Optional list of input file paths
        project_root: Root of the project
        config: MCP configuration
    """
    validator = SafetyValidator(project_root)
    
    # Validate output path
    res = validator.validate_cognitive_task(output_path)
    if not res.valid:
        return {"status": "error", "error": res.reason, "risk_level": res.risk_level}
        
    # Validate input artifacts
    if input_artifacts:
        for path in input_artifacts:
            res = validator.validate_path(path)
            if not res.valid:
                 return {"status": "error", "error": f"Invalid input artifact: {res.reason}", "risk_level": res.risk_level}

    command = {
        "task_description": description,
        "output_artifact_path": output_path,
        "config": {
            "max_rounds": max_rounds,
            "max_cortex_queries": max_cortex_queries
        }
    }
    
    if force_engine:
        command["config"]["force_engine"] = force_engine
        
    if input_artifacts:
        command["input_artifacts"] = input_artifacts
        
    cmd_path = write_command_file(command, project_root, config)
    
    return {
        "status": "success",
        "command_file": cmd_path,
        "message": "Cognitive task queued for Council deliberation"
    }

def create_development_cycle(
    description: str,
    project_name: str,
    output_path: str,
    max_rounds: int = 10,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Generate a command.json for a staged development cycle.
    """
    validator = SafetyValidator(project_root)
    res = validator.validate_path(output_path)
    if not res.valid:
        return {"status": "error", "error": res.reason, "risk_level": res.risk_level}

    command = {
        "task_description": description,
        "task_type": "development_cycle",
        "project_name": project_name,
        "output_artifact_path": output_path,
        "config": {
            "max_rounds": max_rounds
        }
    }
    
    cmd_path = write_command_file(command, project_root, config)
    
    return {
        "status": "success",
        "command_file": cmd_path,
        "message": f"Development cycle '{project_name}' queued"
    }

def query_mnemonic_cortex(
    query: str,
    output_path: str,
    max_results: int = 5,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Generate a command.json for a RAG query task.
    """
    validator = SafetyValidator(project_root)
    res = validator.validate_path(output_path)
    if not res.valid:
        return {"status": "error", "error": res.reason, "risk_level": res.risk_level}

    command = {
        "task_description": f"Query Cortex: {query}",
        "task_type": "rag_query",
        "query": query,
        "output_artifact_path": output_path,
        "config": {
            "max_results": max_results
        }
    }
    
    cmd_path = write_command_file(command, project_root, config)
    
    return {
        "status": "success",
        "command_file": cmd_path,
        "message": "RAG query queued"
    }

--- END OF FILE orchestrator/tools/cognitive.py ---

--- START OF FILE orchestrator/tools/mechanical.py ---

import os
import hashlib
from typing import List, Optional, Dict, Any
from pathlib import Path
from .safety import SafetyValidator
from .utils import write_command_file

def create_file_write_task(
    content: str,
    output_path: str,
    description: str,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Generate a command.json for writing a file.
    """
    validator = SafetyValidator(project_root)
    res = validator.validate_path(output_path)
    if not res.valid:
        return {"status": "error", "error": res.reason, "risk_level": res.risk_level}

    command = {
        "task_description": description,
        "task_type": "file_write",
        "file_operations": {
            "path": output_path,
            "content": content
        }
    }
    
    cmd_path = write_command_file(command, project_root, config)
    
    return {
        "status": "success",
        "command_file": cmd_path,
        "message": f"File write task for '{output_path}' queued"
    }

def create_git_commit_task(
    files: List[str],
    message: str,
    description: str,
    push: bool = False,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Generate a command.json for a git commit (P101 compliant).
    """
    validator = SafetyValidator(project_root)
    res = validator.validate_git_operation(files, message, push)
    if not res.valid:
        return {"status": "error", "error": res.reason, "risk_level": res.risk_level}

    # Protocol 101: Generate Manifest
    manifest = {}
    for file_path in files:
        abs_path = Path(project_root) / file_path
        if abs_path.exists():
            with open(abs_path, "rb") as f:
                file_hash = hashlib.sha256(f.read()).hexdigest()
            manifest[file_path] = file_hash
        else:
            # If file doesn't exist yet (new file), we can't hash it easily unless content was provided elsewhere
            # For now, we assume files exist or will be created by a previous step.
            # If this is a commit of existing files, they must exist.
            return {"status": "error", "error": f"File not found for hashing: {file_path}", "risk_level": "SAFE"}

    command = {
        "task_description": description,
        "task_type": "git_commit",
        "git_operations": {
            "files_to_add": files,
            "commit_message": message,
            "push_to_origin": push,
            "p101_manifest": manifest
        }
    }
    
    cmd_path = write_command_file(command, project_root, config)
    
    return {
        "status": "success",
        "command_file": cmd_path,
        "message": "Git commit task queued"
    }

--- END OF FILE orchestrator/tools/mechanical.py ---

--- START OF FILE orchestrator/tools/query.py ---

import os
import json
import glob
from typing import List, Optional, Dict, Any
from pathlib import Path

def get_orchestrator_status(
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Check if the orchestrator is running and healthy.
    """
    # In a real implementation, this might check a PID file or health endpoint.
    # For now, we check if the directory structure exists.
    orchestrator_dir = Path(project_root) / "council_orchestrator"
    
    if not orchestrator_dir.exists():
        return {
            "status": "offline",
            "message": "Orchestrator directory not found",
            "healthy": False
        }
        
    return {
        "status": "online", # Assumed for now
        "message": "Orchestrator infrastructure present",
        "healthy": True,
        "directory": str(orchestrator_dir)
    }

def list_recent_tasks(
    limit: int = 10,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> List[Dict[str, Any]]:
    """
    List recent tasks from the orchestrator logs/results.
    """
    orchestrator_config = config.get("orchestrator", {})
    results_dir_rel = orchestrator_config.get("results_directory", "../council_orchestrator/command_results/")
    
    # Resolve path
    if os.path.isabs(results_dir_rel):
        results_dir = Path(results_dir_rel)
    else:
        results_dir = Path(project_root) / "council_orchestrator" / "command_results"
        
    if not results_dir.exists():
        return []
        
    # Find JSON result files
    files = sorted(results_dir.glob("*.json"), key=os.path.getmtime, reverse=True)
    recent_files = files[:limit]
    
    tasks = []
    for f in recent_files:
        try:
            with open(f, "r") as json_file:
                data = json.load(json_file)
                tasks.append({
                    "task_id": f.stem,
                    "timestamp": os.path.getmtime(f),
                    "summary": data.get("summary", "No summary"),
                    "status": data.get("status", "unknown")
                })
        except Exception:
            continue
            
    return tasks

def get_task_result(
    task_id: str,
    project_root: str = ".",
    config: Dict[str, Any] = {}
) -> Dict[str, Any]:
    """
    Retrieve the result of a specific task.
    """
    orchestrator_config = config.get("orchestrator", {})
    results_dir_rel = orchestrator_config.get("results_directory", "../council_orchestrator/command_results/")
    
    if os.path.isabs(results_dir_rel):
        results_dir = Path(results_dir_rel)
    else:
        results_dir = Path(project_root) / "council_orchestrator" / "command_results"
        
    # Try to find the file
    # task_id might be the filename without extension or with
    if task_id.endswith(".json"):
        file_path = results_dir / task_id
    else:
        file_path = results_dir / f"{task_id}.json"
        
    if not file_path.exists():
        return {"status": "error", "error": "Task result not found"}
        
    try:
        with open(file_path, "r") as f:
            return json.load(f)
    except Exception as e:
        return {"status": "error", "error": str(e)}

--- END OF FILE orchestrator/tools/query.py ---

--- START OF FILE orchestrator/tools/safety.py ---

import os
import re
import json
from pathlib import Path
from typing import List, Optional, Dict, Any

class ValidationResult:
    def __init__(self, valid: bool, reason: str = "", risk_level: str = "SAFE"):
        self.valid = valid
        self.reason = reason
        self.risk_level = risk_level

class SafetyValidator:
    """Validates commands against safety rules and git safety protocols."""
    
    def __init__(self, project_root: str, config_path: str = None):
        self.project_root = Path(project_root).resolve()
        
        # Load config
        if config_path:
            self.config = self._load_config(config_path)
        else:
            # Default config path relative to this file
            config_path = Path(__file__).parent.parent / "config" / "mcp_config.json"
            self.config = self._load_config(str(config_path))
            
        self.safety_config = self.config.get("safety", {})
        self.protected_paths = self.safety_config.get("protected_paths", [])
        self.allowed_extensions = set(self.safety_config.get("allowed_extensions", []))
        
        # Prohibited patterns for git commands
        self.prohibited_patterns = [
            r"git\s+reset\s+--hard",
            r"git\s+push\s+(-f|--force)",
            r"git\s+rebase",
            r"rm\s+-rf",
        ]

    def _load_config(self, path: str) -> Dict[str, Any]:
        try:
            with open(path, 'r') as f:
                return json.load(f)
        except Exception as e:
            print(f"Warning: Could not load config from {path}: {e}")
            return {}

    def validate_path(self, path: str) -> ValidationResult:
        """Validate that a file path is safe to write to."""
        try:
            # Resolve absolute path
            if os.path.isabs(path):
                abs_path = Path(path).resolve()
            else:
                abs_path = (self.project_root / path).resolve()
            
            # Check if path is within project root
            if not str(abs_path).startswith(str(self.project_root)):
                return ValidationResult(False, f"Path traversal detected: {path}", "DANGEROUS")
            
            # Check protected paths
            rel_path = abs_path.relative_to(self.project_root)
            for protected in self.protected_paths:
                if str(rel_path).startswith(protected):
                    return ValidationResult(False, f"Cannot modify protected path: {rel_path}", "DANGEROUS")
            
            # Check extension
            if self.allowed_extensions and abs_path.suffix not in self.allowed_extensions:
                 return ValidationResult(False, f"File extension not allowed: {abs_path.suffix}", "MODERATE")

            return ValidationResult(True, risk_level="SAFE")
            
        except Exception as e:
            return ValidationResult(False, f"Path validation error: {str(e)}", "DANGEROUS")

    def validate_git_operation(self, files: List[str], message: str, push: bool) -> ValidationResult:
        """Validate git commit operation against safety rules."""
        
        # Validate all files
        for file_path in files:
            res = self.validate_path(file_path)
            if not res.valid:
                return res
        
        # Validate commit message format (conventional commits)
        # Regex for: type(scope): description or type: description
        conventional_commit_pattern = r"^(feat|fix|docs|style|refactor|perf|test|build|ci|chore|revert)(\(.+\))?: .+"
        if not re.match(conventional_commit_pattern, message):
            return ValidationResult(
                valid=False,
                reason="Commit message must follow conventional commit format (e.g., 'feat(scope): description')",
                risk_level="MODERATE"
            )
        
        # Check for prohibited patterns in message (injection check)
        for pattern in self.prohibited_patterns:
            if re.search(pattern, message):
                 return ValidationResult(False, "Commit message contains prohibited patterns", "DANGEROUS")

        # Check if we're on main branch (requires extra caution)
        # Note: In a real implementation, we'd check the current branch via git
        # For now, we assume push=True is risky if not verified
        if push:
             # We allow push if it's explicitly requested, but mark it as MODERATE risk
             # The tool implementation should decide whether to block it based on user approval settings
             return ValidationResult(True, risk_level="MODERATE")
        
        return ValidationResult(valid=True, risk_level="SAFE")

    def validate_cognitive_task(self, output_path: str) -> ValidationResult:
        """Validate cognitive task parameters."""
        return self.validate_path(output_path)

--- END OF FILE orchestrator/tools/safety.py ---

--- START OF FILE orchestrator/tools/utils.py ---

import os
import json
from typing import Dict, Any
from pathlib import Path

def write_command_file(command: Dict[str, Any], project_root: str, config: Dict[str, Any]) -> str:
    """Helper to write command.json to the orchestrator directory."""
    orchestrator_config = config.get("orchestrator", {})
    rel_path = orchestrator_config.get("command_file_path", "mcp_servers/orchestrator/command.json")
    
    # Resolve absolute path
    if os.path.isabs(rel_path):
        cmd_path = Path(rel_path)
    else:
        # Default to project_root/mcp_servers/orchestrator/command.json
        cmd_path = Path(project_root) / "mcp_servers" / "orchestrator" / "command.json"

    # Ensure directory exists
    cmd_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(cmd_path, "w") as f:
        json.dump(command, f, indent=2)
        
    return str(cmd_path)

--- END OF FILE orchestrator/tools/utils.py ---

--- START OF FILE protocol/README.md ---

# Protocol MCP Server

MCP server for managing system protocols in `01_PROTOCOLS/`.

## Purpose

The Protocol MCP provides structured access to the project's protocol library, ensuring consistent formatting and metadata management.

## Tools

### `protocol_create`
Create a new protocol.
- **Args:** `number`, `title`, `status`, `classification`, `version`, `authority`, `content`, `linked_protocols` (optional)
- **Returns:** Protocol number and file path

### `protocol_update`
Update an existing protocol.
- **Args:** `number`, `updates` (dict), `reason`
- **Returns:** Updated fields

### `protocol_get`
Retrieve a specific protocol.
- **Args:** `number`
- **Returns:** Protocol details

### `protocol_list`
List protocols.
- **Args:** `status` (optional filter)
- **Returns:** List of protocols

### `protocol_search`
Search protocols by content.
- **Args:** `query`
- **Returns:** List of matching protocols

## Safety Rules

1.  **Unique Numbers:** Protocol numbers must be unique.
2.  **Header Integrity:** All protocols maintain standard header format.
3.  **No Deletion:** Protocols can be marked as `DEPRECATED` but never deleted.

## Configuration

Add to `mcp_config.json`:

```json
"protocol": {
  "displayName": "Protocol MCP",
  "command": "/path/to/venv/bin/python",
  "args": ["-m", "mcp_servers.system.protocol.server"],
  "env": {
    "PROJECT_ROOT": "/path/to/project",
    "PYTHONPATH": "/path/to/project"
  }
}
```

--- END OF FILE protocol/README.md ---

--- START OF FILE protocol/__init__.py ---



--- END OF FILE protocol/__init__.py ---

--- START OF FILE protocol/models.py ---

"""
Data models for the Protocol MCP server.
"""
from dataclasses import dataclass
from enum import Enum
from typing import Optional


class ProtocolStatus(str, Enum):
    PROPOSED = "PROPOSED"
    CANONICAL = "CANONICAL"
    DEPRECATED = "DEPRECATED"


@dataclass
class Protocol:
    number: int
    title: str
    status: ProtocolStatus
    classification: str
    version: str
    authority: str
    content: str
    linked_protocols: Optional[str] = None
    
    @property
    def filename(self) -> str:
        """Generate filename for the protocol."""
        # Format: 00_Protocol_Title.md
        slug = self.title.replace(" ", "_").replace("-", "_")
        # Remove non-alphanumeric chars except underscore
        slug = "".join(c for c in slug if c.isalnum() or c == "_")
        return f"{self.number:02d}_{slug}.md"


PROTOCOL_TEMPLATE = """# Protocol {number}: {title}

**Status:** {status}
**Classification:** {classification}
**Version:** {version}
**Authority:** {authority}
{linked_protocols_line}
---

{content}
"""

--- END OF FILE protocol/models.py ---

--- START OF FILE protocol/operations.py ---

"""
File operations for Protocol MCP.
"""
import os
import re
from typing import List, Optional, Dict, Any
from .models import Protocol, ProtocolStatus, PROTOCOL_TEMPLATE
from .validator import ProtocolValidator


class ProtocolOperations:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        self.validator = ProtocolValidator(base_dir)
        
        # Ensure directory exists
        if not os.path.exists(base_dir):
            os.makedirs(base_dir)

    def create_protocol(
        self,
        number: int,
        title: str,
        status: str,
        classification: str,
        version: str,
        authority: str,
        content: str,
        linked_protocols: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create a new protocol."""
        # Validate inputs
        self.validator.validate_required_fields(title, classification, version, authority, content)
        self.validator.validate_protocol_number(number)
        
        # Create protocol object
        protocol = Protocol(
            number=number,
            title=title,
            status=ProtocolStatus(status),
            classification=classification,
            version=version,
            authority=authority,
            content=content,
            linked_protocols=linked_protocols
        )
        
        # Generate linked protocols line
        linked_line = f"**Linked Protocols:** {linked_protocols}" if linked_protocols else ""
        
        # Generate content
        file_content = PROTOCOL_TEMPLATE.format(
            number=protocol.number,
            title=protocol.title,
            status=protocol.status.value,
            classification=protocol.classification,
            version=protocol.version,
            authority=protocol.authority,
            linked_protocols_line=linked_line,
            content=protocol.content
        )
        
        # Write file
        file_path = os.path.join(self.base_dir, protocol.filename)
        with open(file_path, "w") as f:
            f.write(file_content)
            
        return {
            "protocol_number": number,
            "file_path": file_path,
            "status": protocol.status.value
        }

    def update_protocol(
        self,
        number: int,
        updates: Dict[str, Any],
        reason: str
    ) -> Dict[str, Any]:
        """Update an existing protocol."""
        # Find file
        file_path = self._find_protocol_file(number)
        if not file_path:
            raise ValueError(f"Protocol {number} not found")
            
        # Read existing
        current_protocol = self.get_protocol(number)
        
        # Apply updates
        new_title = updates.get("title", current_protocol["title"])
        new_content = updates.get("content", current_protocol["content"])
        new_status = updates.get("status", current_protocol["status"])
        new_classification = updates.get("classification", current_protocol["classification"])
        new_version = updates.get("version", current_protocol["version"])
        new_authority = updates.get("authority", current_protocol["authority"])
        new_linked = updates.get("linked_protocols", current_protocol.get("linked_protocols", ""))
        
        # Generate linked protocols line
        linked_line = f"**Linked Protocols:** {new_linked}" if new_linked else ""
        
        # Re-generate content
        file_content = PROTOCOL_TEMPLATE.format(
            number=number,
            title=new_title,
            status=new_status,
            classification=new_classification,
            version=new_version,
            authority=new_authority,
            linked_protocols_line=linked_line,
            content=new_content
        )
        
        # Write file
        with open(file_path, "w") as f:
            f.write(file_content)
            
        return {
            "protocol_number": number,
            "updated_fields": list(updates.keys())
        }

    def get_protocol(self, number: int) -> Dict[str, Any]:
        """Retrieve a protocol."""
        file_path = self._find_protocol_file(number)
        if not file_path:
            raise ValueError(f"Protocol {number} not found")
            
        with open(file_path, "r") as f:
            content = f.read()
            
        return self._parse_protocol(content, number)

    def list_protocols(self, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """List protocols."""
        if not os.path.exists(self.base_dir):
            return []
            
        files = sorted(os.listdir(self.base_dir))
        protocols = []
        
        for f in files:
            if not f.endswith(".md") or f.startswith("."):
                continue
                
            match = re.match(r"(\d+)_", f)
            if match:
                number = int(match.group(1))
                try:
                    protocol = self.get_protocol(number)
                    if status is None or protocol["status"] == status:
                        protocols.append(protocol)
                except Exception:
                    continue
                    
        return protocols

    def search_protocols(self, query: str) -> List[Dict[str, Any]]:
        """Search protocols."""
        if not os.path.exists(self.base_dir):
            return []
            
        results = []
        files = sorted(os.listdir(self.base_dir))
        
        for f in files:
            if not f.endswith(".md") or f.startswith("."):
                continue
                
            path = os.path.join(self.base_dir, f)
            with open(path, "r") as file:
                content = file.read()
                
            if query.lower() in content.lower():
                match = re.match(r"(\d+)_", f)
                if match:
                    number = int(match.group(1))
                    results.append(self._parse_protocol(content, number))
                    
        return results

    def _find_protocol_file(self, number: int) -> Optional[str]:
        """Find file path for a protocol number."""
        if not os.path.exists(self.base_dir):
            return None
            
        for f in os.listdir(self.base_dir):
            match = re.match(r"(\d+)_", f)
            if match and int(match.group(1)) == number:
                return os.path.join(self.base_dir, f)
        return None

    def _parse_protocol(self, content: str, number: int) -> Dict[str, Any]:
        """Parse markdown content into protocol dict."""
        lines = content.split("\n")
        metadata = {}
        body_start = 0
        
        for i, line in enumerate(lines):
            if line.startswith("**Status:**"):
                metadata["status"] = line.replace("**Status:**", "").strip()
            elif line.startswith("**Classification:**"):
                metadata["classification"] = line.replace("**Classification:**", "").strip()
            elif line.startswith("**Version:**"):
                metadata["version"] = line.replace("**Version:**", "").strip()
            elif line.startswith("**Authority:**"):
                metadata["authority"] = line.replace("**Authority:**", "").strip()
            elif line.startswith("**Linked Protocols:**"):
                metadata["linked_protocols"] = line.replace("**Linked Protocols:**", "").strip()
            elif line.strip() == "---":
                body_start = i + 1
                break
        
        # Extract title from H1
        title = "Unknown Protocol"
        for line in lines:
            if line.startswith("# Protocol"):
                parts = line.split(":", 1)
                if len(parts) > 1:
                    title = parts[1].strip()
                break
        
        return {
            "number": number,
            "title": title,
            "status": metadata.get("status", "PROPOSED"),
            "classification": metadata.get("classification", ""),
            "version": metadata.get("version", "1.0"),
            "authority": metadata.get("authority", ""),
            "linked_protocols": metadata.get("linked_protocols", ""),
            "content": "\n".join(lines[body_start:]).strip() if body_start > 0 else content
        }

--- END OF FILE protocol/operations.py ---

--- START OF FILE protocol/requirements.txt ---

fastmcp

--- END OF FILE protocol/requirements.txt ---

--- START OF FILE protocol/server.py ---

from fastmcp import FastMCP
import os
from typing import Optional, Dict, Any, List
from .operations import ProtocolOperations

# Initialize FastMCP
mcp = FastMCP("project_sanctuary.protocol")

# Configuration
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")
PROTOCOL_DIR = os.path.join(PROJECT_ROOT, "01_PROTOCOLS")

# Initialize operations
ops = ProtocolOperations(PROTOCOL_DIR)


@mcp.tool()
def protocol_create(
    number: int,
    title: str,
    status: str,
    classification: str,
    version: str,
    authority: str,
    content: str,
    linked_protocols: Optional[str] = None
) -> str:
    """
    Create a new protocol.
    
    Args:
        number: Protocol number (e.g., 117)
        title: Protocol title
        status: PROPOSED, CANONICAL, or DEPRECATED
        classification: Classification (e.g., "Foundational Framework")
        version: Version string (e.g., "1.0")
        authority: Authority/author
        content: Protocol content (markdown)
        linked_protocols: Optional linked protocol references
    """
    try:
        result = ops.create_protocol(
            number, title, status, classification, version, authority, content, linked_protocols
        )
        return f"Created Protocol {result['protocol_number']}: {result['file_path']}"
    except Exception as e:
        return f"Error creating protocol: {str(e)}"


@mcp.tool()
def protocol_update(
    number: int,
    updates: Dict[str, Any],
    reason: str
) -> str:
    """
    Update an existing protocol.
    
    Args:
        number: Protocol number to update
        updates: Dictionary of fields to update
        reason: Reason for the update
    """
    try:
        result = ops.update_protocol(number, updates, reason)
        return f"Updated Protocol {result['protocol_number']}. Fields: {', '.join(result['updated_fields'])}"
    except Exception as e:
        return f"Error updating protocol: {str(e)}"


@mcp.tool()
def protocol_get(number: int) -> str:
    """
    Retrieve a specific protocol.
    
    Args:
        number: Protocol number to retrieve
    """
    try:
        protocol = ops.get_protocol(number)
        return f"""Protocol {protocol['number']}: {protocol['title']}
Status: {protocol['status']}
Classification: {protocol['classification']}
Version: {protocol['version']}
Authority: {protocol['authority']}
Linked Protocols: {protocol.get('linked_protocols', 'None')}

{protocol['content']}"""
    except Exception as e:
        return f"Error retrieving protocol: {str(e)}"


@mcp.tool()
def protocol_list(status: Optional[str] = None) -> str:
    """
    List protocols.
    
    Args:
        status: Optional status filter (PROPOSED, CANONICAL, DEPRECATED)
    """
    try:
        protocols = ops.list_protocols(status)
        if not protocols:
            return "No protocols found."
            
        output = [f"Found {len(protocols)} protocol(s):"]
        for p in protocols:
            output.append(f"- {p['number']:03d}: {p['title']} [{p['status']}] v{p['version']}")
        return "\n".join(output)
    except Exception as e:
        return f"Error listing protocols: {str(e)}"


@mcp.tool()
def protocol_search(query: str) -> str:
    """
    Search protocols by content.
    
    Args:
        query: Search query string
    """
    try:
        results = ops.search_protocols(query)
        if not results:
            return f"No protocols found matching '{query}'"
            
        output = [f"Found {len(results)} protocol(s) matching '{query}':"]
        for r in results:
            output.append(f"- {r['number']:03d}: {r['title']}")
        return "\n".join(output)
    except Exception as e:
        return f"Error searching protocols: {str(e)}"


@mcp.tool()
def protocol_validate_action(
    action_description: str,
    protocol_ids: Optional[List[int]] = None
) -> str:
    """
    Validate a proposed action against specific protocols.
    
    Args:
        action_description: Description of the action to validate
        protocol_ids: Optional list of protocol numbers to check against. 
                     If None, checks against all CANONICAL protocols (expensive).
    """
    # TODO: Implement actual validation logic using LLM or rule engine.
    # For now, this is a placeholder that retrieves the protocols and prompts the user/agent to check.
    try:
        if protocol_ids:
            protocols = [ops.get_protocol(pid) for pid in protocol_ids]
        else:
            # Limit to top 5 canonical protocols if not specified to avoid context overflow in placeholder
            protocols = ops.list_protocols(status="CANONICAL")[:5]
            
        output = [f"Validation Context for Action: '{action_description}'\n"]
        output.append("Relevant Protocols:")
        for p in protocols:
            output.append(f"\n--- Protocol {p['number']}: {p['title']} ---\n{p['content'][:500]}...") # Truncate for brevity
            
        output.append("\n\n[SYSTEM] Please review the above protocols and determine if the action is compliant.")
        return "\n".join(output)
    except Exception as e:
        return f"Error validating action: {str(e)}"


if __name__ == "__main__":
    mcp.run()

--- END OF FILE protocol/server.py ---

--- START OF FILE protocol/validator.py ---

"""
Validation logic for Protocol MCP.
"""
import os
import re
from typing import Optional
from .models import ProtocolStatus


class ProtocolValidator:
    def __init__(self, base_dir: str):
        self.base_dir = base_dir

    def validate_protocol_number(self, number: int) -> None:
        """Ensure protocol number is unique."""
        if not os.path.exists(self.base_dir):
            return
            
        files = os.listdir(self.base_dir)
        for f in files:
            # Match files like "00_Title.md" or "100_Title.md"
            match = re.match(r"(\d+)_", f)
            if match and int(match.group(1)) == number:
                raise ValueError(f"Protocol {number} already exists: {f}")

    def validate_required_fields(
        self, 
        title: str, 
        classification: str, 
        version: str, 
        authority: str, 
        content: str
    ) -> None:
        """Validate that required fields are present and not empty."""
        if not title or not title.strip():
            raise ValueError("Title is required")
        if not classification or not classification.strip():
            raise ValueError("Classification is required")
        if not version or not version.strip():
            raise ValueError("Version is required")
        if not authority or not authority.strip():
            raise ValueError("Authority is required")
        if not content or not content.strip():
            raise ValueError("Content is required")

--- END OF FILE protocol/validator.py ---

--- START OF FILE requirements.txt ---

fastmcp>=0.1.0
pydantic>=2.0.0

--- END OF FILE requirements.txt ---

--- START OF FILE start_mcp_servers.sh ---

#!/bin/bash

# Start MCP Servers for Project Sanctuary
# Usage: ./start_mcp_servers.sh

echo "Starting Project Sanctuary MCP Servers..."

# Check for virtual environment
if [ -z "$VIRTUAL_ENV" ]; then
    echo "WARNING: No virtual environment detected. It is recommended to run this inside a venv."
fi

# Define server paths
CORTEX_SERVER="cognitive/cortex/server.py"
CHRONICLE_SERVER="chronicle/server.py"
PROTOCOL_SERVER="protocol/server.py"
ORCHESTRATOR_SERVER="orchestrator/server.py"

# Function to check if file exists
check_file() {
    if [ ! -f "$1" ]; then
        echo "ERROR: Server file not found: $1"
        exit 1
    fi
}

check_file "$CORTEX_SERVER"
check_file "$CHRONICLE_SERVER"
check_file "$PROTOCOL_SERVER"
check_file "$ORCHESTRATOR_SERVER"

echo "All server files located."
echo ""
echo "To run a specific server, use:"
echo "  python $CORTEX_SERVER"
echo "  python $CHRONICLE_SERVER"
echo "  python $PROTOCOL_SERVER"
echo "  python $ORCHESTRATOR_SERVER"
echo ""
echo "Note: These servers are designed to be run by an MCP Client (like Claude Desktop)."
echo "Please configure your client to point to these scripts."

--- END OF FILE start_mcp_servers.sh ---

--- START OF FILE system/__init__.py ---



--- END OF FILE system/__init__.py ---

--- START OF FILE system/forge/README.md ---

# Forge MCP Server

**Domain:** `project_sanctuary.system.forge`  
**Category:** System / Model Domain  
**Hardware:** CUDA GPU (for fine-tuning operations)

## Overview

The Forge MCP server provides tools for interacting with the fine-tuned Sanctuary model and managing the model lifecycle. Currently implements model querying via Ollama.

## Tools

### 1. `query_sanctuary_model`

Query the fine-tuned Sanctuary-Qwen2 model for specialized knowledge and decision-making.

**Parameters:**
- `prompt` (string, required): The question or prompt to send to the model
- `temperature` (float, optional): Sampling temperature 0.0-2.0 (default: 0.7)
- `max_tokens` (int, optional): Maximum tokens to generate 1-8192 (default: 2048)
- `system_prompt` (string, optional): System prompt to set context

**Returns:** JSON with model response and metadata

**Example:**
```python
query_sanctuary_model("What is the strategic priority for Q1 2025?")

query_sanctuary_model(
    prompt="Explain Protocol 101",
    temperature=0.3,
    system_prompt="You are a Sanctuary protocol expert"
)
```

### 2. `check_sanctuary_model_status`

Check if the Sanctuary model is available and ready to use in Ollama.

**Parameters:** None

**Returns:** JSON with model availability status

**Example:**
```python
check_sanctuary_model_status()
```

## Model Information

**Model:** `hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:Q4_K_M`  
**Base:** Qwen2-7B  
**Quantization:** Q4_K_M (4-bit)  
**Training:** Fine-tuned on Project Sanctuary knowledge

## Requirements

- Ollama installed and running
- Sanctuary model loaded in Ollama
- Python package: `ollama`

## Installation

```bash
# Install Ollama Python package
pip install ollama

# Verify model is loaded
ollama list | grep Sanctuary
```

## Running the Server

```bash
# Set project root
export PROJECT_ROOT=/path/to/Project_Sanctuary

# Run server
python -m mcp_servers.system.forge.server
```

## Integration with Claude Desktop

Add to `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "forge": {
      "command": "python",
      "args": ["-m", "mcp_servers.system.forge.server"],
      "env": {
        "PROJECT_ROOT": "/path/to/Project_Sanctuary"
      }
    }
  }
}
```

## Future Tools (Not Yet Implemented)

- `initiate_model_forge` - Start fine-tuning job
- `get_forge_job_status` - Check training progress
- `package_and_deploy_artifact` - Convert and deploy model
- `run_inference_test` - Test model quality
- `publish_to_registry` - Upload to Hugging Face

## Safety

- Input validation on all parameters
- Temperature clamped to 0.0-2.0
- Max tokens clamped to 1-8192
- Prompt length limits enforced
- Error handling for missing dependencies

## Architecture

```
forge/
├── __init__.py          # Package exports
├── server.py            # FastMCP server with tools
├── operations.py        # Core Ollama integration
├── validator.py         # Input validation
├── models.py            # Data models
└── README.md            # This file
```

## Status

**Implemented:** ✅ Model querying via Ollama  
**Pending:** Fine-tuning lifecycle tools (requires CUDA GPU setup)

--- END OF FILE system/forge/README.md ---

--- START OF FILE system/forge/__init__.py ---

"""
Forge MCP Server
Domain: project_sanctuary.system.forge

Provides MCP tools for interacting with the fine-tuned Sanctuary model
and managing the model fine-tuning lifecycle.
"""

__all__ = ['ForgeOperations', 'ForgeValidator', 'ValidationError', 'ModelQueryResponse']

from .operations import ForgeOperations
from .validator import ForgeValidator, ValidationError
from .models import ModelQueryResponse

--- END OF FILE system/forge/__init__.py ---

--- START OF FILE system/forge/models.py ---

"""
Forge MCP Models
Domain: project_sanctuary.system.forge

Data models for Forge MCP operations.
"""
from dataclasses import dataclass
from typing import Optional, Dict, Any


@dataclass
class ModelQueryResponse:
    """Response from querying the Sanctuary model."""
    model: str
    response: str
    prompt_tokens: Optional[int] = None
    completion_tokens: Optional[int] = None
    total_tokens: Optional[int] = None
    temperature: Optional[float] = None
    status: str = "success"
    error: Optional[str] = None


def to_dict(obj: Any) -> Dict[str, Any]:
    """Convert dataclass to dictionary."""
    if hasattr(obj, '__dataclass_fields__'):
        return {k: v for k, v in obj.__dict__.items() if v is not None}
    return obj

--- END OF FILE system/forge/models.py ---

--- START OF FILE system/forge/operations.py ---

"""
Forge MCP Operations
Domain: project_sanctuary.system.forge

Core operations for interacting with the fine-tuned Sanctuary model.
"""
import os
from typing import Optional, List, Dict, Any
from .models import ModelQueryResponse


class ForgeOperations:
    """Operations for Forge MCP server."""
    
    def __init__(self, project_root: str):
        """Initialize Forge operations.
        
        Args:
            project_root: Path to project root directory
        """
        self.project_root = project_root
        self.sanctuary_model = "hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:Q4_K_M"
    
    def query_sanctuary_model(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2048,
        system_prompt: Optional[str] = None
    ) -> ModelQueryResponse:
        """Query the fine-tuned Sanctuary model via Ollama.
        
        Args:
            prompt: The user prompt/question
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens to generate
            system_prompt: Optional system prompt for context
            
        Returns:
            ModelQueryResponse with the model's answer
        """
        try:
            import ollama
            
            # Build messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # Query Ollama
            response = ollama.chat(
                model=self.sanctuary_model,
                messages=messages,
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )
            
            # Extract response
            answer = response['message']['content']
            
            # Get token counts if available
            prompt_tokens = response.get('prompt_eval_count')
            completion_tokens = response.get('eval_count')
            total_tokens = (prompt_tokens or 0) + (completion_tokens or 0) if prompt_tokens and completion_tokens else None
            
            return ModelQueryResponse(
                model=self.sanctuary_model,
                response=answer,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                temperature=temperature,
                status="success"
            )
            
        except ImportError:
            return ModelQueryResponse(
                model=self.sanctuary_model,
                response="",
                status="error",
                error="ollama package not installed. Install with: pip install ollama"
            )
        except Exception as e:
            return ModelQueryResponse(
                model=self.sanctuary_model,
                response="",
                status="error",
                error=f"Failed to query model: {str(e)}"
            )
    
    def check_model_availability(self) -> Dict[str, Any]:
        """Check if the Sanctuary model is available in Ollama.
        
        Returns:
            Dictionary with availability status
        """
        try:
            import ollama
            
            # List available models
            models_response = ollama.list()
            
            # Extract model names - handle different response formats
            if isinstance(models_response, dict):
                models_list = models_response.get('models', [])
            else:
                models_list = models_response
            
            model_names = [m.get('name', m.get('model', str(m))) if isinstance(m, dict) else str(m) for m in models_list]
            
            # Check if our model is available
            is_available = any(self.sanctuary_model in name for name in model_names)
            
            return {
                "status": "success",
                "model": self.sanctuary_model,
                "available": is_available,
                "all_models": model_names
            }
            
        except ImportError:
            return {
                "status": "error",
                "error": "ollama package not installed"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

--- END OF FILE system/forge/operations.py ---

--- START OF FILE system/forge/server.py ---

"""
Forge MCP Server
Domain: project_sanctuary.system.forge

Provides MCP tools for interacting with the fine-tuned Sanctuary model.
"""
from fastmcp import FastMCP
from .operations import ForgeOperations
from .validator import ForgeValidator, ValidationError
from .models import to_dict
import os
import json
from typing import Optional

# Initialize FastMCP with canonical domain name
mcp = FastMCP("project_sanctuary.system.forge")

# Initialize operations and validator
PROJECT_ROOT = os.environ.get("PROJECT_ROOT", ".")
forge_ops = ForgeOperations(PROJECT_ROOT)
forge_validator = ForgeValidator(PROJECT_ROOT)


@mcp.tool()
def query_sanctuary_model(
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 2048,
    system_prompt: Optional[str] = None
) -> str:
    """
    Query the fine-tuned Sanctuary model for specialized knowledge and decision-making.
    
    This tool enables LLM assistants to consult the custom-trained Sanctuary-Qwen2
    model for Project Sanctuary-specific knowledge, strategic insights, and
    protocol-aware responses.
    
    Args:
        prompt: The question or prompt to send to the Sanctuary model
        temperature: Sampling temperature (0.0-2.0, default: 0.7)
                    Lower = more focused, Higher = more creative
        max_tokens: Maximum tokens to generate (1-8192, default: 2048)
        system_prompt: Optional system prompt to set context
        
    Returns:
        JSON string with the model's response and metadata
        
    Example:
        query_sanctuary_model("What is the strategic priority for Q1 2025?")
        query_sanctuary_model(
            prompt="Explain Protocol 101",
            temperature=0.3,
            system_prompt="You are a Sanctuary protocol expert"
        )
    """
    try:
        # Validate inputs
        validated = forge_validator.validate_query_sanctuary_model(
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            system_prompt=system_prompt
        )
        
        # Query the model
        response = forge_ops.query_sanctuary_model(
            prompt=validated["prompt"],
            temperature=validated["temperature"],
            max_tokens=validated["max_tokens"],
            system_prompt=validated["system_prompt"]
        )
        
        # Convert to dict and return as JSON
        result = to_dict(response)
        return json.dumps(result, indent=2)
        
    except ValidationError as e:
        return json.dumps({
            "status": "error",
            "error": f"Validation error: {str(e)}"
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e)
        }, indent=2)


@mcp.tool()
def check_sanctuary_model_status() -> str:
    """
    Check if the Sanctuary model is available and ready to use.
    
    Verifies that the fine-tuned Sanctuary-Qwen2 model is loaded in Ollama
    and ready for queries.
    
    Returns:
        JSON string with model availability status
        
    Example:
        check_sanctuary_model_status()
    """
    try:
        result = forge_ops.check_model_availability()
        return json.dumps(result, indent=2)
    except Exception as e:
        return json.dumps({
            "status": "error",
            "error": str(e)
        }, indent=2)


if __name__ == "__main__":
    mcp.run()

--- END OF FILE system/forge/server.py ---

--- START OF FILE system/forge/test_forge.py ---

#!/usr/bin/env python3
"""
Test script for Forge MCP server.
Verifies that the query_sanctuary_model tool works correctly.
"""
import sys
import os

# Add project root to path
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
sys.path.insert(0, PROJECT_ROOT)

from mcp_servers.system.forge.operations import ForgeOperations

def test_model_availability():
    """Test if the Sanctuary model is available."""
    print("=" * 60)
    print("Testing Sanctuary Model Availability")
    print("=" * 60)
    
    forge_ops = ForgeOperations(PROJECT_ROOT)
    result = forge_ops.check_model_availability()
    
    print(f"\nStatus: {result['status']}")
    if result['status'] == 'success':
        print(f"Model: {result['model']}")
        print(f"Available: {result['available']}")
        print(f"\nAll models in Ollama:")
        for model in result.get('all_models', []):
            print(f"  - {model}")
    else:
        print(f"Error: {result.get('error')}")
    
    return result['status'] == 'success' and result.get('available', False)

def test_model_query():
    """Test querying the Sanctuary model."""
    print("\n" + "=" * 60)
    print("Testing Sanctuary Model Query")
    print("=" * 60)
    
    forge_ops = ForgeOperations(PROJECT_ROOT)
    
    prompt = "What is Protocol 101?"
    print(f"\nPrompt: {prompt}")
    print("\nQuerying model...")
    
    response = forge_ops.query_sanctuary_model(
        prompt=prompt,
        temperature=0.7,
        max_tokens=500
    )
    
    print(f"\nStatus: {response.status}")
    if response.status == "success":
        print(f"Model: {response.model}")
        print(f"Temperature: {response.temperature}")
        print(f"Tokens: {response.total_tokens}")
        print(f"\nResponse:\n{response.response}")
    else:
        print(f"Error: {response.error}")
    
    return response.status == "success"

def main():
    """Run all tests."""
    print("\n🔥 Forge MCP Server Test Suite 🔥\n")
    
    # Test 1: Model availability
    availability_ok = test_model_availability()
    
    if not availability_ok:
        print("\n❌ Model not available. Please ensure:")
        print("   1. Ollama is installed and running")
        print("   2. Sanctuary model is loaded: ollama list")
        print("   3. Python ollama package is installed: pip install ollama")
        return 1
    
    # Test 2: Model query
    query_ok = test_model_query()
    
    # Summary
    print("\n" + "=" * 60)
    print("Test Summary")
    print("=" * 60)
    print(f"Model Availability: {'✅ PASS' if availability_ok else '❌ FAIL'}")
    print(f"Model Query: {'✅ PASS' if query_ok else '❌ FAIL'}")
    
    if availability_ok and query_ok:
        print("\n🎉 All tests passed! Forge MCP is ready to use.")
        return 0
    else:
        print("\n❌ Some tests failed. Please review errors above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())

--- END OF FILE system/forge/test_forge.py ---

--- START OF FILE system/forge/validator.py ---

"""
Forge MCP Validator
Domain: project_sanctuary.system.forge

Validation logic for Forge MCP operations.
"""
from typing import Optional


class ValidationError(Exception):
    """Raised when validation fails."""
    pass


class ForgeValidator:
    """Validator for Forge MCP operations."""
    
    def __init__(self, project_root: str):
        """Initialize validator.
        
        Args:
            project_root: Path to project root directory
        """
        self.project_root = project_root
    
    def validate_query_sanctuary_model(
        self,
        prompt: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> dict:
        """Validate query_sanctuary_model parameters.
        
        Args:
            prompt: User prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens
            system_prompt: Optional system prompt
            
        Returns:
            Validated parameters dictionary
            
        Raises:
            ValidationError: If validation fails
        """
        # Validate prompt
        if not prompt or not prompt.strip():
            raise ValidationError("Prompt cannot be empty")
        
        if len(prompt) > 10000:
            raise ValidationError("Prompt too long (max 10000 characters)")
        
        # Validate temperature
        if not 0.0 <= temperature <= 2.0:
            raise ValidationError("Temperature must be between 0.0 and 2.0")
        
        # Validate max_tokens
        if not 1 <= max_tokens <= 8192:
            raise ValidationError("max_tokens must be between 1 and 8192")
        
        # Validate system_prompt if provided
        if system_prompt and len(system_prompt) > 5000:
            raise ValidationError("System prompt too long (max 5000 characters)")
        
        return {
            "prompt": prompt.strip(),
            "temperature": temperature,
            "max_tokens": max_tokens,
            "system_prompt": system_prompt.strip() if system_prompt else None
        }

--- END OF FILE system/forge/validator.py ---

--- START OF FILE system/git_workflow/README.md ---

# Git Workflow MCP Server

**Domain:** `project_sanctuary.system.git_workflow`  
**Version:** 2.0.0  
**Status:** Production Ready (Protocol 101 v3.0 Compliant)

---

## Overview

The Git Workflow MCP server provides **Protocol 101 v3.0-compliant git operations** with an opinionated, safe workflow for feature development. It enforces **Functional Coherence** through automated test suite execution and integrates with GitHub for PR creation.

**Key Principle:** Safety-first, test-driven git operations with functional integrity verification.

---

## Quick Start

### Prerequisites

1. **Python 3.11+**
2. **Git** installed and configured
3. **GitHub CLI** (optional, for PR creation): `brew install gh && gh auth login`

### Start the MCP Server

**Local Development:**
```bash
cd /Users/richardfremmerlid/Projects/Project_Sanctuary
python3 -m mcp_servers.system.git_workflow.server
```

**Via Claude Desktop:**
Already configured in `claude_desktop_config.json`. Just restart Claude.

---

## Tools (9)

### 1. Workflow Tools

#### `git_start_feature(task_id, description)`
Create and checkout a new feature branch.

**Example:**
```python
git_start_feature("045", "smart-git-mcp")
# Creates: feature/task-045-smart-git-mcp
```

#### `git_smart_commit(message)` ⭐
**The "Smart" Part:** Automatically enforces Protocol 101 v3.0 (Functional Coherence).

**What it does:**
1. Commits staged files
2. Pre-commit hook automatically runs `./scripts/run_genome_tests.sh`
3. All tests must pass for commit to proceed
4. Ensures functional integrity before accepting commit

**Example:**
```python
git_smart_commit("Implement feature X")
# Result: Commit only if all tests pass
```

#### `git_push_feature()`
Push current feature branch to origin.

#### `git_create_pr(title, body, base)` ⭐
Create a GitHub Pull Request using GitHub CLI.

**Example:**
```python
git_create_pr(
    title="Add Smart Git MCP",
    body="Implements Protocol 101 compliance",
    base="main"
)
# Returns: PR URL
```

#### `git_finish_feature(branch_name)`
Cleanup after PR is merged (on GitHub).

**What it does:**
1. Checkout main
2. Pull latest
3. Delete local feature branch

#### `git_sync_main()`
Pull latest changes from origin/main.

---

### 2. Read-Only Tools

#### `git_get_status()`
Get repository status (branch, staged, modified, untracked files).

#### `git_diff(cached, file_path)`
Show changes in working directory or staged files.

**Examples:**
```python
git_diff(cached=False)  # Unstaged changes
git_diff(cached=True)   # Staged changes
git_diff(file_path="core/git/git_ops.py")  # Specific file
```

#### `git_log(max_count, oneline)`
Show commit history.

**Examples:**
```python
git_log(max_count=10, oneline=False)  # Last 10 commits (detailed)
git_log(max_count=5, oneline=True)    # Last 5 commits (compact)
```

---

## Complete Workflow

```
1. git_start_feature("046", "configure-mcp-client")
   → Creates feature/task-046-configure-mcp-client

2. (Make your changes)

3. git_diff(cached=False)
   → Review unstaged changes

4. (Stage files: git add ...)

5. git_diff(cached=True)
   → Review staged changes

6. git_smart_commit("Add MCP client configuration")
   → Commit (tests run automatically via pre-commit hook)

7. git_push_feature()
   → Push to GitHub

8. git_create_pr("Configure MCP Client", "Adds .agent/mcp_config.json")
   → Create PR on GitHub

9. (Review and merge PR on GitHub)

10. git_finish_feature("feature/task-046-configure-mcp-client")
    → Cleanup local branch
```

---

## Security Features

### 1. Base Directory Restriction
Set `GIT_BASE_DIR` environment variable to restrict all operations to a specific directory tree.

```bash
export GIT_BASE_DIR=/Users/richardfremmerlid/Projects/Project_Sanctuary
```

### 2. Path Sanitization
All file paths are validated and sanitized to prevent directory traversal attacks.

### 3. No Destructive Operations
The following dangerous operations are **NOT** exposed:
- `git reset --hard`
- `git rebase`
- `git push --force`
- Branch deletion (except via `finish_feature`)

---

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `REPO_PATH` | Repository root path | `.` (current directory) |
| `GIT_BASE_DIR` | Security sandbox (optional) | None |
| `PROJECT_ROOT` | Project root for PYTHONPATH | Required |

### Claude Desktop Config

```json
{
  "mcpServers": {
    "git_workflow": {
      "displayName": "Git Workflow MCP",
      "command": "/usr/local/bin/python3",
      "args": ["-m", "mcp_servers.system.git_workflow.server"],
      "env": {
        "PYTHONPATH": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
        "PROJECT_ROOT": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
        "GIT_BASE_DIR": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
      },
      "cwd": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
    }
  }
}
```

---

## Comparison to Other Git MCPs

See [git_mcp_comparison.md](file:///Users/richardfremmerlid/.gemini/antigravity/brain/8e7a3729-cc05-40ae-a5dd-38935c512229/git_mcp_comparison.md) for a detailed comparison with cyanheads/git-mcp-server.

**Our Unique Features:**
- ⭐ Automatic Protocol 101 v3.0 enforcement (Functional Coherence)
- ⭐ GitHub PR creation via `gh` CLI
- ⭐ Opinionated, safe workflow (prevents mistakes)

---

## Testing

```bash
# Run all tests
PYTHONPATH=. python3 tests/test_git_ops.py -v

# Test specific functionality
PYTHONPATH=. python3 tests/test_git_ops.py -v TestGitOperations.test_branch_operations
```

---

## Troubleshooting

### GitHub CLI Not Found
```
Error: GitHub CLI (gh) not found
```
**Solution:** Install GitHub CLI: `brew install gh && gh auth login`

### Base Directory Violation
```
Error: Repository path is outside base directory
```
**Solution:** Ensure `REPO_PATH` is within `GIT_BASE_DIR`.

### Test Suite Failed
```
Error: Commit rejected - tests failed
```
**Solution:** Fix failing tests before committing. Run `./scripts/run_genome_tests.sh` to see failures.

---

## Related Documentation

- [Protocol 101 v3.0: The Doctrine of Absolute Stability](../../../01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md)
- [Protocol 102 v2.0: The Doctrine of Mnemonic Synchronization](../../../01_PROTOCOLS/102_The_Doctrine_of_Mnemonic_Synchronization.md)
- [ADR 037: MCP Git Strategy - Immediate Compliance](../../../ADRs/037_mcp_git_migration_strategy.md)
- [ADR 019: Cognitive Genome Publishing Architecture](../../../ADRs/019_protocol_101_unbreakable_commit.md)

---

**Last Updated:** 2025-11-29  
**Maintainer:** Project Sanctuary Team

--- END OF FILE system/git_workflow/README.md ---

--- START OF FILE system/git_workflow/__init__.py ---



--- END OF FILE system/git_workflow/__init__.py ---

--- START OF FILE system/git_workflow/requirements.txt ---

fastmcp

--- END OF FILE system/git_workflow/requirements.txt ---

--- START OF FILE system/git_workflow/server.py ---

from fastmcp import FastMCP
from mcp_servers.lib.git.git_ops import GitOperations
import os
from typing import List

# Initialize FastMCP with canonical domain name
mcp = FastMCP("project_sanctuary.system.git_workflow")

# Initialize GitOperations
REPO_PATH = os.environ.get("REPO_PATH", ".")
BASE_DIR = os.environ.get("GIT_BASE_DIR", None)
git_ops = GitOperations(REPO_PATH, base_dir=BASE_DIR)

@mcp.tool()
def git_smart_commit(message: str) -> str:
    """
    Commit staged files with automatic Protocol 101 manifest generation.
    
    WORKFLOW: Before calling this tool:
    1. Use git_get_status to see what files have changed
    2. Stage files using standard git commands (git add <files>)
    3. Then call this tool to commit with automatic P101 compliance
    
    Args:
        message: The commit message.
        
    Returns:
        The commit hash or error message.
    """
    try:
        # Pillar 4: Pre-Execution Verification (Smart Commit Variant)
        # We allow staged files (obviously), but we MUST reject unstaged changes or untracked files
        # to ensure the commit is atomic and the working tree is clean otherwise.
        status = git_ops.status()
        if status['modified'] or status['untracked']:
             return f"PROTOCOL VIOLATION: Working directory is not clean. Modified: {len(status['modified'])}, Untracked: {len(status['untracked'])}. Please stage or stash changes."

        commit_hash = git_ops.commit(message)
        return f"Commit successful. Hash: {commit_hash}"
    except Exception as e:
        return f"Commit failed: {str(e)}"

def check_requirements() -> str:
    """
    Pillar 6: Pre-Flight Check.
    Verifies that all dependencies in REQUIREMENTS.env are installed.
    Returns None if successful, or error message if failed.
    """
    req_file = os.path.join(REPO_PATH, "REQUIREMENTS.env")
    if not os.path.exists(req_file):
        return None # No requirements file, skip check
        
    try:
        with open(req_file, 'r') as f:
            requirements = [line.strip() for line in f if line.strip() and not line.startswith('#')]
            
        for req in requirements:
            tool_name = req.split('>')[0].split('=')[0].split('<')[0].strip()
            # Basic check: try to run the tool with --version
            try:
                subprocess.run([tool_name, "--version"], check=True, capture_output=True)
            except (subprocess.CalledProcessError, FileNotFoundError):
                return f"PROTOCOL VIOLATION: Missing required dependency: {tool_name}. Please install it as per REQUIREMENTS.env."
    except Exception as e:
        return f"Failed to verify requirements: {str(e)}"
    return None

@mcp.tool()
def git_get_status() -> str:
    """
    Get the current status of the repository.
    
    Returns:
        A formatted string describing the repo status (branch, staged files, etc).
    """
    try:
        status = git_ops.status()
        return (
            f"Branch: {status['branch']}\n"
            f"Staged Files: {', '.join(status['staged'])}\n"
            f"Modified Files: {', '.join(status['modified'])}\n"
            f"Untracked Files: {', '.join(status['untracked'])}"
        )
    except Exception as e:
        return f"Failed to get status: {str(e)}"

@mcp.tool()
def git_add(files: List[str] = None) -> str:
    """
    Stage files for commit.
    
    Args:
        files: List of file paths to stage. If None or empty, stages all changes (git add -A).
        
    Returns:
        Success message.
        
    Example:
        git_add(["core/git/git_ops.py", "tests/test_git_ops.py"])
        git_add()  # Stage all changes
    """
    try:
        git_ops.add(files)
        if files:
            return f"Staged {len(files)} file(s): {', '.join(files)}"
        else:
            return "Staged all changes (git add -A)"
    except Exception as e:
        return f"Failed to stage files: {str(e)}"

@mcp.tool()
def git_push_feature(force: bool = False, no_verify: bool = False) -> str:
    """
    Push the current feature branch to origin.
    
    Args:
        force: Force push (git push --force). Use with caution.
        no_verify: Bypass pre-push hooks (git push --no-verify). Useful if git-lfs is missing.
    
    Returns:
        Push status.
    """
    try:
        current = git_ops.get_current_branch()
        if current == "main":
            return "Error: Cannot push main directly via this tool."
            
        output = git_ops.push("origin", current, force=force, no_verify=no_verify)
        pr_url = f"https://github.com/richfrem/Project_Sanctuary/pull/new/{current}"
        return f"Pushed {current} to origin: {output}\n\n📝 Next: Create PR at {pr_url}"
    except Exception as e:
        return f"Failed to push feature: {str(e)}"

@mcp.tool()
def git_start_feature(task_id: str, description: str) -> str:
    """
    Start a new feature branch.
    Format: feature/task-{task_id}-{description}
    
    Args:
        task_id: The task ID (e.g., "045").
        description: Short description (e.g., "smart-git-mcp").
        
    Returns:
        Success message with branch name.
    """
    try:
        # Pillar 6: Pre-Flight Check
        req_error = check_requirements()
        if req_error:
            return req_error

        # Pillar 4: Verify clean state before starting a new feature
        git_ops.verify_clean_state()

        # Sanitize description
        safe_desc = description.lower().replace(" ", "-")
        branch_name = f"feature/task-{task_id}-{safe_desc}"
        
        git_ops.create_branch(branch_name)
        git_ops.checkout(branch_name)
        
        return f"Started feature: {branch_name}"
    except Exception as e:
        return f"Failed to start feature: {str(e)}"

@mcp.tool()
def git_finish_feature(branch_name: str) -> str:
    """
    Finish a feature branch (cleanup).
    Assumes the PR has been merged on GitHub.
    1. Checkout main
    2. Pull latest main
    3. Delete local feature branch
    4. Delete remote feature branch
    
    Args:
        branch_name: The branch to finish.
        
    Returns:
        Cleanup status.
    """
    try:
        # Pillar 4: Verify clean state before finishing (merging/deleting)
        git_ops.verify_clean_state()

        # ALWAYS checkout main first to avoid merging main into the feature branch
        git_ops.checkout("main")
            
        git_ops.pull("origin", "main")
        
        # Delete local branch (force delete since we just pulled main and it might look unmerged if we didn't rebase)
        # But usually if it's merged in main, -d is fine. However, to be safe and ensure cleanup:
        git_ops.delete_local_branch(branch_name, force=True)
        
        # Delete remote branch
        try:
            git_ops.delete_remote_branch(branch_name)
        except Exception:
            # Remote branch might already be deleted, that's okay
            pass
        
        return f"Finished feature {branch_name}. Deleted local and remote branches, pulled latest main."
    except Exception as e:
        return f"Failed to finish feature: {str(e)}"

@mcp.tool()
def git_sync_main() -> str:
    """
    Sync the main branch with remote.
    
    Returns:
        Sync status.
    """
    try:
        # Pillar 6: Pre-Flight Check
        req_error = check_requirements()
        if req_error:
            return req_error

        current = git_ops.get_current_branch()
        if current != "main":
            return "Error: Must be on main branch to sync."
            
        output = git_ops.pull("origin", "main")
        return f"Synced main: {output}"
    except Exception as e:
        return f"Failed to sync main: {str(e)}"

@mcp.tool()
def git_diff(cached: bool = False, file_path: str = None) -> str:
    """
    Show changes in the working directory or staged files.
    
    Args:
        cached: If True, show staged changes. If False, show unstaged changes.
        file_path: Optional specific file to diff.
        
    Returns:
        Diff output.
    """
    try:
        diff_output = git_ops.diff(cached=cached, file_path=file_path)
        if not diff_output:
            return "No changes to display."
        return diff_output
    except Exception as e:
        return f"Failed to get diff: {str(e)}"

@mcp.tool()
def git_log(max_count: int = 10, oneline: bool = False) -> str:
    """
    Show commit history.
    
    Args:
        max_count: Maximum number of commits to show (default: 10).
        oneline: If True, show compact one-line format.
        
    Returns:
        Commit log.
    """
    try:
        return git_ops.log(max_count=max_count, oneline=oneline)
    except Exception as e:
        return f"Failed to get log: {str(e)}"


if __name__ == "__main__":
    mcp.run()

--- END OF FILE system/git_workflow/server.py ---

--- START OF FILE task/README.md ---

# Task MCP Server

**Domain:** `project_sanctuary.document.task`  
**Version:** 0.1.0  
**Status:** In Development

---

## Overview

The Task MCP server manages task files in the `TASKS/` directory structure. It provides tools for creating, updating, moving, reading, listing, and searching tasks following the canonical task schema.

**Key Principle:** File operations only - No Git commits (follows separation of concerns)

---

## Quick Start

### Start the MCP Server

**Option 1: Run Locally (Development - No Container)**
```bash
cd mcp_servers/task
source ../../.venv/bin/activate
python server.py
```
*Use this for development, debugging, and quick testing. No Podman required.*

**Option 2: Run in Podman Container (Production)**
```bash
# From project root
podman run -d \
  --name task-mcp \
  -v /Users/richardfremmerlid/Projects/Project_Sanctuary/TASKS:/app/TASKS:rw \
  -v /Users/richardfremmerlid/Projects/Project_Sanctuary/tools:/app/tools:ro \
  -p 8080:8080 \
  task-mcp:latest
```
*Use this for production deployment. Runs in isolated Podman container. View in **Podman Desktop** app.*

**Verify Running:**
```bash
# Check status (should show Up or Exited(0))
podman ps -a | grep task-mcp

# View logs
podman logs task-mcp
```

**Note:** Both options run the same MCP server code. The container version provides isolation and reproducibility.

### Run Tests

```bash
# Activate virtual environment
source .venv/bin/activate

# Run all unit tests
python -m pytest tests/mcp_servers/task/test_operations.py -v

# Run end-to-end workflow test
python tests/mcp_servers/task/test_e2e_workflow.py

# Run specific test
python -m pytest tests/mcp_servers/task/test_operations.py::TestCreateTask::test_create_task_success -v
```

### Create a Task

```python
from mcp_servers.task.operations import TaskOperations
from mcp_servers.task.models import TaskPriority, TaskStatus
from pathlib import Path

# Initialize
project_root = Path.cwd()
task_ops = TaskOperations(project_root)

# Create task
result = task_ops.create_task(
    title="Implement New Feature",
    objective="Add feature X to improve user experience",
    deliverables=[
        "Feature implementation",
        "Unit tests",
        "Documentation"
    ],
    acceptance_criteria=[
        "Feature works as expected",
        "Tests pass",
        "Docs updated"
    ],
    priority=TaskPriority.HIGH
)

print(f"Created: {result.file_path}")
```

### Update a Task

```python
# Update task priority
result = task_ops.update_task(
    task_number=37,
    updates={"priority": TaskPriority.CRITICAL}
)

# Move task to in-progress
result = task_ops.update_task_status(
    task_number=37,
    new_status=TaskStatus.IN_PROGRESS,
    notes="Starting work on this task"
)

print(f"Updated: {result.file_path}")
```

### Search and List Tasks

```python
# Search for tasks
results = task_ops.search_tasks("authentication")
for task in results:
    print(f"#{task['number']}: {task['title']}")

# List in-progress tasks
tasks = task_ops.list_tasks(status=TaskStatus.IN_PROGRESS)
print(f"Found {len(tasks)} in-progress tasks")
```

---

## Architecture

```mermaid
graph TB
    subgraph "LLM Assistant"
        LLM[Gemini/Claude/GPT]
    end
    
    subgraph "Task MCP Server"
        Server[MCP Server<br/>server.py]
        Ops[File Operations<br/>operations.py]
        Val[Validator<br/>validator.py]
        Models[Data Models<br/>models.py]
    end
    
    subgraph "File System"
        Backlog[TASKS/backlog/]
        Todo[TASKS/todo/]
        InProgress[TASKS/in-progress/]
        Done[TASKS/done/]
    end
    
    subgraph "Shared Tools"
        GetNext[get_next_task_number.py]
        Schema[task_schema.md]
    end
    
    LLM -->|MCP Protocol| Server
    Server --> Ops
    Ops --> Val
    Ops --> Models
    Ops --> GetNext
    Val --> Schema
    
    Ops -->|create/update| Backlog
    Ops -->|create/update| Todo
    Ops -->|create/update| InProgress
    Ops -->|create/update| Done
    
    style Server fill:#667eea
    style Ops fill:#764ba2
    style Val fill:#f093fb
    style Models fill:#4facfe
```

**Related Diagrams:**
- [Task MCP Class Diagram](../../docs/mcp/diagrams/task_mcp_class.mmd)
- [MCP Ecosystem Overview](../../docs/mcp/diagrams/mcp_ecosystem_class.mmd)

---

## Operations

### 1. create_task

**Purpose:** Create a new task file

**Preconditions:**
- Task number must be unique (auto-generated if not provided)
- Title, objective, deliverables, and acceptance criteria are required
- Dependencies must reference existing tasks (if specified)

**Input:**
```python
create_task(
    title: str,                          # Required
    objective: str,                      # Required
    deliverables: List[str],             # Required (min 1)
    acceptance_criteria: List[str],      # Required (min 1)
    priority: TaskPriority = MEDIUM,     # Optional
    status: TaskStatus = BACKLOG,        # Optional
    lead: str = "Unassigned",            # Optional
    dependencies: str = None,            # Optional
    related_documents: str = None,       # Optional
    notes: str = None,                   # Optional
    task_number: int = None              # Optional (auto-generated)
)
```

**Output:**
```python
FileOperationResult {
    file_path: "TASKS/backlog/037_implement_feature.md",
    content: "# TASK: Implement Feature\n...",
    operation: "created",
    task_number: 37,
    status: "success",
    message: "Task #037 created successfully"
}
```

**Example:**
```python
result = task_mcp.create_task(
    title="Implement User Authentication",
    objective="Add secure user authentication to the application",
    deliverables=[
        "Login page with email/password",
        "JWT token generation",
        "Protected routes middleware"
    ],
    acceptance_criteria=[
        "Users can log in with valid credentials",
        "Invalid credentials show error message",
        "Protected routes require authentication"
    ],
    priority=TaskPriority.HIGH,
    dependencies="Requires #036"
)
# Returns: { file_path: "TASKS/backlog/037_implement_user_authentication.md", ... }
```

---

### 2. update_task

**Purpose:** Update an existing task's metadata or content

**Preconditions:**
- Task must exist
- Updates must maintain schema validity

**Input:**
```python
update_task(
    task_number: int,        # Required
    updates: Dict[str, any]  # Required
)
```

**Output:**
```python
FileOperationResult {
    file_path: "TASKS/backlog/037_implement_feature.md",
    content: "# TASK: Implement Feature (Updated)\n...",
    operation: "updated",
    task_number: 37,
    status: "success",
    message: "Task #037 updated successfully"
}
```

**Example:**
```python
result = task_mcp.update_task(
    task_number=37,
    updates={
        "priority": TaskPriority.CRITICAL,
        "lead": "GUARDIAN-02",
        "deliverables": [
            "Login page with email/password",
            "JWT token generation",
            "Protected routes middleware",
            "Password reset functionality"  # Added
        ]
    }
)
```

---

### 3. update_task_status

**Purpose:** Change task status (moves file between directories)

**Preconditions:**
- Task must exist
- New status must be valid

**Input:**
```python
update_task_status(
    task_number: int,          # Required
    new_status: TaskStatus,    # Required
    notes: str = None          # Optional
)
```

**Output:**
```python
FileOperationResult {
    file_path: "TASKS/in-progress/037_implement_feature.md",
    content: "# TASK: Implement Feature\n...",
    operation: "moved",
    task_number: 37,
    status: "success",
    message: "Task #037 moved to in-progress"
}
```

**Status Transitions:**
```
backlog → todo → in-progress → done
   ↓        ↓         ↓
blocked (stays in in-progress dir)
```

**Example:**
```python
result = task_mcp.update_task_status(
    task_number=37,
    new_status=TaskStatus.IN_PROGRESS,
    notes="Starting implementation. Assigned to GUARDIAN-02."
)
# File moves from TASKS/backlog/ to TASKS/in-progress/
```

---

### 4. get_task

**Purpose:** Retrieve a specific task by number

**Preconditions:**
- Task must exist

**Input:**
```python
get_task(task_number: int)  # Required
```

**Output:**
```python
{
    "number": 37,
    "title": "Implement User Authentication",
    "status": "in-progress",
    "priority": "Critical",
    "lead": "GUARDIAN-02",
    "file_path": "TASKS/in-progress/037_implement_user_authentication.md",
    "content": "# TASK: Implement User Authentication\n..."
}
```

**Example:**
```python
task = task_mcp.get_task(37)
print(task["title"])  # "Implement User Authentication"
```

---

### 5. list_tasks

**Purpose:** List tasks with optional filters

**Preconditions:**
- None (returns empty list if no tasks match)

**Input:**
```python
list_tasks(
    status: TaskStatus = None,      # Optional filter
    priority: TaskPriority = None   # Optional filter
)
```

**Output:**
```python
[
    {
        "number": 37,
        "title": "Implement User Authentication",
        "status": "in-progress",
        "priority": "Critical",
        "lead": "GUARDIAN-02",
        "file_path": "TASKS/in-progress/037_implement_user_authentication.md"
    },
    {
        "number": 38,
        "title": "Add Unit Tests",
        "status": "in-progress",
        "priority": "High",
        "lead": "Unassigned",
        "file_path": "TASKS/in-progress/038_add_unit_tests.md"
    }
]
```

**Example:**
```python
# Get all tasks in progress
in_progress = task_mcp.list_tasks(status=TaskStatus.IN_PROGRESS)

# Get all critical priority tasks
critical = task_mcp.list_tasks(priority=TaskPriority.CRITICAL)

# Get all tasks (no filters)
all_tasks = task_mcp.list_tasks()
```

---

### 6. search_tasks

**Purpose:** Search tasks by content (full-text search)

**Preconditions:**
- None (returns empty list if no matches)

**Input:**
```python
search_tasks(query: str)  # Required
```

**Output:**
```python
[
    {
        "number": 37,
        "title": "Implement User Authentication",
        "status": "in-progress",
        "priority": "Critical",
        "file_path": "TASKS/in-progress/037_implement_user_authentication.md",
        "matches": [
            "Add secure user authentication to the application",
            "JWT token generation",
            "Protected routes require authentication"
        ]
    }
]
```

**Example:**
```python
# Search for tasks mentioning "authentication"
results = task_mcp.search_tasks("authentication")

# Search for tasks mentioning "GUARDIAN-02"
guardian_tasks = task_mcp.search_tasks("GUARDIAN-02")
```

---

## Connecting to Claude Desktop

To use the Task MCP server with Claude Desktop, you need to register it in Claude's MCP configuration.

### Step 1: Locate Claude Desktop Config

The config file is at:
```bash
cd ~/Library/Application\ Support/Claude/
nano claude_desktop_config.json
```

### Step 2: Add Task MCP Server

Edit the config file and add the Task MCP server:

```json
{
  "mcpServers": {
    "task-mcp": {
      "command": "/Users/richardfremmerlid/Projects/Project_Sanctuary/.venv/bin/python",
      "args": [
        "-m",
        "mcp_servers.task.server"
      ],
      "env": {
        "PYTHONPATH": "/Users/richardfremmerlid/Projects/Project_Sanctuary",
        "PROJECT_ROOT": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
      },
      "cwd": "/Users/richardfremmerlid/Projects/Project_Sanctuary"
    }
  }
}
```

**Note:** We use absolute paths to the virtual environment python executable to avoid `ENOENT` errors.

### Step 3: Restart Claude Desktop

1. Quit Claude Desktop completely
2. Reopen Claude Desktop
3. The Task MCP server will be available

### Step 4: Verify Connection

In Claude Desktop, you should see the Task MCP server listed in the MCP servers panel. You can now use natural language:

```
"Create a high priority task to implement user authentication"
"List all tasks in progress"
"Move task #037 to done"
"Search for tasks about authentication"
```

### Troubleshooting

**Server not showing up:**
- Check the config file syntax (valid JSON)
- Ensure Python path is correct
- Check Claude Desktop logs: `~/Library/Logs/Claude/`

**Permission errors:**
- Ensure the virtual environment is activated
- Check file permissions on TASKS directory

---

## Safety Rules

1. **Task Number Uniqueness** - Cannot create duplicate task numbers
2. **Circular Dependencies** - Validates dependency references exist
3. **Schema Compliance** - All tasks must follow `TASKS/task_schema.md`
4. **File Path Safety** - All operations restricted to `TASKS/` directory
5. **No Deletion** - Tasks cannot be deleted (archive by moving to done)
6. **No Git Operations** - File operations only (separation of concerns)

---

## Running in Podman

### Build the Image

```bash
cd mcp_servers/task
podman build -t task-mcp:latest .
```

### Run the Container

```bash
podman run -d \
  --name task-mcp \
  -v /Users/richardfremmerlid/Projects/Project_Sanctuary/TASKS:/app/TASKS:rw \
  -v /Users/richardfremmerlid/Projects/Project_Sanctuary/tools:/app/tools:ro \
  -p 8080:8080 \
  task-mcp:latest
```

**Volume Mounts:**
- `TASKS/` - Read/write access for task file operations
- `tools/` - Read-only access for `get_next_task_number.py`

**Port Mapping:**
- `8080:8080` - MCP protocol communication

### Run in Podman Desktop

1. Open **Podman Desktop**
2. Go to **Images** tab
3. Find `task-mcp:latest`
4. Click **▶️ play button**
5. Configure:
   - **Port:** `8080:8080`
   - **Volume 1:** `/path/to/TASKS:/app/TASKS:rw`
   - **Volume 2:** `/path/to/tools:/app/tools:ro`
   - **Name:** `task-mcp`
6. Click **Start Container**

### Verify Running

```bash
# Check container status
podman ps | grep task-mcp

# View logs
podman logs task-mcp

# Test health endpoint
curl http://localhost:8080/health
```

---

## Development

### Project Structure

```
mcp_servers/task/
├── __init__.py          # Package initialization
├── models.py            # Data models (TaskSchema, FileOperationResult)
├── validator.py         # Schema and safety validation
├── operations.py        # File operations (create, update, move, etc.)
├── server.py            # MCP server implementation
├── Dockerfile           # Container definition
└── README.md            # This file
```

### Local Testing (Without Container)

```bash
# Activate virtual environment
source .venv/bin/activate

# Install dependencies
pip install mcp

# Run tests
python -m pytest tests/mcp_servers/task/

# Run server locally
cd mcp_servers/task
python server.py
```

---

## Dependencies

- **Python 3.11+**
- **MCP SDK:** `pip install mcp`
- **Project Tools:** `get_next_task_number.py`
- **Task Schema:** `TASKS/task_schema.md`

---

## Related Documentation

- [MCP Architecture](../../docs/mcp/architecture.md)
- [MCP Setup Guide](../../docs/mcp/setup_guide.md)
- [Task Schema](../../TASKS/task_schema.md)
- [ADR 034: Containerize MCP Servers](../../ADRs/034_containerize_mcp_servers_with_podman.md)
- [Prerequisites Guide](../../docs/mcp/prerequisites.md)
- [Task #031: Implement Task MCP](../../TASKS/backlog/031_implement_task_mcp.md)

---

## Troubleshooting

### Task Number Already Exists

**Error:** `Task #037 already exists in backlog/`

**Solution:** Use auto-generated task number (don't specify `task_number`)

### Validation Failed

**Error:** `Schema validation failed: At least one deliverable is required`

**Solution:** Ensure all required fields are provided:
- title
- objective
- deliverables (min 1)
- acceptance_criteria (min 1)

### File Not Found

**Error:** `Task #037 not found`

**Solution:** Verify task exists:
```python
task = task_mcp.get_task(37)
if task is None:
    print("Task does not exist")
```

---

**Last Updated:** 2025-11-26  
**Maintainer:** Project Sanctuary Team

--- END OF FILE task/README.md ---

--- START OF FILE task/__init__.py ---

"""
Task MCP Server
Domain: project_sanctuary.document.task
Purpose: Manage task files in TASKS/ directories (file operations only)
"""

__version__ = "0.1.0"
__domain__ = "project_sanctuary.document.task"

--- END OF FILE task/__init__.py ---

--- START OF FILE task/models.py ---

"""
Task MCP Server - Data Models
Defines schemas for tasks and operation results
"""

from dataclasses import dataclass
from enum import Enum
from typing import Optional, List
from datetime import datetime


class TaskStatus(str, Enum):
    """Task status following task_schema.md"""
    BACKLOG = "backlog"
    TODO = "todo"
    IN_PROGRESS = "in-progress"
    COMPLETE = "complete"
    BLOCKED = "blocked"


class TaskPriority(str, Enum):
    """Task priority levels"""
    CRITICAL = "Critical"
    HIGH = "High"
    MEDIUM = "Medium"
    LOW = "Low"


@dataclass
class TaskSchema:
    """
    Task schema following TASKS/task_schema.md
    """
    number: int
    title: str
    status: TaskStatus
    priority: TaskPriority
    lead: str
    dependencies: Optional[str] = None
    related_documents: Optional[str] = None
    objective: str = ""
    deliverables: List[str] = None
    acceptance_criteria: List[str] = None
    notes: Optional[str] = None
    
    def __post_init__(self):
        if self.deliverables is None:
            self.deliverables = []
        if self.acceptance_criteria is None:
            self.acceptance_criteria = []


@dataclass
class FileOperationResult:
    """
    Result of a file operation (following separation of concerns)
    Returns file path for Git Workflow MCP to commit
    """
    file_path: str
    content: str
    operation: str  # "created", "updated", "moved"
    task_number: int
    status: str = "success"
    message: str = ""
    
    def to_dict(self):
        return {
            "file_path": self.file_path,
            "content": self.content,
            "operation": self.operation,
            "task_number": self.task_number,
            "status": self.status,
            "message": self.message
        }

--- END OF FILE task/models.py ---

--- START OF FILE task/operations.py ---

"""
Task MCP Server - File Operations
Handles all task file operations (create, update, move, read, list, search)
Following separation of concerns: File operations only, no Git commits
"""

import re
from pathlib import Path
from typing import List, Optional, Dict, Tuple
from datetime import datetime
import sys
import os

# Add tools directory to path for get_next_task_number
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent / "tools" / "scaffolds"))
from get_next_task_number import get_next_task_number

from .models import TaskSchema, TaskStatus, TaskPriority, FileOperationResult
from .validator import TaskValidator


class TaskOperations:
    """Handles all task file operations"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.tasks_dir = project_root / "TASKS"
        self.validator = TaskValidator(project_root)
        
        # Status to directory mapping
        self.status_dirs = {
            TaskStatus.BACKLOG: self.tasks_dir / "backlog",
            TaskStatus.TODO: self.tasks_dir / "todo",
            TaskStatus.IN_PROGRESS: self.tasks_dir / "in-progress",
            TaskStatus.COMPLETE: self.tasks_dir / "done",
            TaskStatus.BLOCKED: self.tasks_dir / "in-progress"  # Blocked tasks stay in in-progress
        }
    
    def create_task(
        self,
        title: str,
        objective: str,
        deliverables: List[str],
        acceptance_criteria: List[str],
        priority: TaskPriority = TaskPriority.MEDIUM,
        status: TaskStatus = TaskStatus.BACKLOG,
        lead: str = "Unassigned",
        dependencies: Optional[str] = None,
        related_documents: Optional[str] = None,
        notes: Optional[str] = None,
        task_number: Optional[int] = None
    ) -> FileOperationResult:
        """
        Create a new task file
        
        Args:
            title: Task title
            objective: What and why
            deliverables: List of concrete outputs
            acceptance_criteria: List of completion conditions
            priority: Task priority (default: MEDIUM)
            status: Initial status (default: BACKLOG)
            lead: Assigned person/agent (default: Unassigned)
            dependencies: Task dependencies (e.g., "Requires #012")
            related_documents: Related files/protocols
            notes: Additional context
            task_number: Specific task number (auto-generated if None)
        
        Returns:
            FileOperationResult with file path and content
        """
        # Get next task number if not provided
        if task_number is None:
            task_number = int(get_next_task_number())
        
        # Validate task number is unique
        is_valid, error_msg = self.validator.validate_task_number(task_number)
        if not is_valid:
            return FileOperationResult(
                file_path="",
                content="",
                operation="create",
                task_number=task_number,
                status="error",
                message=error_msg
            )
        
        # Create task schema
        task = TaskSchema(
            number=task_number,
            title=title,
            status=status,
            priority=priority,
            lead=lead,
            dependencies=dependencies or "None",
            related_documents=related_documents or "None",
            objective=objective,
            deliverables=deliverables,
            acceptance_criteria=acceptance_criteria,
            notes=notes
        )
        
        # Validate schema
        is_valid, errors = self.validator.validate_task_schema(task)
        if not is_valid:
            return FileOperationResult(
                file_path="",
                content="",
                operation="create",
                task_number=task_number,
                status="error",
                message=f"Schema validation failed: {', '.join(errors)}"
            )
        
        # Validate dependencies
        if dependencies and dependencies.lower() != "none":
            is_valid, error_msg = self.validator.validate_dependencies(dependencies)
            if not is_valid:
                return FileOperationResult(
                    file_path="",
                    content="",
                    operation="create",
                    task_number=task_number,
                    status="error",
                    message=error_msg
                )
        
        # Generate file content
        content = self._generate_task_markdown(task)
        
        # Determine file path
        target_dir = self.status_dirs[status]
        target_dir.mkdir(parents=True, exist_ok=True)
        
        filename = f"{task_number:03d}_{self._title_to_filename(title)}.md"
        file_path = target_dir / filename
        
        # Write file
        file_path.write_text(content, encoding='utf-8')
        
        return FileOperationResult(
            file_path=str(file_path.relative_to(self.project_root)),
            content=content,
            operation="created",
            task_number=task_number,
            status="success",
            message=f"Task #{task_number:03d} created successfully"
        )
    
    def update_task(
        self,
        task_number: int,
        updates: Dict[str, any]
    ) -> FileOperationResult:
        """
        Update an existing task
        
        Args:
            task_number: Task number to update
            updates: Dictionary of fields to update
        
        Returns:
            FileOperationResult with updated file path and content
        """
        # Find existing task
        exists, current_dir = self.validator.task_exists(task_number)
        if not exists:
            return FileOperationResult(
                file_path="",
                content="",
                operation="update",
                task_number=task_number,
                status="error",
                message=f"Task #{task_number:03d} not found"
            )
        
        # Read current task
        current_path = self._find_task_file(task_number, Path(current_dir))
        if not current_path:
            return FileOperationResult(
                file_path="",
                content="",
                operation="update",
                task_number=task_number,
                status="error",
                message=f"Task file not found for #{task_number:03d}"
            )
        
        current_content = current_path.read_text(encoding='utf-8')
        
        # Parse current task
        task = self._parse_task_markdown(current_content, task_number)
        
        # Apply updates
        for key, value in updates.items():
            if hasattr(task, key):
                setattr(task, key, value)
        
        # Validate updated task
        is_valid, errors = self.validator.validate_task_schema(task)
        if not is_valid:
            return FileOperationResult(
                file_path="",
                content="",
                operation="update",
                task_number=task_number,
                status="error",
                message=f"Validation failed: {', '.join(errors)}"
            )
        
        # Generate updated content
        new_content = self._generate_task_markdown(task)
        
        # Write updated file
        current_path.write_text(new_content, encoding='utf-8')
        
        return FileOperationResult(
            file_path=str(current_path.relative_to(self.project_root)),
            content=new_content,
            operation="updated",
            task_number=task_number,
            status="success",
            message=f"Task #{task_number:03d} updated successfully"
        )
    
    def update_task_status(
        self,
        task_number: int,
        new_status: TaskStatus,
        notes: Optional[str] = None
    ) -> FileOperationResult:
        """
        Update task status (moves file between directories)
        
        Args:
            task_number: Task number
            new_status: New status
            notes: Optional notes about status change
        
        Returns:
            FileOperationResult with new file path
        """
        # Find current task
        exists, current_dir = self.validator.task_exists(task_number)
        if not exists:
            return FileOperationResult(
                file_path="",
                content="",
                operation="move",
                task_number=task_number,
                status="error",
                message=f"Task #{task_number:03d} not found"
            )
        
        current_path = self._find_task_file(task_number, Path(current_dir))
        if not current_path:
            return FileOperationResult(
                file_path="",
                content="",
                operation="move",
                task_number=task_number,
                status="error",
                message=f"Task file not found"
            )
        
        # Read and parse task
        content = current_path.read_text(encoding='utf-8')
        task = self._parse_task_markdown(content, task_number)
        
        # Update status
        old_status = task.status
        task.status = new_status
        
        # Add notes if provided
        if notes:
            if task.notes:
                task.notes += f"\n\n**Status Change ({datetime.now().strftime('%Y-%m-%d')}):** {old_status.value} → {new_status.value}\n{notes}"
            else:
                task.notes = f"**Status Change ({datetime.now().strftime('%Y-%m-%d')}):** {old_status.value} → {new_status.value}\n{notes}"
        
        # Generate updated content
        new_content = self._generate_task_markdown(task)
        
        # Determine new directory
        new_dir = self.status_dirs[new_status]
        new_dir.mkdir(parents=True, exist_ok=True)
        
        new_path = new_dir / current_path.name
        
        # Move file
        current_path.rename(new_path)
        
        # Write updated content
        new_path.write_text(new_content, encoding='utf-8')
        
        return FileOperationResult(
            file_path=str(new_path.relative_to(self.project_root)),
            content=new_content,
            operation="moved",
            task_number=task_number,
            status="success",
            message=f"Task #{task_number:03d} moved to {new_status.value}"
        )
    
    def get_task(self, task_number: int) -> Optional[Dict]:
        """Get task by number"""
        exists, task_dir = self.validator.task_exists(task_number)
        if not exists:
            return None
        
        task_path = self._find_task_file(task_number, Path(task_dir))
        if not task_path:
            return None
        
        content = task_path.read_text(encoding='utf-8')
        task = self._parse_task_markdown(content, task_number)
        
        return {
            "number": task.number,
            "title": task.title,
            "status": task.status.value,
            "priority": task.priority.value,
            "lead": task.lead,
            "file_path": str(task_path.relative_to(self.project_root)),
            "content": content
        }
    
    def list_tasks(
        self,
        status: Optional[TaskStatus] = None,
        priority: Optional[TaskPriority] = None
    ) -> List[Dict]:
        """List tasks with optional filters"""
        tasks = []
        
        # Determine which directories to search
        if status:
            dirs_to_search = [self.status_dirs[status]]
        else:
            dirs_to_search = list(self.status_dirs.values())
        
        # Search directories
        for task_dir in dirs_to_search:
            if not task_dir.exists():
                continue
            
            for file_path in task_dir.glob("*.md"):
                # Extract task number from filename
                match = re.match(r"^(\d{3})_", file_path.name)
                if not match:
                    continue
                
                task_num = int(match.group(1))
                content = file_path.read_text(encoding='utf-8')
                task = self._parse_task_markdown(content, task_num)
                
                # Apply priority filter
                if priority and task.priority != priority:
                    continue
                
                tasks.append({
                    "number": task.number,
                    "title": task.title,
                    "status": task.status.value,
                    "priority": task.priority.value,
                    "lead": task.lead,
                    "file_path": str(file_path.relative_to(self.project_root))
                })
        
        # Sort by task number
        tasks.sort(key=lambda x: x["number"])
        return tasks
    
    def search_tasks(self, query: str) -> List[Dict]:
        """Search tasks by content"""
        results = []
        query_lower = query.lower()
        
        for task_dir in self.status_dirs.values():
            if not task_dir.exists():
                continue
            
            for file_path in task_dir.glob("*.md"):
                content = file_path.read_text(encoding='utf-8')
                
                if query_lower in content.lower():
                    match = re.match(r"^(\d{3})_", file_path.name)
                    if match:
                        task_num = int(match.group(1))
                        task = self._parse_task_markdown(content, task_num)
                        
                        results.append({
                            "number": task.number,
                            "title": task.title,
                            "status": task.status.value,
                            "priority": task.priority.value,
                            "file_path": str(file_path.relative_to(self.project_root)),
                            "matches": self._find_matches(content, query)
                        })
        
        return results
    
    # Helper methods
    
    def _generate_task_markdown(self, task: TaskSchema) -> str:
        """Generate markdown content from task schema"""
        # Handle both enum and string values for status/priority
        status_value = task.status.value if isinstance(task.status, TaskStatus) else task.status
        priority_value = task.priority.value if isinstance(task.priority, TaskPriority) else task.priority
        
        lines = [
            f"# TASK: {task.title}",
            "",
            f"**Status:** {status_value}",
            f"**Priority:** {priority_value}",
            f"**Lead:** {task.lead}",
            f"**Dependencies:** {task.dependencies}",
            f"**Related Documents:** {task.related_documents}",
            "",
            "---",
            "",
            "## 1. Objective",
            "",
            task.objective,
            "",
            "## 2. Deliverables",
            ""
        ]
        
        for i, deliverable in enumerate(task.deliverables, 1):
            lines.append(f"{i}. {deliverable}")
        
        lines.extend([
            "",
            "## 3. Acceptance Criteria",
            ""
        ])
        
        for criterion in task.acceptance_criteria:
            lines.append(f"- {criterion}")
        
        if task.notes:
            lines.extend([
                "",
                "## Notes",
                "",
                task.notes
            ])
        
        lines.append("")  # Final newline
        return "\n".join(lines)
    
    def _parse_task_markdown(self, content: str, task_number: int) -> TaskSchema:
        """Parse markdown content into task schema"""
        lines = content.split("\n")
        
        # Extract metadata
        title = ""
        status = TaskStatus.BACKLOG
        priority = TaskPriority.MEDIUM
        lead = "Unassigned"
        dependencies = "None"
        related_documents = "None"
        objective = ""
        deliverables = []
        acceptance_criteria = []
        notes = ""
        
        current_section = None
        
        for line in lines:
            # Title
            if line.startswith("# TASK:"):
                title = line.replace("# TASK:", "").strip()
            
            # Metadata
            elif line.startswith("**Status:**"):
                status_str = line.split("**Status:**")[1].strip()
                # Case-insensitive lookup
                try:
                    status = TaskStatus(status_str.lower())
                except ValueError:
                    # Try to match by value (case-insensitive)
                    for s in TaskStatus:
                        if s.value.lower() == status_str.lower():
                            status = s
                            break
            elif line.startswith("**Priority:**"):
                priority_str = line.split("**Priority:**")[1].strip()
                # Case-insensitive lookup
                try:
                    priority = TaskPriority(priority_str)
                except ValueError:
                    # Try to match by value (case-insensitive)
                    for p in TaskPriority:
                        if p.value.lower() == priority_str.lower():
                            priority = p
                            break
            elif line.startswith("**Lead:**"):
                lead = line.split("**Lead:**")[1].strip()
            elif line.startswith("**Dependencies:**"):
                dependencies = line.split("**Dependencies:**")[1].strip()
            elif line.startswith("**Related Documents:**"):
                related_documents = line.split("**Related Documents:**")[1].strip()
            
            # Sections
            elif line.startswith("## 1. Objective"):
                current_section = "objective"
            elif line.startswith("## 2. Deliverables"):
                current_section = "deliverables"
            elif line.startswith("## 3. Acceptance Criteria"):
                current_section = "acceptance"
            elif line.startswith("## Notes"):
                current_section = "notes"
            elif line.startswith("##"):
                current_section = None
            
            # Content
            elif current_section == "objective" and line.strip() and not line.startswith("---"):
                objective += line + "\n"
            elif current_section == "deliverables" and line.strip().startswith(("1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.")):
                deliverables.append(line.split(".", 1)[1].strip())
            elif current_section == "acceptance" and line.strip().startswith("-"):
                acceptance_criteria.append(line.strip()[2:])
            elif current_section == "notes" and line.strip():
                notes += line + "\n"
        
        return TaskSchema(
            number=task_number,
            title=title,
            status=status,
            priority=priority,
            lead=lead,
            dependencies=dependencies,
            related_documents=related_documents,
            objective=objective.strip(),
            deliverables=deliverables,
            acceptance_criteria=acceptance_criteria,
            notes=notes.strip() if notes else None
        )
    
    def _title_to_filename(self, title: str) -> str:
        """Convert title to filename-safe format"""
        # Lowercase
        filename = title.lower()
        # Replace spaces with underscores
        filename = filename.replace(" ", "_")
        # Remove special characters
        filename = re.sub(r'[^a-z0-9_]', '', filename)
        # Limit length
        return filename[:50]
    
    def _find_task_file(self, task_number: int, directory: Path) -> Optional[Path]:
        """Find task file by number in directory"""
        pattern = f"{task_number:03d}_*.md"
        files = list(directory.glob(pattern))
        return files[0] if files else None
    
    def _find_matches(self, content: str, query: str) -> List[str]:
        """Find matching lines in content"""
        matches = []
        query_lower = query.lower()
        
        for line in content.split("\n"):
            if query_lower in line.lower():
                matches.append(line.strip())
                if len(matches) >= 3:  # Limit to 3 matches
                    break
        
        return matches

--- END OF FILE task/operations.py ---

--- START OF FILE task/requirements.txt ---

mcp>=1.0.0

--- END OF FILE task/requirements.txt ---

--- START OF FILE task/server.py ---

"""
Task MCP Server - MCP Protocol Implementation
Exposes task operations via Model Context Protocol
"""

from mcp.server import Server
from mcp.types import Tool, TextContent
import mcp.server.stdio
from pathlib import Path
import json
import sys
from typing import Any

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))

from mcp_servers.task.operations import TaskOperations
from mcp_servers.task.models import TaskStatus, TaskPriority


# Initialize server
app = Server("task-mcp")

# Initialize operations
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
task_ops = TaskOperations(PROJECT_ROOT)


@app.list_tools()
async def list_tools() -> list[Tool]:
    """List available MCP tools"""
    return [
        Tool(
            name="create_task",
            description="Create a new task file in TASKS/ directory",
            inputSchema={
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "Task title"
                    },
                    "objective": {
                        "type": "string",
                        "description": "What and why of the task"
                    },
                    "deliverables": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of concrete outputs"
                    },
                    "acceptance_criteria": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "List of completion conditions"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["Critical", "High", "Medium", "Low"],
                        "description": "Task priority (default: Medium)"
                    },
                    "status": {
                        "type": "string",
                        "enum": ["backlog", "todo", "in-progress", "done", "blocked"],
                        "description": "Initial status (default: backlog)"
                    },
                    "lead": {
                        "type": "string",
                        "description": "Assigned person/agent (default: Unassigned)"
                    },
                    "dependencies": {
                        "type": "string",
                        "description": "Task dependencies (e.g., 'Requires #012')"
                    },
                    "related_documents": {
                        "type": "string",
                        "description": "Related files/protocols"
                    },
                    "notes": {
                        "type": "string",
                        "description": "Additional context"
                    },
                    "task_number": {
                        "type": "integer",
                        "description": "Specific task number (auto-generated if not provided)"
                    }
                },
                "required": ["title", "objective", "deliverables", "acceptance_criteria"]
            }
        ),
        Tool(
            name="update_task",
            description="Update an existing task's metadata or content",
            inputSchema={
                "type": "object",
                "properties": {
                    "task_number": {
                        "type": "integer",
                        "description": "Task number to update"
                    },
                    "updates": {
                        "type": "object",
                        "description": "Dictionary of fields to update"
                    }
                },
                "required": ["task_number", "updates"]
            }
        ),
        Tool(
            name="update_task_status",
            description="Change task status (moves file between directories)",
            inputSchema={
                "type": "object",
                "properties": {
                    "task_number": {
                        "type": "integer",
                        "description": "Task number"
                    },
                    "new_status": {
                        "type": "string",
                        "enum": ["backlog", "todo", "in-progress", "done", "blocked"],
                        "description": "New status"
                    },
                    "notes": {
                        "type": "string",
                        "description": "Optional notes about status change"
                    }
                },
                "required": ["task_number", "new_status"]
            }
        ),
        Tool(
            name="get_task",
            description="Retrieve a specific task by number",
            inputSchema={
                "type": "object",
                "properties": {
                    "task_number": {
                        "type": "integer",
                        "description": "Task number to retrieve"
                    }
                },
                "required": ["task_number"]
            }
        ),
        Tool(
            name="list_tasks",
            description="List tasks with optional filters",
            inputSchema={
                "type": "object",
                "properties": {
                    "status": {
                        "type": "string",
                        "enum": ["backlog", "todo", "in-progress", "done", "blocked"],
                        "description": "Filter by status"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["Critical", "High", "Medium", "Low"],
                        "description": "Filter by priority"
                    }
                }
            }
        ),
        Tool(
            name="search_tasks",
            description="Search tasks by content (full-text search)",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "Search query"
                    }
                },
                "required": ["query"]
            }
        )
    ]


@app.call_tool()
async def call_tool(name: str, arguments: Any) -> list[TextContent]:
    """Handle tool calls"""
    
    try:
        if name == "create_task":
            # Parse arguments
            title = arguments["title"]
            objective = arguments["objective"]
            deliverables = arguments["deliverables"]
            acceptance_criteria = arguments["acceptance_criteria"]
            
            priority = TaskPriority(arguments.get("priority", "Medium"))
            status = TaskStatus(arguments.get("status", "backlog"))
            lead = arguments.get("lead", "Unassigned")
            dependencies = arguments.get("dependencies")
            related_documents = arguments.get("related_documents")
            notes = arguments.get("notes")
            task_number = arguments.get("task_number")
            
            # Create task
            result = task_ops.create_task(
                title=title,
                objective=objective,
                deliverables=deliverables,
                acceptance_criteria=acceptance_criteria,
                priority=priority,
                status=status,
                lead=lead,
                dependencies=dependencies,
                related_documents=related_documents,
                notes=notes,
                task_number=task_number
            )
            
            return [TextContent(
                type="text",
                text=json.dumps(result.to_dict(), indent=2)
            )]
        
        elif name == "update_task":
            task_number = arguments["task_number"]
            updates = arguments["updates"]
            
            result = task_ops.update_task(task_number, updates)
            
            return [TextContent(
                type="text",
                text=json.dumps(result.to_dict(), indent=2)
            )]
        
        elif name == "update_task_status":
            task_number = arguments["task_number"]
            new_status = TaskStatus(arguments["new_status"])
            notes = arguments.get("notes")
            
            result = task_ops.update_task_status(task_number, new_status, notes)
            
            return [TextContent(
                type="text",
                text=json.dumps(result.to_dict(), indent=2)
            )]
        
        elif name == "get_task":
            task_number = arguments["task_number"]
            
            task = task_ops.get_task(task_number)
            
            if task is None:
                return [TextContent(
                    type="text",
                    text=json.dumps({
                        "status": "error",
                        "message": f"Task #{task_number:03d} not found"
                    }, indent=2)
                )]
            
            return [TextContent(
                type="text",
                text=json.dumps(task, indent=2)
            )]
        
        elif name == "list_tasks":
            status = TaskStatus(arguments["status"]) if "status" in arguments else None
            priority = TaskPriority(arguments["priority"]) if "priority" in arguments else None
            
            tasks = task_ops.list_tasks(status, priority)
            
            return [TextContent(
                type="text",
                text=json.dumps(tasks, indent=2)
            )]
        
        elif name == "search_tasks":
            query = arguments["query"]
            
            results = task_ops.search_tasks(query)
            
            return [TextContent(
                type="text",
                text=json.dumps(results, indent=2)
            )]
        
        else:
            return [TextContent(
                type="text",
                text=json.dumps({
                    "status": "error",
                    "message": f"Unknown tool: {name}"
                }, indent=2)
            )]
    
    except Exception as e:
        return [TextContent(
            type="text",
            text=json.dumps({
                "status": "error",
                "message": str(e)
            }, indent=2)
        )]


async def main():
    """Run the MCP server"""
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

--- END OF FILE task/server.py ---

--- START OF FILE task/validator.py ---

"""
Task MCP Server - Schema Validator
Validates tasks against TASKS/task_schema.md
"""

import re
from pathlib import Path
from typing import Dict, List, Tuple
from .models import TaskSchema, TaskStatus, TaskPriority


class TaskValidator:
    """Validates task files against canonical schema"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.tasks_dir = project_root / "TASKS"
        
    def validate_task_number(self, number: int) -> Tuple[bool, str]:
        """
        Validate task number is unique across all task directories
        Returns: (is_valid, error_message)
        """
        task_dirs = [
            self.tasks_dir / "backlog",
            self.tasks_dir / "todo", 
            self.tasks_dir / "in-progress",
            self.tasks_dir / "done"
        ]
        
        task_pattern = re.compile(rf"^{number:03d}_.*\.md$")
        
        for task_dir in task_dirs:
            if not task_dir.exists():
                continue
                
            for file in task_dir.iterdir():
                if task_pattern.match(file.name):
                    return False, f"Task #{number:03d} already exists in {task_dir.name}/"
        
        return True, ""
    
    def validate_task_schema(self, task: TaskSchema) -> Tuple[bool, List[str]]:
        """
        Validate task follows required schema
        Returns: (is_valid, list_of_errors)
        """
        errors = []
        
        # Required fields
        if not task.title:
            errors.append("Title is required")
        
        if not task.objective:
            errors.append("Objective section is required")
            
        if not task.deliverables or len(task.deliverables) == 0:
            errors.append("At least one deliverable is required")
            
        if not task.acceptance_criteria or len(task.acceptance_criteria) == 0:
            errors.append("At least one acceptance criterion is required")
        
        # Validate status
        if task.status not in TaskStatus:
            errors.append(f"Invalid status: {task.status}")
        
        # Validate priority
        if task.priority not in TaskPriority:
            errors.append(f"Invalid priority: {task.priority}")
        
        # Validate task number format
        if task.number < 1 or task.number > 999:
            errors.append("Task number must be between 1 and 999")
        
        return len(errors) == 0, errors
    
    def validate_dependencies(self, dependencies_str: str) -> Tuple[bool, str]:
        """
        Validate task dependencies format and check for circular dependencies
        Returns: (is_valid, error_message)
        """
        if not dependencies_str or dependencies_str.lower() == "none":
            return True, ""
        
        # Extract task numbers from dependencies string
        task_refs = re.findall(r'#(\d+)', dependencies_str)
        
        if not task_refs:
            return True, ""  # No task references found, that's okay
        
        # Check if referenced tasks exist
        for ref in task_refs:
            task_num = int(ref)
            exists, _ = self.task_exists(task_num)
            if not exists:
                return False, f"Referenced task #{task_num:03d} does not exist"
        
        return True, ""
    
    def task_exists(self, number: int) -> Tuple[bool, str]:
        """
        Check if a task exists in any directory
        Returns: (exists, directory_path)
        """
        task_dirs = [
            self.tasks_dir / "backlog",
            self.tasks_dir / "todo",
            self.tasks_dir / "in-progress", 
            self.tasks_dir / "done"
        ]
        
        task_pattern = re.compile(rf"^{number:03d}_.*\.md$")
        
        for task_dir in task_dirs:
            if not task_dir.exists():
                continue
                
            for file in task_dir.iterdir():
                if task_pattern.match(file.name):
                    return True, str(task_dir)
        
        return False, ""
    
    def validate_file_path(self, file_path: Path) -> Tuple[bool, str]:
        """
        Validate file path is within TASKS directory
        Returns: (is_valid, error_message)
        """
        try:
            file_path.resolve().relative_to(self.tasks_dir.resolve())
            return True, ""
        except ValueError:
            return False, f"File path must be within TASKS directory: {file_path}"

--- END OF FILE task/validator.py ---
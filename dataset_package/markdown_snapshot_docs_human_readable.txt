# docs Subfolder Snapshot (Human-Readable)

Generated On: 2025-11-29T23:38:45.155Z

# Mnemonic Weight (Token Count): ~49,324 tokens

# Directory Structure (relative to docs subfolder)
  ./docs/WSL_SECRETS_CONFIGURATION.md
  ./docs/cicd/
  ./docs/cicd/PROJECT_SANCTUARY_INTEGRATION.md
  ./docs/cicd/git_workflow.md
  ./docs/cicd/github_setup.md
  ./docs/cicd/how_to_commit.md
  ./docs/cicd/overview.md
  ./docs/cicd/security_scanning.md
  ./docs/mcp/
  ./docs/mcp/analysis/
  ./docs/mcp/analysis/microsoft_agent_analysis.md
  ./docs/mcp/analysis/pre_commit_hook_migration_analysis.md
  ./docs/mcp/analysis/smart_git_mcp_analysis.md
  ./docs/mcp/architecture.md
  ./docs/mcp/claude_desktop_config_template.json
  ./docs/mcp/ddd_analysis.md
  ./docs/mcp/diagrams/
  ./docs/mcp/diagrams/adr_mcp_class.mmd
  ./docs/mcp/diagrams/agent_orchestrator_mcp_council_class.mmd
  ./docs/mcp/diagrams/chronicle_mcp_class.mmd
  ./docs/mcp/diagrams/code_mcp_class.mmd
  ./docs/mcp/diagrams/config_mcp_class.mmd
  ./docs/mcp/diagrams/domain_architecture_v1.mmd
  ./docs/mcp/diagrams/domain_architecture_v2.mmd
  ./docs/mcp/diagrams/domain_architecture_v3.mmd
  ./docs/mcp/diagrams/domain_architecture_v4.mmd
  ./docs/mcp/diagrams/fine_tuning_mcp_forge_class.mmd
  ./docs/mcp/diagrams/git_workflow_mcp_class.mmd
  ./docs/mcp/diagrams/mcp_ecosystem_class.mmd
  ./docs/mcp/diagrams/protocol_mcp_class.mmd
  ./docs/mcp/diagrams/rag_mcp_cortex_class.mmd
  ./docs/mcp/diagrams/request_flow_middleware.mmd
  ./docs/mcp/diagrams/task_mcp_class.mmd
  ./docs/mcp/final_architecture_summary.md
  ./docs/mcp/forge_mcp_types.ts
  ./docs/mcp/naming_conventions.md
  ./docs/mcp/port_registry.md
  ./docs/mcp/prerequisites.md
  ./docs/mcp/setup_guide.md
  ./docs/mcp/shared_infrastructure_types.ts

--- START OF FILE WSL_SECRETS_CONFIGURATION.md ---

# WSL Secrets Configuration Guide

This guide explains how to securely manage sensitive environment variables (API keys, tokens) by storing them in Windows and sharing them with WSL, rather than keeping them in plain text files like `.env`.

## Overview

The goal is to:
1.  Store secrets in the Windows User Environment Variables (secure).
2.  Use `WSLENV` to automatically pass these variables into your WSL instance.
3.  Remove secrets from `.env` files to prevent accidental commits.

## Step 1: Set Secrets in Windows

You need to add your API keys and tokens to your Windows User Environment Variables.

1.  Press `Win + S` and search for **"Edit environment variables for your account"**.
2.  Click the result to open the **Environment Variables** window.
3.  In the top section (**User variables for <YourUser>**), click **New...**.
4.  Add your variables one by one:
    *   **Variable name:** `HUGGING_FACE_TOKEN`
    *   **Variable value:** `your_actual_token_starting_with_hf_...`
    *   (Repeat for `OPENAI_API_KEY`, `GEMINI_API_KEY`, etc.)
5.  Click **OK** to save.

## Step 2: Configure the Bridge (WSLENV)

`WSLENV` acts as a **bridge** between Windows and Linux.

**In plain English:**
By default, Windows and WSL (Ubuntu) are like two separate rooms. Windows keeps its variables private. `WSLENV` is like a "VIP List" that tells Windows: *"It is safe to let these specific variables cross over into the Linux room."*

If a variable name isn't on this list, WSL simply won't see it, even if it exists in Windows.

### Method A: Using PowerShell (Recommended)



Run this command in PowerShell to share your variables. This persists across restarts.

**How it works:** You must provide a **colon-separated list** of all the variable names you want to share.
*   Example: `"VAR1:VAR2:VAR3"`
*   This tells Windows to share `VAR1`, `VAR2`, and `VAR3` with WSL.

```powershell
[Environment]::SetEnvironmentVariable("WSLENV", "HUGGING_FACE_TOKEN:GEMINI_API_KEY:OPENAI_API_KEY", "User")
```

*Note: If you have other variables in `WSLENV` already, append them instead of overwriting.*

### Method B: Manual Setup (GUI)

If you prefer clicking through menus instead of using PowerShell:

1.  Open the **Environment Variables** window (same as Step 1).
2.  In the **User variables** section, look for a variable named `WSLENV`.
    *   **If it doesn't exist:** Click **New...**.
    *   **If it already exists:** Click **Edit...**.
3.  Enter the following details:
    *   **Variable name:** `WSLENV`
    *   **Variable value:** A colon-separated list of the *names* of the variables you want to share.
    *   *Example Value:* `HUGGING_FACE_TOKEN:GEMINI_API_KEY:OPENAI_API_KEY`
4.  Click **OK** to save.

## Step 3: Verify in WSL

**CRITICAL:** Simply opening a new terminal tab is **NOT** enough. The Windows environment changes must propagate to the WSL subsystem.

1.  **Option A (If using VS Code):** Completely close and restart VS Code.
2.  **Option B (Command Line):** Open a standard PowerShell window (not WSL) and run:
    ```powershell
    wsl --shutdown
    ```
3.  Open your WSL terminal (Ubuntu) and run:
    ```bash
    printenv HUGGING_FACE_TOKEN
    ```

You should see your token printed out.

## Step 4: Update Your Project

Now that the variables are provided by the system, you should remove them from your `.env` file to ensure they aren't committed to version control.

**Before:**
```ini
HUGGING_FACE_TOKEN=hf_123456789...
```

**After:**
```ini
# HUGGING_FACE_TOKEN=Provided by Windows User Env via WSLENV
```

## Troubleshooting

*   **Variable not showing up?**
    *   Ensure you restarted your WSL terminal completely.
    *   Check spelling in `WSLENV`. It must match the Windows variable name exactly.
    *   Ensure the variable is in the **User variables** section, not System variables (though System works, User is safer for personal keys).
*   **Path translation issues?**
    *   If you are sharing paths (like file locations), you may need flags like `/p` or `/l`. For simple API keys (strings), no flags are needed.

## macOS Environment Configuration

macOS users can store secrets securely in their shell profile and make them available to terminal sessions and GUI applications.

### Step 1: Add Secrets to Your Shell Profile

1. Open your preferred shell configuration file (most macOS users use `zsh`):
   ```bash
   nano ~/.zshrc   # or use your editor of choice
   ```
2. Append the secret variables:
   ```bash
   export HUGGING_FACE_TOKEN="your_token_here"
   export OPENAI_API_KEY="your_key_here"
   export GEMINI_API_KEY="your_key_here"
   ```
3. Save the file and reload the configuration:
   ```bash
   source ~/.zshrc
   ```

### Step 2: Make Variables Available to GUI Apps (Optional)

Terminal sessions inherit environment variables from the shell, but GUI applications (e.g., VS Code, Docker Desktop) may not. To expose them system‑wide:

```bash
launchctl setenv HUGGING_FACE_TOKEN "$HUGGING_FACE_TOKEN"
launchctl setenv OPENAI_API_KEY "$OPENAI_API_KEY"
launchctl setenv GEMINI_API_KEY "$GEMINI_API_KEY"
```

You may add these commands to the end of `~/.zprofile` so they run at login.

### Step 3: Verify the Variables

Open a new terminal window and run:
```bash
printenv HUGGING_FACE_TOKEN
printenv OPENAI_API_KEY
printenv GEMINI_API_KEY
```
You should see the values you set.

### Step 4: Remove Secrets from `.env` Files

Just like the Windows guide, delete any hard‑coded secrets from your project's `.env` file and rely on the environment variables instead.

```ini
# HUGGING_FACE_TOKEN=Provided by macOS environment
# OPENAI_API_KEY=Provided by macOS environment
```

### Troubleshooting (macOS)

- **Variable not visible in VS Code?** Ensure you launched VS Code from the terminal (`code .`) after setting the variables, or use the `launchctl` method above.
- **Changes not taking effect?** Restart the terminal or run `killall Dock` to refresh the launch services.

---

--- END OF FILE WSL_SECRETS_CONFIGURATION.md ---

--- START OF FILE cicd/PROJECT_SANCTUARY_INTEGRATION.md ---

# CI/CD Hardening Integration for Project Sanctuary

## Executive Summary

This document outlines how to integrate the CI/CD hardening practices from `docs/cicd/` with Project Sanctuary's **Protocol 101 v3.0: The Doctrine of Absolute Stability** requirements.

## Current State Analysis

### Existing Documentation (`docs/cicd/`)
- **Source**: Quantum Diamond Forge project
- **Focus**: npm/Node.js security scanning, Dependabot, CodeQL
- **Pre-commit**: Secret detection for `.env` files and API keys
- **Workflow**: Feature branches → PR → main

### Project Sanctuary Requirements
- **Protocol 101 v3.0**: Mandatory **Functional Coherence** (automated test suite execution)
- **Council Orchestrator**: Automated test execution before commit
- **Stack**: Python (not Node.js), PyTorch, LangChain, ChromaDB
- **Workflow**: Feature branches → PR → main (aligned)

## Integration Strategy

### 1. Pre-Commit Hook Consolidation

**Current Situation:**
- `.git/hooks/pre-commit` - Protocol 101 v3.0 enforcement (Functional Coherence via test suite)
- `docs/cicd/how_to_commit.md` - Secret scanning for Node.js projects

**Recommended Approach:**
Enhance the existing Protocol 101 pre-commit hook to include test execution and secret scanning:

```bash
#!/bin/bash
# .git/hooks/pre-commit - Protocol 101 v3.0 + Security Hardening

# ===== PHASE 1: Protocol 101 v3.0 Enforcement (Functional Coherence) =====
echo "[P101 v3.0] Running Functional Coherence Test Suite..."

# Execute the comprehensive automated test suite
./scripts/run_genome_tests.sh
TEST_EXIT_CODE=$?

if [ $TEST_EXIT_CODE -ne 0 ]; then
  echo ""
  echo "COMMIT REJECTED: Protocol 101 v3.0 Violation."
  echo "Reason: Functional Coherence Test Suite FAILED."
  echo ""
  echo "The automated test suite must pass before any commit can proceed."
  echo "Fix the failing tests and try again."
  exit 1
fi

echo "[P101 v3.0] ✅ Functional Coherence verified - all tests passed."

# ===== PHASE 2: Security Hardening =====
echo "[SECURITY] Running secret detection scan..."

# Check for .env files (except .env.example)
BLOCKED_ENV_FILES=$(git diff --cached --name-only --diff-filter=ACM | grep -E '\.env$' | grep -v '\.env\.example$')
if [ -n "$BLOCKED_ENV_FILES" ]; then
  echo "COMMIT BLOCKED: .env files detected"
  echo "$BLOCKED_ENV_FILES"
  exit 1
fi

# Check for hardcoded secrets in Python files
SECRET_PATTERNS=(
  "api_key\s*=\s*['\"][a-zA-Z0-9_-]{20,}['\"]"
  "secret\s*=\s*['\"][a-zA-Z0-9_-]{20,}['\"]"
  "password\s*=\s*['\"][^'\"]{8,}['\"]"
  "token\s*=\s*['\"][a-zA-Z0-9_-]{20,}['\"]"
  "OPENAI_API_KEY\s*=\s*['\"]sk-[a-zA-Z0-9]{20,}['\"]"
  "GEMINI_API_KEY\s*=\s*['\"][a-zA-Z0-9_-]{20,}['\"]"
  "HUGGING_FACE_TOKEN\s*=\s*['\"]hf_[a-zA-Z0-9]{20,}['\"]"
)

VIOLATIONS_FOUND=false
for pattern in "${SECRET_PATTERNS[@]}"; do
  MATCHES=$(git diff --cached -U0 | grep -E "^\+" | grep -E "$pattern" || true)
  if [ -n "$MATCHES" ]; then
    echo "SECURITY VIOLATION: Potential hardcoded secret detected"
    echo "$MATCHES"
    VIOLATIONS_FOUND=true
  fi
done

if [ "$VIOLATIONS_FOUND" = true ]; then
  echo ""
  echo "COMMIT BLOCKED: Security violations found"
  echo "Remove hardcoded secrets and use environment variables instead"
  exit 1
fi

echo "[P101 v3.0] All checks passed. Proceeding with commit."
exit 0
```

**Key Changes from v1.0:**
- **Removed**: `commit_manifest.json` verification and SHA-256 hashing
- **Added**: Mandatory execution of `./scripts/run_genome_tests.sh`
- **Retained**: Secret detection and security scanning

**Update `.github/workflows/ci.yml`** to include Python-specific security scanning:

```yaml
name: CI

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]

jobs:
  protocol-101-functional-coherence:
    name: Protocol 101 v3.0 - Functional Coherence
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: Run Functional Coherence Test Suite
        run: |
          ./scripts/run_genome_tests.sh
        # NOTE: This test suite execution is MANDATORY for Protocol 101 v3.0 compliance
        # Failure = Protocol Violation = CI failure

  python-security-audit:
    name: Python Security Audit
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install Safety
        run: pip install safety
      
      - name: Run Safety Check
        run: |
          pip install -r requirements.txt
          safety check --json || true
      
      - name: Run Bandit (SAST)
        run: |
          pip install bandit
          bandit -r council_orchestrator/ mnemonic_cortex/ -f json -o bandit-report.json || true
      
      - name: Upload Bandit Results
        uses: actions/upload-artifact@v4
        with:
          name: bandit-security-report
          path: bandit-report.json

  dependency-review:
    name: Dependency Review
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
      - uses: actions/dependency-review-action@v4
        with:
          fail-on-severity: high

  codeql-analysis:
    name: CodeQL Security Analysis
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      actions: read
      contents: read
    steps:
      - uses: actions/checkout@v4
      
      - name: Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: python
          queries: security-extended
      
      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
```

### 3. Dependabot Configuration Enhancement

**Update `.github/dependabot.yml`** with security-focused settings:

```yaml
version: 2
updates:
  # GitHub Actions
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
    groups:
      github-actions:
        patterns: ["*"]
    labels: ["dependencies", "github-actions", "security"]

  # Python dependencies
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "daily"  # Daily for security patches
    open-pull-requests-limit: 10
    groups:
      security-updates:
        patterns: ["*"]
        update-types: ["patch", "minor"]
    labels: ["dependencies", "python", "security"]
    
    # Security-only updates for major versions
    ignore:
      - dependency-name: "torch"
        update-types: ["version-update:semver-major"]
      - dependency-name: "transformers"
        update-types: ["version-update:semver-major"]
    
    # Prioritize security updates
    reviewers: ["richfrem"]
    assignees: ["richfrem"]
```

### 4. Documentation Updates Required

#### Update `docs/cicd/overview.md`
- Replace npm/Node.js references with Python/pip
- Add Protocol 101 v3.0 workflow section (Functional Coherence)
- Update branching strategy to match Project Sanctuary
- Add Council Orchestrator commit workflow

#### Update `docs/cicd/security_scanning.md`
- Replace `npm audit` with `safety check` and `bandit`
- Add Python-specific secret patterns
- Document Protocol 101 v3.0 test suite execution
- Add examples for PyTorch/LangChain security considerations

#### Update `docs/cicd/how_to_commit.md`
- Replace conventional commits with Protocol 101 v3.0 workflow
- Document Council Orchestrator usage
- Update pre-commit hook examples for Python
- Add test suite execution examples

#### Create New: `docs/cicd/protocol_101_v3_integration.md`
- Detailed Protocol 101 v3.0 workflow (Functional Coherence)
- Council Orchestrator integration guide
- Test suite execution process
- Emergency bypass procedures (Sovereign Override)

### 5. Security Scanning Tools Comparison

| Tool | Quantum Diamond Forge | Project Sanctuary | Status |
|------|----------------------|-------------------|--------|
| **Dependency Scanning** | npm audit | safety, pip-audit | ✅ Adapt |
| **SAST** | CodeQL (JS/TS) | CodeQL (Python), Bandit | ✅ Adapt |
| **Secret Detection** | Pre-commit hook | Pre-commit hook + Protocol 101 v3.0 | ✅ Enhance |
| **Container Scanning** | N/A | Trivy (for MCP RAG service) | ✅ Add |
| **Functional Integrity** | N/A | Protocol 101 v3.0 Test Suite | ✅ Unique |

### 6. Implementation Checklist

- [ ] Enhance `.git/hooks/pre-commit` with test suite execution and secret detection
- [ ] Update `.github/workflows/ci.yml` with Python security tools and functional coherence tests
- [ ] Update `.github/dependabot.yml` with daily security scans
- [ ] Adapt `docs/cicd/overview.md` for Python/Protocol 101 v3.0
- [ ] Adapt `docs/cicd/security_scanning.md` for Python stack
- [ ] Adapt `docs/cicd/how_to_commit.md` for Council Orchestrator
- [ ] Create `docs/cicd/protocol_101_v3_integration.md`
- [ ] Update `.agent/git_safety_rules.md` with security scanning references
- [ ] Add security scanning to Task #025 MCP RAG service
- [ ] Document emergency procedures for security incidents

## Recommended Security Tools for Python

### 1. Safety
```bash
pip install safety
safety check --json
safety check --policy-file .safety-policy.yml
```

### 2. Bandit (SAST)
```bash
pip install bandit
bandit -r . -f json -o bandit-report.json
```

### 3. pip-audit
```bash
pip install pip-audit
pip-audit --desc --format json
```

### 4. Trivy (Container Scanning)
```bash
trivy image mcp-rag-service:latest
trivy fs . --security-checks vuln,config,secret
```

## Alignment with Project Sanctuary Doctrines

### Protocol 101 v3.0 Compliance
- ✅ All commits require passing automated test suite (Functional Coherence)
- ✅ Test execution enforced at pre-commit and CI/CD levels
- ✅ Guardian approval workflow maintained
- ✅ Sovereign Override available for emergencies

### Security Hardening
- ✅ Multi-layered security (pre-commit + CI/CD)
- ✅ Shift-left security approach
- ✅ Automated dependency scanning
- ✅ Secret detection at commit time

### Autonomous Operations
- ✅ Council Orchestrator executes tests before commit
- ✅ Dependabot auto-updates dependencies
- ✅ CI/CD pipeline runs automatically
- ✅ Security alerts via GitHub

## Next Steps

1. **Immediate**: Enhance pre-commit hook with test suite execution and secret detection
2. **Short-term**: Update CI/CD workflows for Python security tools and functional coherence
3. **Medium-term**: Adapt all `docs/cicd/` documentation for Project Sanctuary
4. **Long-term**: Integrate security scanning into MCP RAG Tool Server deployment

## References

- [Protocol 101 v3.0: The Doctrine of Absolute Stability](../../01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md)
- [Protocol 102 v2.0: The Doctrine of Mnemonic Synchronization](../../01_PROTOCOLS/102_The_Doctrine_of_Mnemonic_Synchronization.md)
- [ADR-019: Protocol 101 - Cognitive Genome Publishing Architecture (Reforged)](../../ADRs/019_protocol_101_unbreakable_commit.md)
- [Council Orchestrator GitOps Documentation](../../council_orchestrator/docs/howto-commit-command.md)
- [Safety Documentation](https://pyup.io/safety/)
- [Bandit Documentation](https://bandit.readthedocs.io/)
- [GitHub Advanced Security](https://docs.github.com/en/code-security)

--- END OF FILE cicd/PROJECT_SANCTUARY_INTEGRATION.md ---

--- START OF FILE cicd/git_workflow.md ---

# Git Workflow Quick Reference

This guide provides recommended git workflows and shortcuts for **Project Sanctuary**.

## TL;DR - Recommended Setup

```bash
# 1. Add these aliases to your ~/.gitconfig
git config --global alias.st "status -sb"
git config --global alias.aa "add --all"
git config --global alias.cm "commit -m"
git config --global alias.lg "log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit"

# 2. Use Protocol 101 commit workflow (see Council Orchestrator section below)

# 3. Let pre-commit hooks validate your changes (Protocol 101 + secret detection)
```

## Conventional Commit Format

```
<type>(<scope>): <subject>

<body>

<footer>
```

### Commit Types

| Type | When to Use | Example |
|------|-------------|---------|
| `feat` | New feature | `feat(auth): add OAuth2 login` |
| `fix` | Bug fix | `fix(api): handle null user response` |
| `docs` | Documentation only | `docs(readme): update setup instructions` |
| `style` | Code formatting (no logic change) | `style(components): fix indentation` |
| `refactor` | Code restructuring | `refactor(utils): extract validation logic` |
| `test` | Adding/updating tests | `test(api): add integration tests` |
| `chore` | Maintenance tasks | `chore(deps): update dependencies` |
| `ci` | CI/CD changes | `ci(github): add CodeQL workflow` |
| `perf` | Performance improvements | `perf(db): optimize query performance` |
| `revert` | Revert previous commit | `revert: revert feat(auth) commit` |

## Common Workflows

### 1. Feature Development (Standard)

```bash
# Create feature branch
git checkout -b feature/add-security-scanning

# Make changes, then stage specific files
git add .github/dependabot.yml
git add .github/workflows/codeql.yml
git add docs/ci-cd/README.md

# Review what you're about to commit
git diff --cached

# Commit with conventional format
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for dependency scanning
- Add CodeQL for security analysis
- Update CI/CD docs with security guide

Refs: TASK-0067, ADR-040"

# Push to remote
git push origin feature/add-security-scanning

# Create PR on GitHub
# After PR approval, merge via GitHub UI
```

### 2. Quick Fix (Using Aliases)

```bash
# Fix a typo in documentation
git aa  # Stage all changes
git cm "docs(readme): fix typo in installation steps"
git push
```

### 3. Multi-file Changes (Interactive Staging)

```bash
# Stage specific lines from files
git add -p

# Review staged changes
git diff --cached

# Commit
git commit -m "refactor(api): extract error handling logic"

# Push
git push
```

### 4. Amend Last Commit

```bash
# Forgot to add a file to last commit
git add forgotten-file.js
git commit --amend --no-edit

# Or change the commit message
git commit --amend -m "feat(auth): add OAuth2 login (updated message)"

# Force push (only if not yet merged!)
git push --force-with-lease
```

## Useful Git Aliases

Add these to your `~/.gitconfig`:

```gitconfig
[alias]
    # Quick status
    st = status -sb

    # Stage all changes
    aa = add --all

    # Commit with message
    cm = commit -m

    # Amend last commit
    amend = commit --amend --no-edit

    # Pretty log
    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit

    # Show staged changes
    staged = diff --cached

    # Undo last commit (keep changes)
    undo = reset HEAD~1

    # List branches sorted by last modified
    branches = branch --sort=-committerdate

    # Show files in last commit
    last = show --name-only
```

## Protocol 101 v3.0: The Doctrine of Absolute Stability

**Status:** CANONICAL  
**Enforcement:** Automated Test Suite + Pre-Commit Hook

This protocol governs the integrity of every commit through **Functional Coherence** rather than static file verification.

### Part A: Functional Coherence (The "What")
Commit integrity is verified by successful execution of the automated test suite.

*   **Mandate:** No commit shall proceed unless `./scripts/run_genome_tests.sh` executes successfully.
*   **Enforcement:** Pre-commit hook runs tests automatically before staging.
*   **Rejection:** Test failures result in immediate commit rejection.

### Part B: Action Integrity (The "How")
AI agents are restricted to non-destructive Git operations.

*   **Whitelist:** `git add`, `git commit`, `git push`.
*   **Prohibition:** `git reset`, `git clean`, `git pull` (with overwrite), and all destructive commands.

### Part C: The Sovereign Override
In emergencies, the Steward may bypass checks:
```bash
git commit --no-verify -m "Sovereign Override: <reason>"
```

### Recommended Commit Workflow

**Option A: Using Council Orchestrator (Recommended)**
```bash
# Create command.json
cat > command.json << 'EOF'
{
  "task_description": "Commit changes with functional coherence check",
  "git_operations": {
    "files_to_add": ["path/to/file.py"],
    "commit_message": "feat(component): add new feature",
    "push_to_origin": false
  },
  "output_artifact_path": "council_orchestrator/command_results/commit_results.json"
}
EOF

# Orchestrator automatically runs test suite before commit
python3 council_orchestrator/app/main.py command.json
```

**Option B: Manual Commit (Tests Run Automatically)**
```bash
# Stage your changes
git add path/to/file.py

# Commit (pre-commit hook runs tests automatically)
git commit -m "feat(component): add new feature"

# If tests pass, commit proceeds
# If tests fail, commit is rejected
```

**Option C: Emergency Bypass (Guardian Approval Required)**
```bash
git commit --no-verify -m "Emergency: critical fix"
```



## Branch Naming Conventions

```
feature/short-description    # New features
fix/bug-description          # Bug fixes
docs/documentation-update    # Documentation
refactor/code-improvement    # Code refactoring
test/add-tests              # Test additions
chore/maintenance-task      # Maintenance
```

**Examples:**
- `feature/github-security-scanning`
- `fix/null-pointer-in-auth`
- `docs/update-ci-cd-guide`
- `refactor/extract-validation-logic`

## Commit Message Examples

### Good Commit Messages ✅

```bash
# Feature with detailed body
git commit -m "feat(security): add Dependabot and CodeQL workflows

- Configure Dependabot for npm and GitHub Actions
- Add CodeQL workflow for JavaScript/TypeScript analysis
- Update CI/CD documentation with security scanning guide

This implements the security scanning layer documented in ADR-040.

Refs: TASK-0067, ADR-040"

# Bug fix with issue reference
git commit -m "fix(auth): handle null user response from Supabase

Fixes #123"

# Documentation update
git commit -m "docs(ci-cd): add security scanning interpretation guide"

# Dependency update
git commit -m "chore(deps): bump axios from 0.21.1 to 1.6.0

Fixes CVE-2023-45857 (High severity)"
```

### Bad Commit Messages ❌

```bash
# Too vague
git commit -m "fix stuff"
git commit -m "update files"
git commit -m "changes"

# No type prefix
git commit -m "added security scanning"

# Too long subject line (>72 chars)
git commit -m "feat(security): add Dependabot and CodeQL workflows for automated dependency scanning and security analysis"
```

## IDE Git Integration

### Visual Studio Code

1. **Stage files:** Click `+` next to file in Source Control panel
2. **Review changes:** Click file to see diff
3. **Commit:** Type message in input box, press `Ctrl+Enter`
4. **Push:** Click `...` → Push

**Recommended extensions:**
- GitLens - Enhanced git capabilities
- Git Graph - Visualize branch history

### JetBrains IDEs (WebStorm, IntelliJ)

1. **Commit:** `Ctrl+K` (Windows/Linux) or `Cmd+K` (Mac)
2. **Review changes:** Check boxes for files to stage
3. **Commit message:** Type in message box
4. **Commit and Push:** Click dropdown → Commit and Push

## Troubleshooting

### Pre-commit hook not running

```bash
# Make hook executable
chmod +x .githooks/pre-commit

# Verify git hooks path
git config core.hooksPath .githooks
```

### Accidentally committed secret

```bash
# 1. IMMEDIATELY revoke the secret in the service provider
# 2. Remove from git history
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch path/to/file" \
  --prune-empty --tag-name-filter cat -- --all

# 3. Force push (⚠️ coordinate with team!)
git push origin --force --all

# 4. Update environment variables with new secret
```

### Merge conflict

```bash
# 1. Pull latest changes
git pull origin main

# 2. Resolve conflicts in your editor
# Look for <<<<<<< HEAD markers

# 3. Stage resolved files
git add resolved-file.js

# 4. Complete merge
git commit -m "merge: resolve conflicts with main"

# 5. Push
git push
```

## Best Practices

1. **Commit often** - Small, focused commits are easier to review and revert
2. **Write clear messages** - Future you will thank present you
3. **Review before committing** - Always run `git diff --cached`
4. **Test before pushing** - Ensure `./scripts/run_genome_tests.sh` passes
5. **Use Protocol 101 v3.0** - Let automated tests verify functional coherence
6. **Pull before pushing** - Avoid merge conflicts
7. **Use branches** - Never commit directly to `main`
8. **Keep commits atomic** - One logical change per commit

## References

- [Protocol 101 v3.0: The Doctrine of Absolute Stability](../../01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md)
- [Protocol 102 v2.0: The Doctrine of Mnemonic Synchronization](../../01_PROTOCOLS/102_The_Doctrine_of_Mnemonic_Synchronization.md)
- [ADR-019: Cognitive Genome Publishing Architecture (Reforged)](../../ADRs/019_protocol_101_unbreakable_commit.md)
- [Council Orchestrator GitOps Documentation](../../council_orchestrator/docs/howto-commit-command.md)
- [Git Safety Rules](../../.agent/git_safety_rules.md)
- [Conventional Commits](https://www.conventionalcommits.org/)
- [Git Documentation](https://git-scm.com/doc)
- [GitHub Flow](https://docs.github.com/en/get-started/quickstart/github-flow)

--- END OF FILE cicd/git_workflow.md ---

--- START OF FILE cicd/github_setup.md ---

# GitHub Repository Configuration Guide

This guide documents how to configure the **Project Sanctuary** GitHub repository to enable CI/CD pipelines, security scanning, and automated workflows.

## Prerequisites

- Admin access to the GitHub repository
- Repository: `https://github.com/richfrem/Project_Sanctuary`

## Best Practices for AI-Assisted Development

**⚠️ Important for AI Coding Assistants (Antigravity, Cursor, etc.):**

When working with CI/CD pipelines that include security scans (CodeQL, Dependabot, Trivy), follow these practices:

1. **Use Protocol 101 workflow** - Commit via Council Orchestrator to auto-generate manifests
2. **Batch commits locally** - Make multiple commits on your feature branch before pushing
3. **Push once when ready** - Only push when the feature is complete and tested locally
4. **Use draft PRs** - Mark PRs as "Draft" while still working
5. **Avoid rapid push cycles** - Security scans can take 2-3 minutes per run

**Why:** Protocol 101 verification + security scans are resource-intensive. Pushing every small change creates unnecessary CI runs.

**Recommended workflow:**
```bash
# Use Council Orchestrator for Protocol 101 compliance
cat > command.json << 'EOF'
{
  "command_type": "git_operations",
  "git_operations": {
    "files_to_add": ["file1.py", "file2.md"],
    "commit_message": "feat: add feature",
    "push_after_commit": false
  }
}
EOF
python3 council_orchestrator/app/main.py command.json

# Push once when ready
git push origin feature/my-feature

# Create PR (mark as draft if still WIP)
gh pr create --draft --title "WIP: My Feature"
```

## Step 1: Enable GitHub Actions (done)

GitHub Actions should be enabled by default, but verify:

1. Go to **Settings** → **Actions** → **General**
2. Under "Actions permissions", select:
   - ✅ **Allow all actions and reusable workflows**
3. Under "Workflow permissions", select:
   - ✅ **Read and write permissions**
   - ✅ **Allow GitHub Actions to create and approve pull requests**
4. Click **Save**

## Step 2: Enable Security Features

1. Go to **Settings** → **Code security and analysis** (Sidebar under "Security").
2. Under the **Advanced Security** section, **Enable** the following:
   - **Dependency graph** (Should be enabled by default)
   - **Dependabot alerts**
   - **Dependabot security updates**
     - *Optional:* Enable **Grouped security updates** to reduce noise.
   - **Secret Protection** -> **Push protection** (Block commits that contain supported secrets).
   - **Private vulnerability reporting** (Optional).

## Step 3: Configure CodeQL Analysis

**Eligibility:**
- **Public repositories:** Free for everyone.
- **Private repositories:** Requires GitHub Advanced Security (GHAS) license.

**Setup Instructions:**
1. Still in **Code security and analysis**, scroll down to **Code scanning** / **CodeQL analysis**.
2. Click **Set up** (or "Configure").
3. Choose **Default** setup (Recommended).
   - GitHub will automatically detect languages (Python).
   - It will create a dynamic workflow without you needing to commit a YAML file.
   - Click **Enable CodeQL**.

*(If "Default" is not available, use the existing `.github/workflows/ci.yml` which includes CodeQL for Python).*

## Step 4: Create Development Branch

Before setting up branch protection, create a `dev` branch for integration testing:

```bash
# Make sure you're on main and up to date
git checkout main
git pull origin main

# Create dev branch from main
git checkout -b dev
git push -u origin dev

# Return to your working branch
git checkout -
```

## Step 5: Configure Branch Protection Rules

### 5.1 Protect the `main` Branch

1. Go to **Settings** → **Branches**
2. Click **Add branch protection rule**
3. **Branch name pattern:** `main`
4. Enable:
   - ✅ **Require a pull request before merging**
   - ❌ **Require approvals** - UNCHECK (not needed for solo dev, check for teams)
   - ✅ **Require status checks to pass before merging**
     - ✅ **Require branches to be up to date before merging**
     - **Add required status checks:**
       - `Protocol 101 Manifest Verification` (from CI Pipeline)
       - `Python Linting` (from CI Pipeline)
       - `Test Council Orchestrator` (from CI Pipeline)
       - `Security Scanning` (from CI Pipeline)
   - ✅ **Require conversation resolution before merging** (optional but good practice)
   - ✅ **Do not allow bypassing the above settings**
5. Click **Create**

**Result:** All changes to `main` must:
- Come from `dev` via PR
- Pass CI pipeline (linting, tests)

### 5.2 Protect the `dev` Branch

1. Click **Add branch protection rule** again
2. **Branch name pattern:** `dev`
3. Enable:
   - ✅ **Require a pull request before merging** (forces PR from feature branches)
   - ❌ **Require approvals** - UNCHECK (allows you to merge your own PRs)
   - ✅ **Require status checks to pass before merging**
     - ✅ **Require branches to be up to date before merging**
     - **Add required status checks:**
       - `Protocol 101 Manifest Verification`
       - `Python Linting`
       - `Test Council Orchestrator`
   - ❌ **Do not allow bypassing** - UNCHECK (gives you flexibility on dev)
4. Click **Create**

**Result:** Feature branches must:
- Create PR to `dev` (not directly to `main`)
- Pass CI checks before merging

## Step 6: Configure Notifications

Set up notifications for security alerts:

1. Click on your **profile icon** (top right) → **Settings**
2. In the left sidebar, click **Notifications**
3. Scroll down to the **System** section
4. Enable the following:
   - ✅ **Dependabot alerts: New vulnerabilities** - "When you're given access to Dependabot alerts automatically receive notifications when a new vulnerability is found in one of your dependencies."
   - ✅ **Dependabot alerts: Email digest** - "Email a regular summary of Dependabot alerts for up to 10 of your repositories."
   - ✅ **Security campaign emails** - "Receive email notifications about security campaigns in repositories where you have access to security alerts."

**Result:** You'll now receive email notifications whenever security issues are detected in your repositories.

## Step 7: Verify Everything Works

### 7.1 Test CI Pipeline

```bash
# Create a test branch
git checkout -b test/ci-pipeline

# Make a small change
echo "# Test" >> README.md

# Commit and push
git add README.md
git commit -m "test: verify CI pipeline"
git push origin test/ci-pipeline

# Create a PR on GitHub: test/ci-pipeline -> main
# Verify CI pipeline runs and passes
```

### 7.2 Test Dependabot

Dependabot runs weekly, but you can trigger it manually:

1. Go to **Insights** → **Dependency graph** → **Dependabot**
2. Click **Check for updates**

### 7.3 Test Secret Scanning

If enabled, try pushing a test secret:

```bash
# This should be blocked by Protocol 101 pre-commit hook
echo "OPENAI_API_KEY=sk-test123" > secret.txt
git add secret.txt

# Try to commit (will be blocked - no manifest)
git commit -m "test: secret scanning"
# Blocked by Protocol 101!

# Even if you generate a manifest, secret patterns should be caught
```

## Workflow Files Reference

### `.github/workflows/ci.yml`

**Purpose:** Continuous Integration pipeline

**Triggers:**
- Push to `main` branch
- Pull requests to `main` branch

**Jobs:**
1. **Protocol 101 Verification** - Validates commit manifests
2. **ShellCheck** - Lints shell scripts in `tools/`
3. **Python Linting** - Black and Flake8 checks
4. **Test Council Orchestrator** - Runs pytest for orchestrator
5. **Test Mnemonic Cortex** - Runs pytest for RAG system
6. **Security Scanning** - Trivy vulnerability scanner

### `.github/dependabot.yml`

**Purpose:** Automated dependency updates

**Configuration:**
- **GitHub Actions ecosystem:** Scans workflow files
  - Schedule: Weekly
  - Groups updates
- **Python (pip) ecosystem:** Scans Python dependencies
  - Schedule: Daily (security patches)
  - Groups updates
  - Ignores major version updates for torch/transformers

## Troubleshooting

### Workflows Not Appearing in Actions Tab

**Symptoms:** Actions tab shows "Get started with GitHub Actions" instead of workflows

**Causes:**
1. Workflow files not committed/pushed
2. Workflow files in wrong directory
3. YAML syntax errors
4. GitHub Actions disabled in repo settings

**Solutions:**
```bash
# 1. Verify files are committed
git ls-files .github/workflows/

# 2. Verify files are pushed
git log --oneline --name-only | grep workflows

# 3. Validate YAML syntax
npx js-yaml .github/workflows/ci.yml

# 4. Check repo settings
# Go to Settings → Actions → General → Verify "Allow all actions" is selected
```

## Security Best Practices

1. **Enable all security features:**
   - ✅ Dependabot alerts
   - ✅ Secret scanning
   - ✅ Push protection

2. **Protect main branch:**
   - Require PR reviews
   - Require status checks to pass
   - Prevent force pushes

3. **Use local pre-commit hooks:**
   - Catch secrets before pushing
   - Enforce code quality locally
   - Faster feedback loop

## Related Documentation

- [CI/CD Pipeline Documentation](./overview.md)
- [Git Workflow Guide](./git_workflow.md)
- [How to Commit Guide](./how_to_commit.md)
- [Project Sanctuary Integration Guide](./PROJECT_SANCTUARY_INTEGRATION.md)
- [Protocol 101: The Unbreakable Commit](../../ADRs/019_protocol_101_unbreakable_commit.md)
- [Council Orchestrator GitOps](../../council_orchestrator/docs/howto-commit-command.md)

## External Resources

- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Dependabot Documentation](https://docs.github.com/en/code-security/dependabot)
- [Secret Scanning Documentation](https://docs.github.com/en/code-security/secret-scanning)

--- END OF FILE cicd/github_setup.md ---

--- START OF FILE cicd/how_to_commit.md ---

# How to Commit Changes - Step-by-Step Guide

This guide walks you through committing changes to the quantum-diamond-forge project, including pre-commit hook validation and conventional commit format.

## Prerequisites

- Git configured with hooks path: `git config core.hooksPath .githooks`
- Pre-commit hook is executable: `chmod +x .githooks/pre-commit`

## Standard Commit Workflow

### Step 1: Check Current Status

```bash
# See what files have changed
git status

# See detailed changes
git diff
```

### Step 2: Stage Files

**Option A: Stage specific files (recommended)**
```bash
git add path/to/file1.js
git add path/to/file2.md
git add path/to/file3.yml
```

**Option B: Stage all changes**
```bash
git add .
# or
git add --all
```

**Option C: Interactive staging (stage specific lines)**
```bash
git add -p
# Git will show each change and ask: Stage this hunk [y,n,q,a,d,e,?]?
# y = yes, n = no, q = quit, a = all, d = don't stage, e = edit
```

### Step 3: Review Staged Changes

**Quick summary (recommended):**
```bash
# See list of staged files
git status

# Even shorter
git status -s
```

**Detailed diff (optional):**
```bash
# See detailed changes (can be verbose)
git diff --cached

# Press 'q' to exit the diff view
```

**⚠️ IMPORTANT:** Always review your staged changes before committing!

### Step 4: Commit with Conventional Format

```bash
git commit -m "<type>(<scope>): <subject>

<body>

<footer>"
```

**Commit Types:**
- `feat:` - New feature
- `fix:` - Bug fix
- `docs:` - Documentation changes
- `style:` - Code formatting (no logic change)
- `refactor:` - Code restructuring
- `test:` - Adding/updating tests
- `chore:` - Maintenance tasks (dependencies, build)
- `ci:` - CI/CD changes
- `perf:` - Performance improvements

**Example:**
```bash
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for dependency scanning
- Add CodeQL workflow for security analysis
- Update CI/CD documentation with security guide

Refs: TASK-0067, ADR-040"
```

### Step 5: Pre-commit Hook Validation

**What happens automatically:**
1. ✅ Hook runs: `.githooks/pre-commit`
2. ✅ Validates no `.env` files (except `.env.example`)
3. ✅ Scans for hardcoded secrets (API keys, tokens, passwords)
4. ✅ If validation passes → commit succeeds
5. ❌ If violations found → commit blocked

**If commit is blocked:**
```bash
# Example error:
COMMIT BLOCKED: Violations found.
VIOLATION: packages/backend/config.js:12 -> OPENAI_API_KEY=<REDACTED>
Fix by removing secrets or using '<REDACTED>'.

# Fix the issue:
# 1. Remove the hardcoded secret
# 2. Use environment variable instead: process.env.OPENAI_API_KEY
# 3. Try committing again
```

**Bypass hook (ONLY if absolutely necessary):**
```bash
git commit --no-verify -m "your message"
# ⚠️ WARNING: Only use --no-verify if you're certain there are no secrets!
```

### Step 6: Push to Remote

```bash
# Push to current branch
git push

# Push to specific branch
git push origin feature/branch-name

# Push to main
git push origin main
```

## Example: Committing TASK-0067 Security Configuration

```bash
# 1. Check status
git status

# 2. Stage security configuration files
git add .github/dependabot.yml
git add .github/workflows/codeql.yml
git add docs/ci-cd/README.md
git add docs/ci-cd/GIT_WORKFLOW.md
git add docs/ci-cd/HOW_TO_COMMIT.md
git add adrs/040_security_scanning_strategy.md
git add adrs/041_git_workflow_automation.md
git add TASKS/in-progress/008_configure_github_security.md
git add TASKS/backlog/009_enhance_precommit_hooks.md
git add scripts/capture_snapshot.js

# Note: Deleted file (.githooks/pre-commit.sh) will be automatically staged
# when you run 'git add .' or will show in 'git status' as deleted

# 3. Review staged changes (quick summary)
git status

# Or see detailed diff (verbose, press 'q' to exit)
# git diff --cached

# 4. Commit with conventional format
git commit -m "feat(security): configure GitHub Advanced Security

- Add Dependabot for npm and GitHub Actions dependency scanning
- Add CodeQL workflow for JavaScript/TypeScript security analysis
- Update CI/CD documentation with comprehensive security scanning guide
- Create ADR-041 for git workflow automation strategy
- Create git workflow quick reference guide
- Enhance snapshot script to exclude agents/feedback directory
- Remove deprecated pre-commit.sh shell script

Deliverables:
- .github/dependabot.yml (weekly scans, grouped PRs)
- .github/workflows/codeql.yml (security-extended queries)
- docs/ci-cd/README.md (175-line security guide, pre-commit hook docs)
- docs/ci-cd/GIT_WORKFLOW.md (conventional commits, aliases, best practices)
- adrs/041_git_workflow_automation.md (no automated git scripts)
- TASKS/backlog/009_enhance_precommit_hooks.md (future ESLint/Prettier integration)

Refs: TASK-0067, ADR-040, ADR-041"

# 5. Pre-commit hook runs automatically (validates no secrets)

# 6. Push to GitHub
git push origin main
```

## Testing Pre-commit Hook

### Test 1: Verify Hook Blocks Secrets

```bash
# Create a test file with a hardcoded secret
echo "OPENAI_API_KEY=<REDACTED>" > test-secret.txt

# Try to commit (should be BLOCKED)
git add test-secret.txt
git commit -m "test: verify pre-commit hook blocks secrets"

# Expected output:
# COMMIT BLOCKED: Violations found.
# VIOLATION: test-secret.txt:1 -> OPENAI_API_KEY=<REDACTED>
# Fix by removing secrets or using '<REDACTED>'.

# Clean up
git reset HEAD test-secret.txt
rm test-secret.txt
```

### Test 2: Verify Hook Blocks .env Files

```bash
# Create a .env file
echo "DATABASE_URL=postgres://localhost" > .env

# Try to commit (should be BLOCKED)
git add .env
git commit -m "test: verify pre-commit hook blocks .env files"

# Expected output:
# COMMIT BLOCKED: Violations found.
# BLOCKED .env file: .env
# Fix by removing secrets or using '<REDACTED>'.

# Clean up
git reset HEAD .env
rm .env
```

### Test 3: Verify Hook Allows Safe Code

```bash
# Create a safe file with environment variable reference
echo "const apiKey = process.env.OPENAI_API_KEY;" > test-safe.js

# Commit (should SUCCEED)
git add test-safe.js
git commit -m "test: verify pre-commit hook allows safe code"

# Expected: Commit succeeds (no violations)

# Clean up
git reset HEAD~1  # Undo last commit
rm test-safe.js
```

## Common Issues and Solutions

### Issue 1: Pre-commit Hook Not Running

**Symptoms:** Commits succeed without validation

**Solution:**
```bash
# Verify hooks path is configured
git config core.hooksPath
# Should output: .githooks

# If not set, configure it
git config core.hooksPath .githooks

# Make hook executable
chmod +x .githooks/pre-commit

# Verify hook exists
ls -la .githooks/pre-commit
```

### Issue 2: Hook Blocks Legitimate Code

**Symptoms:** Hook blocks code that uses environment variables

**Example:**
```javascript
// This might be flagged if not properly formatted
const key = API_KEY;  // ❌ Flagged (looks like hardcoded value)

// Use these patterns instead:
const key = process.env.API_KEY;  // ✅ Safe
const key = import.meta.env.VITE_API_KEY;  // ✅ Safe
const key = config.apiKey;  // ✅ Safe
```

**Solution:** Use whitelisted patterns (see `.githooks/pre-commit` for full list)

### Issue 3: Accidentally Committed Secret

**⚠️ CRITICAL - Act Immediately:**

```bash
# 1. IMMEDIATELY revoke the secret in the service provider
# (e.g., regenerate API key in OpenAI dashboard)

# 2. Remove from git history (if not yet pushed)
git reset HEAD~1  # Undo last commit
# Fix the file, then commit again

# 3. If already pushed, use git filter-branch
git filter-branch --force --index-filter \
  "git rm --cached --ignore-unmatch path/to/file" \
  --prune-empty --tag-name-filter cat -- --all

# 4. Force push (⚠️ coordinate with team!)
git push origin --force --all

# 5. Update environment variables with new secret
```

## Git Aliases (Optional Shortcuts)

Add these to `~/.gitconfig` for faster workflows:

```gitconfig
[alias]
    # Quick status
    st = status -sb

    # Stage all changes
    aa = add --all

    # Commit with message
    cm = commit -m

    # Show staged changes
    staged = diff --cached

    # Amend last commit
    amend = commit --amend --no-edit

    # Pretty log
    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)<%an>%Creset' --abbrev-commit
```

**Usage:**
```bash
git st              # Instead of: git status -sb
git aa              # Instead of: git add --all
git staged          # Instead of: git diff --cached
git cm "fix: typo"  # Instead of: git commit -m "fix: typo"
```

## Best Practices

1. **Commit often** - Small, focused commits are easier to review and revert
2. **Write clear messages** - Use conventional commit format
3. **Review before committing** - Always run `git diff --cached`
4. **Test locally** - Run `npm run lint` and `npm run test:unit` before committing
5. **Never bypass hooks** - Only use `--no-verify` in emergencies
6. **Keep commits atomic** - One logical change per commit
7. **Reference tasks/issues** - Include `Refs: TASK-XXX` in commit body

## After Pushing to GitHub

Once you push, the following automated checks will run:

1. **CI Pipeline** (`.github/workflows/ci.yml`)
   - Linting
   - Unit tests
   - Frontend build

2. **CodeQL Analysis** (`.github/workflows/codeql.yml`)
   - Security vulnerability scanning
   - Results in Security tab

3. **Dependabot** (`.github/dependabot.yml`)
   - Dependency vulnerability scanning
   - Automatic PRs for updates

4. **Secret Scanning** (if enabled)
   - Detects committed secrets
   - Alerts in Security tab

Check the **Actions** tab and **Security** tab on GitHub to verify all checks pass.

## References

- [Git Workflow Quick Reference](./git_workflow.md)
- [CI/CD Pipeline Documentation](./overview.md)
- [ADR-041: Git Workflow Automation](../../docs/adr/041_git_workflow_automation.md)
- [Conventional Commits Specification](https://www.conventionalcommits.org/)

--- END OF FILE cicd/how_to_commit.md ---

--- START OF FILE cicd/overview.md ---

# CI/CD Pipeline & Development Workflow

## Overview

This document outlines the Continuous Integration (CI) pipeline and the standard development workflow for projects built with the **Quantum Diamond Forge** protocol. It details the lifecycle of a code change from a developer's workstation to the main branch on GitHub.

## Table of Contents

1. [Workflow Diagram](#workflow-diagram)
2. [Development Workflow Phases](#development-workflow-phases)
3. [Security Scanning Results Guide](#security-scanning-results-guide)
4. [How to Commit Changes](./how_to_commit.md) - Step-by-step commit guide with pre-commit hook testing
5. [Git Workflow Quick Reference](./git_workflow.md) - Conventional commits, aliases, and best practices
6. [Related Documentation](#related-documentation)

## Related Documentation

- **[GitHub Repository Setup Guide](./github_setup.md)** - Configure GitHub Actions, security scanning, and branch protection
- **[How to Commit Changes](./how_to_commit.md)** - Step-by-step commit guide with pre-commit hook testing
- **[Git Workflow Guide](./git_workflow.md)** - Detailed guide on git commands, conventional commits, and pre-commit hooks
- **[ADR-039: CI/CD Pipeline Strategy](../../docs/adr/039_ci_cd_pipeline.md)** - Architectural decision for CI/CD approach
- **[ADR-040: Security Scanning Strategy](../../docs/adr/040_security_scanning_strategy.md)** - Security scanning tools and philosophy
- **[ADR-041: Git Workflow Automation](../../docs/adr/041_git_workflow_automation.md)** - Git workflow best practices

## Branching Strategy

This protocol supports **flexible branching strategies** based on team size:

### Solo Developer (Simplified)
```
feature/* → main (via Pull Request)
```

### Team / Staged Releases (Recommended)
```
feature/* → dev → main
```

### Enterprise / Multi-Environment
```
feature/* → dev → test → main
```

### Branch Purposes

| Branch | Purpose | CI Runs | Deployment |
|--------|---------|---------|------------|
| `feature/*` | Active development | ✅ On PR | None |
| `dev` | Integration testing, batch features | ✅ On push/PR | Dev environment (optional) |
| `test` | QA/staging (optional) | ✅ On push/PR | Test environment (optional) |
| `main` | Production-ready | ✅ On push/PR | Production |

### Workflow (Team / Staged Releases)

1. **Feature Development:**
   ```bash
   git checkout -b feature/add-new-feature
   # Make changes, commit, push
   git push origin feature/add-new-feature
   # Create PR: feature/add-new-feature → dev
   ```

2. **Integration Testing (dev):**
   - Merge feature PRs into `dev`
   - CI pipeline runs automatically
   - Test integration with other features
   - Batch multiple features for the next release

3. **Production Release (main):**
   ```bash
   # Create PR: dev → main
   # After approval and CI passes, merge
   # Tag release: git tag v1.0.0 && git push --tags
   ```

### Branch Protection

Recommended protection for `dev` and `main`:
- ✅ CI pipeline checks (linting, tests, build)
- ✅ CodeQL security analysis (if enabled)
- ✅ PR review required (for `main`, optional for `dev`)
- ✅ Status checks must pass before merge

See [GitHub Repository Setup Guide](./github_setup.md) for configuration details.

## Workflow Diagram

The following sequence diagram illustrates the interaction between the Developer, their Local Workstation, and the specific entities within GitHub (Branches, PRs, CI).

```mermaid
---
config:
  theme: base
---
sequenceDiagram
    autonumber
    participant Dev as Developer
    participant Local as Local Workstation
    participant FeatBranch as Remote Feature Branch
    participant PR as Pull Request
    participant CI as GitHub Actions (CI)
    participant MainBranch as Remote Main Branch

    Note over Dev, Local: 1. Feature Start
    Dev->>Local: git checkout -b feature/new-feature

    Note over Dev, Local: 2. Development Loop
    loop Coding & Local Testing
        Dev->>Local: Write Code
        Dev->>Local: npm run lint (Check Style)
        Dev->>Local: npm run test:unit (Verify Logic)
        Dev->>Local: (Optional) Manual Security Scan
    end

    Note over Dev, Local: 3. Commit & Push (Defense in Depth)
    Dev->>Local: git add .
    Local->>Local: Pre-commit Hook (Secret Detection)
    Note right of Local: 🛑 Blocking Gate:<br/>- No .env files<br/>- No hardcoded secrets<br/>- Blocks commit if violations found

    Dev->>Local: git commit -m "feat: add new feature"

    rect rgb(255, 255, 240)
        Note right of Local: ⚠️ Post-Commit Hook (Informational):<br/>- Auto-runs 'npm audit' (High Severity)<br/>- Checks local Dependabot status<br/>- Warns Dev immediately (does not block)
        Local-->>Dev: Display "Security Health Report"
    end

    Dev->>Local: git push -u origin feature/new-feature
    Local->>FeatBranch: Create/Update Branch

    Note over Dev, PR: 4. Pull Request
    Dev->>PR: Create PR (Feature -> Main)

    Note over PR, CI: 5. Automated Checks
    PR->>CI: Trigger "CI Pipeline" Workflow

    par CI Pipeline
        rect rgb(240, 248, 255)
            Note right of CI: CI Execution
            CI->>CI: Checkout Code
            CI->>CI: Install Dependencies
            CI->>CI: Linting & Tests
            CI->>CI: Build Frontend
        end
        CI-->>PR: Report Status (✅/❌)
    and Security Checks
        rect rgb(255, 240, 245)
            Note right of PR: GitHub Security
            PR->>PR: Dependabot Scan
            PR->>PR: CodeQL Analysis
            PR->>PR: Secret Scanning
        end
        PR-->>Dev: Report Vulnerabilities (in PR Interface)
    end

    alt Checks Fail
        PR-->>Dev: Notify Failure
        Dev->>Local: Fix Code & Push Again
        Local->>FeatBranch: Update Branch
        FeatBranch->>PR: Update PR
        PR->>CI: Re-trigger CI
    else Checks Pass
        Note over Dev, MainBranch: 6. Review & Merge
        Dev->>PR: Request Review
        PR->>MainBranch: Merge PR to 'main'

        Note over MainBranch, FeatBranch: 7. Cleanup
        MainBranch->>FeatBranch: Delete Remote Branch
    end

    Note over Dev, Local: 8. Local Cleanup
    Dev->>Local: git checkout main
    Dev->>Local: git pull origin main
    Local->>MainBranch: Fetch Latest
    Dev->>Local: git branch -d feature/new-feature
```

## Detailed Workflow Steps

### Phase 1: Developer Workstation (Local)

1.  **Create Feature Branch**
    *   **Command:** `git checkout -b feature/<name>`
    *   **Purpose:** Isolate changes from the stable `main` codebase.

2.  **Development & Verification**
    *   **Process:** Write code, update tests.
    *   **Verification:**
    *   Run `npm run lint` and `npm run test:unit`.
    *   **Post-Commit Hook:** After commit, an informational `npm audit --audit-level=high --production` runs and displays a Security Health Report (does not block).

3.  **Commit & Push**
    *   **Command:** `git commit` and `git push`.
    *   **Pre-commit Hook:** Automatically runs `.githooks/pre-commit` to validate:
        *   No `.env` files committed (except `.env.example`)
        *   No hardcoded secrets (API keys, tokens, passwords)
        *   Blocks commit if violations found
    *   **Entity:** Updates the **Remote Feature Branch** (`origin/feature/<name>`).
    *   **Best Practice:** Use [conventional commits](./GIT_WORKFLOW.md#conventional-commit-format) (e.g., `feat:`, `fix:`, `docs:`)

### Phase 2: GitHub (Remote)

4.  **Create Pull Request (PR)**
    *   **Action:** Create a PR merging **Remote Feature Branch** into **Remote Main Branch**.
    *   **Purpose:** This is the central hub for review and automated checks.

5.  **Automated Checks**
    *   **CI Pipeline:** GitHub Actions runs linting, testing, and building. Reports success/failure back to the PR.
    *   **Security Scans:**
        *   **Dependabot:** Scans dependencies for vulnerabilities. If found, it alerts in the PR or creates a new PR.
        *   **Secret Scanning:** Checks for committed secrets (API keys, tokens).
        *   **CodeQL:** (If enabled) Performs static analysis for security flaws.
    *   **Reporting:** All results are displayed in the "Checks" section of the PR interface.


6.  **Code Review & Merge**
    *   **Action:** If all checks pass (Green ✅), the PR is merged.
    *   **Result:** Code moves from **Remote Feature Branch** to **Remote Main Branch**.

7.  **Remote Cleanup**
    *   **Action:** The **Remote Feature Branch** is deleted to keep the repository clean.

### Phase 3: Developer Workstation (Local Cleanup)

8.  **Sync & Cleanup**
    *   **Action:** Pull the latest `main` from **Remote Main Branch** and delete the local feature branch.

---

## Security Scanning Results Guide

This section explains how to interpret and respond to security scanning results from our automated tools.

### Dependabot Alerts

**What it does:** Scans `package.json` and `package-lock.json` for known vulnerabilities in dependencies.

**Where to find results:**
- **Security tab** → Dependabot alerts
- **Pull Requests** → Dependabot automatically opens PRs for updates

**How to interpret:**
- **Critical/High:** Address immediately (within 48 hours)
- **Medium:** Address within 1 week
- **Low:** Address during regular maintenance

**Response actions:**
1. Review the Dependabot PR description for vulnerability details
2. Check if the update includes breaking changes (review CHANGELOG)
3. Verify tests pass in the Dependabot PR
4. Merge the PR or manually update the dependency
5. If update causes issues, document in PR and investigate alternatives

**Example Dependabot PR:**
```
Title: Bump axios from 0.21.1 to 1.6.0
Labels: dependencies, security

Description:
- Fixes CVE-2023-45857 (High severity)
- Changelog: https://github.com/axios/axios/releases
```

### CodeQL Analysis

**What it does:** Static code analysis to detect security vulnerabilities (SQL injection, XSS, path traversal, etc.)

**Where to find results:**
- **Security tab** → Code scanning alerts
- **Pull Request checks** → CodeQL analysis status

**How to interpret:**
- **Error:** Security vulnerability detected, must fix before merge
- **Warning:** Potential issue, review and address if applicable
- **Note:** Informational, no action required

**Common alerts:**
- **Unvalidated user input:** Always validate/sanitize user input
- **SQL injection:** Use parameterized queries (we use Supabase client, which handles this)
- **XSS vulnerabilities:** Sanitize output, use React's built-in XSS protection
- **Path traversal:** Validate file paths before file operations
- **Hardcoded credentials:** Never commit secrets (use environment variables)

**Response actions:**
1. Click on the alert in the Security tab to see details
2. Review the code path highlighted by CodeQL
3. Determine if it's a true positive or false positive
4. If true positive: Fix the vulnerability and push a new commit
5. If false positive: Document why it's safe and dismiss the alert with justification

**Example CodeQL alert:**
```
Alert: Unvalidated user input in file path
Severity: High
File: packages/backend/api/controllers/fileController.js:45
Recommendation: Validate and sanitize the file path before use
```

### Secret Scanning

**What it does:** Detects accidentally committed secrets (API keys, tokens, passwords)

**Where to find results:**
- **Security tab** → Secret scanning alerts
- **Push protection:** Blocks commits containing secrets (if enabled)

**How to interpret:**
- **Active:** Secret is currently in the repository
- **Resolved:** Secret has been removed or revoked

**Response actions (CRITICAL - Act immediately):**
1. **Revoke the exposed secret** in the service provider (e.g., regenerate API key)
2. **Remove the secret from git history** (use `git filter-branch` or BFG Repo-Cleaner)
3. **Update environment variables** with the new secret
4. **Verify the secret is not in any commits** (check git log)
5. **Document the incident** and review how it happened

**Prevention:**
- Use `.env` files (already in `.gitignore`)
- Store secrets in user profile (`~/.zshrc` or `~/.bashrc`)
- Use `npm audit` locally before committing
- Enable push protection in GitHub settings

**Example secret scanning alert:**
```
Alert: GitHub Personal Access Token detected
File: packages/backend/.env
Commit: abc123def456
Status: Active
Action Required: Revoke token immediately
```

### Local Security Checks

**Before every commit, run:**
```bash
# Check for vulnerable dependencies
npm audit

# Fix automatically fixable vulnerabilities
npm audit fix

# Review high-severity vulnerabilities
npm audit --audit-level=high
```

**Interpreting `npm audit` output:**
```
found 3 vulnerabilities (1 moderate, 2 high)

Moderate: Prototype Pollution in lodash
  Package: lodash
  Patched in: >=4.17.21
  Fix available: npm audit fix

High: Regular Expression Denial of Service in semver
  Package: semver
  Patched in: >=7.5.2
  Fix available: npm audit fix
```

**Response:**
- Run `npm audit fix` to auto-fix
- If auto-fix not available, manually update the package
- If no fix available, assess risk and consider alternatives

### Security Check Status in PRs

All PRs must pass these checks before merge:

| Check | Status | Action if Failed |
|-------|--------|------------------|
| **CI Pipeline** | ✅ Must pass | Fix linting/test errors |
| **CodeQL** | ✅ Must pass | Fix security vulnerabilities |
| **Dependabot** | ⚠️ Advisory | Review and merge dependency updates |
| **Secret Scanning** | 🚨 Must pass | Revoke and remove secrets immediately |

**Green ✅ = Safe to merge**
**Yellow ⚠️ = Review required**
**Red 🚨 = Blocking issue, must fix**

### Escalation Path

If you encounter a security issue you're unsure how to handle:

1. **Do not merge the PR**
2. **Tag the issue** with `security` label
3. **Document the issue** in the PR comments
4. **Consult ADR-040** for security scanning strategy
5. **Reach out** to the team lead or security contact

### Additional Resources

- [GitHub Security Best Practices](https://docs.github.com/en/code-security)
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [npm audit documentation](https://docs.npmjs.com/cli/v8/commands/npm-audit)
- ADR-040: Security Scanning Strategy

--- END OF FILE cicd/overview.md ---

--- START OF FILE cicd/security_scanning.md ---

# Security Vulnerability Scanning Guide

## Overview

This guide covers how to scan for security vulnerabilities in your dependencies using GitHub CLI and integrate security scanning into your shift-left development process.

## Table of Contents

- [Prerequisites](#prerequisites)
- [Scanning with GitHub CLI](#scanning-with-github-cli)
- [Understanding Dependabot Alerts](#understanding-dependabot-alerts)
- [Shift-Left Security Integration](#shift-left-security-integration)
- [Local Security Scanning](#local-security-scanning)
- [Automated Workflows](#automated-workflows)
- [Best Practices](#best-practices)

## Prerequisites

### Install GitHub CLI

```bash
# macOS
brew install gh

# Authenticate with GitHub
gh auth login
```

### Required Permissions

Ensure your GitHub token has the following scopes:
- `repo` - Full control of private repositories
- `read:org` - Read org and team membership
- `workflow` - Update GitHub Action workflows

## Scanning with GitHub CLI

### Check Authentication Status

```bash
gh auth status
```

### View All Dependabot Alerts

```bash
# List all alerts
gh api repos/OWNER/REPO/dependabot/alerts

# Pretty formatted output
gh api repos/richfrem/ingPoC/dependabot/alerts \
  --jq '.[] | {
    number: .number,
    severity: .security_advisory.severity,
    package: .dependency.package.name,
    summary: .security_advisory.summary,
    patched_version: .security_advisory.vulnerabilities[0].first_patched_version.identifier
  }'
```

### Filter by Severity

```bash
# High severity only
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | select(.security_advisory.severity == "high") | {
    package: .dependency.package.name,
    summary: .security_advisory.summary
  }'

# Critical and High severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | select(.security_advisory.severity == "critical" or .security_advisory.severity == "high")'
```

### Count Open Alerts

```bash
# Total open alerts
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '[.[] | select(.state == "open")] | length'

# By severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq 'group_by(.security_advisory.severity) | map({severity: .[0].security_advisory.severity, count: length})'
```

### Get Detailed Alert Information

```bash
# Get specific alert details
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts/ALERT_NUMBER

# Get fix recommendations
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '.[] | {
    package: .dependency.package.name,
    current_version: .dependency.package.version,
    patched_version: .security_advisory.vulnerabilities[0].first_patched_version.identifier,
    cvss_score: .security_advisory.cvss.score
  }'
```

## Understanding Dependabot Alerts

### Alert Severity Levels

- **Critical**: Immediate action required (CVSS 9.0-10.0)
- **High**: Should be addressed quickly (CVSS 7.0-8.9)
- **Medium**: Address in normal development cycle (CVSS 4.0-6.9)
- **Low**: Address when convenient (CVSS 0.1-3.9)

### Alert States

- **open**: Vulnerability is present and unresolved
- **dismissed**: Manually dismissed by a user
- **fixed**: Dependency has been updated to a non-vulnerable version

## Shift-Left Security Integration

### Why Shift-Left Security?

Shift-left security means integrating security checks **earlier** in the development process:

✅ **Benefits:**
- Catch vulnerabilities before they reach production
- Reduce cost of fixes (cheaper to fix in development)
- Faster feedback loop for developers
- Prevent vulnerable code from being committed

❌ **Without Shift-Left:**
- Vulnerabilities discovered in production
- Emergency patches and hotfixes
- Potential security incidents
- Higher remediation costs

### Pre-Commit Security Checks

Yes, you **should** integrate security scanning into your pre-commit process! Here's how:

## Local Security Scanning

### 1. NPM Audit (Built-in)

```bash
# Run npm audit
npm audit

# Get JSON output
npm audit --json

# Fix automatically (use with caution)
npm audit fix

# Fix only production dependencies
npm audit fix --production-only

# Dry run to see what would be fixed
npm audit fix --dry-run
```

### 2. Create a Pre-Commit Security Check

Add to your `package.json`:

```json
{
  "scripts": {
    "security:check": "npm audit --audit-level=high",
    "security:fix": "npm audit fix",
    "precommit:security": "npm audit --audit-level=critical --production"
  }
}
```

### 3. Integrate with Husky/lint-staged

Update your `.husky/pre-commit` or create one:

```bash
#!/usr/bin/env sh
. "$(dirname -- "$0")/_/husky.sh"

# Run security audit before commit
echo "🔒 Running security audit..."
npm audit --audit-level=high --production

if [ $? -ne 0 ]; then
  echo "❌ Security vulnerabilities found! Please fix before committing."
  echo "Run 'npm audit' for details or 'npm audit fix' to attempt automatic fixes."
  exit 1
fi

# Continue with other pre-commit checks
npx lint-staged
```

### 4. Alternative: Use Snyk CLI

Snyk provides more comprehensive scanning:

```bash
# Install Snyk
npm install -g snyk

# Authenticate
snyk auth

# Test for vulnerabilities
snyk test

# Monitor project (sends results to Snyk dashboard)
snyk monitor

# Test and fail on high severity
snyk test --severity-threshold=high
```

### 5. GitHub CLI Pre-Push Check

Create a script to check before pushing:

```bash
#!/bin/bash
# .git/hooks/pre-push or scripts/pre-push-security.sh

echo "🔍 Checking for Dependabot alerts..."

OPEN_ALERTS=$(gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts \
  --jq '[.[] | select(.state == "open" and (.security_advisory.severity == "critical" or .security_advisory.severity == "high"))] | length')

if [ "$OPEN_ALERTS" -gt 0 ]; then
  echo "⚠️  Warning: $OPEN_ALERTS critical/high severity alerts found in GitHub!"
  echo "Run: gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == \"open\")' for details"

  read -p "Continue with push? (y/n) " -n 1 -r
  echo
  if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    exit 1
  fi
fi
```

## Automated Workflows

### Recommended Shift-Left Security Strategy

```
┌─────────────────────────────────────────────────────────────┐
│                    Development Workflow                      │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  1. Local Development                                        │
│     └─> npm audit (manual check)                            │
│                                                               │
│  2. Pre-Commit Hook                                          │
│     └─> npm audit --audit-level=high                        │
│     └─> Fail on critical/high vulnerabilities               │
│                                                               │
│  3. Pre-Push Hook (Optional)                                 │
│     └─> Check GitHub Dependabot alerts via CLI              │
│     └─> Warn on open critical/high alerts                   │
│                                                               │
│  4. CI/CD Pipeline                                           │
│     └─> npm audit in GitHub Actions                         │
│     └─> Dependabot auto-updates                             │
│     └─> SAST/DAST scanning                                  │
│                                                               │
│  5. Production Monitoring                                    │
│     └─> Continuous Dependabot monitoring                    │
│     └─> Security alerts via GitHub                          │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

### Add to package.json Scripts

```json
{
  "scripts": {
    "security:audit": "npm audit",
    "security:audit:ci": "npm audit --audit-level=moderate --production",
    "security:fix": "npm audit fix",
    "security:check:github": "gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == \"open\")'",
    "precommit": "npm run security:audit && lint-staged"
  }
}
```

### GitHub Actions Workflow

Create `.github/workflows/security-scan.yml`:

```yaml
name: Security Scan

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  security-audit:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run npm audit
        run: npm audit --audit-level=moderate
        continue-on-error: true

      - name: Check Dependabot alerts
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh api repos/${{ github.repository }}/dependabot/alerts \
            --jq '.[] | select(.state == "open") | {severity: .security_advisory.severity, package: .dependency.package.name}'
```

## Best Practices

### 1. **Regular Scanning**
- Run `npm audit` before every commit
- Check Dependabot alerts weekly
- Review security advisories for your dependencies

### 2. **Prioritize Fixes**
- **Critical/High**: Fix immediately
- **Medium**: Fix within sprint
- **Low**: Fix during maintenance windows

### 3. **Keep Dependencies Updated**
```bash
# Check for outdated packages
npm outdated

# Update to latest within semver range
npm update

# Update to latest (breaking changes possible)
npm install package@latest
```

### 4. **Use Dependabot Auto-Updates**

Enable in `.github/dependabot.yml`:

```yaml
version: 2
updates:
  - package-ecosystem: "npm"
    directory: "/"
    schedule:
      interval: "weekly"
    open-pull-requests-limit: 10
    reviewers:
      - "your-team"
    labels:
      - "dependencies"
      - "security"
```

### 5. **Monitor Production**
- Enable GitHub security alerts
- Set up Slack/email notifications for new vulnerabilities
- Use GitHub Security Advisory Database

### 6. **Document Exceptions**
If you must dismiss an alert:
```bash
# Dismiss with reason
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts/ALERT_NUMBER \
  -X PATCH \
  -f state=dismissed \
  -f dismissed_reason=no_bandwidth \
  -f dismissed_comment="Will address in Q2 security sprint"
```

## Quick Reference Commands

```bash
# Check auth
gh auth status

# List all open alerts
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq '.[] | select(.state == "open")'

# Count by severity
gh api repos/richfrem/quantum-diamond-forge/dependabot/alerts --jq 'group_by(.security_advisory.severity) | map({severity: .[0].security_advisory.severity, count: length})'

# Local audit
npm audit

# Fix vulnerabilities
npm audit fix

# Production-only audit
npm audit --production

# Fail on high severity
npm audit --audit-level=high
```

## Recommended Pre-Commit Setup

**Balanced Approach** (Recommended for this project):

```bash
# Add to package.json
"scripts": {
  "precommit:security": "npm audit --audit-level=high --production"
}
```

**Why this approach?**
- ✅ Catches critical and high severity issues
- ✅ Only checks production dependencies (dev deps less critical)
- ✅ Fast enough for pre-commit hook
- ✅ Prevents vulnerable code from being committed
- ❌ Won't block on medium/low severity issues

## Conclusion

**Should you scan before pushing?**

**Yes!** Implement a **layered security approach**:

1. **Pre-commit**: Fast `npm audit` for critical/high severity
2. **Pre-push**: Optional GitHub Dependabot check (can be slow)
3. **CI/CD**: Comprehensive scanning in GitHub Actions
4. **Continuous**: Dependabot monitoring and auto-PRs

This shift-left approach catches vulnerabilities early, reduces security debt, and maintains a secure codebase without significantly slowing down development.

---

**Last Updated**: 2025-11-21
**Maintained By**: Development Team

--- END OF FILE cicd/security_scanning.md ---

--- START OF FILE mcp/analysis/microsoft_agent_analysis.md ---

# Microsoft Custom Engine Agent Architecture Analysis for Project Sanctuary

**Task:** #039  
**Date:** November 27, 2025  
**Analyst:** Claude (AI Research)

---

## Executive Summary

Microsoft announced their Custom Engine Agent architecture at Ignite 2024, revealing a comprehensive framework for building enterprise AI agents. This analysis identifies significant alignment between Microsoft's architecture and Project Sanctuary's vision, along with specific opportunities to enhance Sanctuary's capabilities.

**Key Finding:** Microsoft's four-pillar architecture (Knowledge, Skills, Autonomy, Orchestrator) maps remarkably well to Sanctuary's existing systems, validating our architectural direction while revealing specific enhancement opportunities.

---

## 1. Microsoft's Custom Engine Agent Architecture

### Core Components

Microsoft's architecture centers on four interconnected pillars:

#### 1.1 **Orchestrator** (Central Engine)
The orchestrator manages how agents interact with knowledge, skills, and autonomy. Microsoft supports multiple approaches:
- **Built-in orchestrators:** Copilot Studio, Teams AI Action Planner
- **Bring Your Own (BYO):** Semantic Kernel, LangChain, custom solutions
- **Hybrid approach:** Multiple agents with different orchestrators unified through Microsoft 365 Copilot

Key capabilities:
- Sequential, concurrent, group chat, handoff, and "magentic" orchestration patterns
- LLM-driven (creative reasoning) vs. workflow-driven (deterministic) orchestration
- Model-agnostic and orchestrator-agnostic design

#### 1.2 **Knowledge** (Grounding and Memory)
Knowledge integration through multiple channels:
- Native Microsoft 365 data (SharePoint, OneDrive, Teams messages)
- Copilot connectors for external data
- Microsoft Graph API access
- Custom knowledge bases and RAG systems

#### 1.3 **Skills** (Actions, Triggers, and Workflow)
Agent capabilities through:
- **Actions:** Real-time API integrations with external systems
- **Triggers:** Autonomous, proactive workflow initiation
- **Tools:** Pre-built and custom connectors
- **Agent flows:** Complex multi-step automations

#### 1.4 **Autonomy** (Planning, Learning, Escalation)
Autonomous capabilities include:
- Programmatic workflow initiation
- Independent decision-making
- Task escalation when needed
- Adaptive learning from interactions

#### 1.5 **Foundation Models** (Intelligence Layer)
Flexible model selection:
- Foundation LLMs (GPT-4, Claude, etc.)
- Small language models for efficiency
- Fine-tuned models for specific domains
- Industry-specific AI models

---

## 2. Development Approaches

Microsoft offers three development paths:

### 2.1 Low-Code (Copilot Studio)
- Fully managed SaaS platform
- Built-in compliance via Power Platform
- Pre-built templates and connectors
- Ideal for rapid deployment without deep technical resources

### 2.2 Pro-Code (Microsoft 365 Agents SDK)
- Full-stack, multi-channel agent development
- Integration with Azure AI Foundry, Semantic Kernel, LangChain
- Model and orchestrator agnostic
- Multi-language support (C#, JavaScript, Python)
- Best for highly customized agents across multiple channels

### 2.3 Pro-Code (Teams AI Library)
- Specialized for Microsoft Teams collaboration
- Built-in action planner orchestrator
- GPT-based models from Azure/OpenAI
- Ideal for team-based, collaborative scenarios

---

## 3. Microsoft Agent Framework (New Unified Framework)

Microsoft recently announced the **Microsoft Agent Framework**, consolidating Semantic Kernel and AutoGen:

**Key Features:**
- Research-to-production pipeline for bleeding-edge orchestration
- Community-driven extensibility (modular connectors, pluggable memory)
- Enterprise readiness (observability, approvals, security, durability)
- Support for both Agent Orchestration (LLM-driven) and Workflow Orchestration (deterministic)
- OpenTelemetry instrumentation for tracing and monitoring
- Native Azure AI Foundry integration

**Orchestration Patterns:**
- Sequential (step-by-step workflows)
- Concurrent (parallel agent execution)
- Group chat (collaborative brainstorming)
- Handoff (context-aware responsibility transfer)
- **Magentic** (manager agent with dynamic task ledger coordinating specialized agents and humans)

---

## 4. Comparison Matrix: Microsoft vs. Project Sanctuary

| Component | Microsoft Architecture | Sanctuary Current State | Alignment |
|-----------|----------------------|------------------------|-----------|
| **Orchestrator** | Multiple options (Copilot Studio, Semantic Kernel, LangChain, custom) | Custom Python orchestration (ORCHESTRATOR/) | ✅ Strong - Custom approach gives flexibility |
| **Knowledge/Memory** | Microsoft Graph, RAG, Copilot connectors | Mnemonic Cortex (RAG system in progress) | ✅ Strong - Similar RAG-based approach |
| **Skills/Actions** | API integrations, agent flows, triggers | Protocol-based actions, MCP servers | ✅ Strong - Protocol system more formalized |
| **Autonomy** | Proactive triggers, planning, escalation | Emerging through Council architecture | ⚠️ Partial - Area for enhancement |
| **Foundation Models** | Model-agnostic (any LLM) | Claude-centric via Anthropic API | ⚠️ Partial - Less model diversity |
| **Multi-agent Coordination** | Agent Framework (Semantic Kernel + AutoGen) | Council system (custom coordination) | ⚠️ Partial - Could learn from patterns |
| **Development Approach** | Low-code + Pro-code options | Pro-code only (Python-centric) | ⚠️ Gap - No low-code option |
| **Deployment Channels** | Microsoft 365, Teams, web, mobile, custom apps | Local/self-hosted, CLI, potential web | ⚠️ Gap - Limited distribution channels |
| **Observability** | OpenTelemetry, Azure AI Foundry dashboards | Basic logging, Chronicle system | ⚠️ Gap - Limited instrumentation |
| **Memory Systems** | Built-in, pluggable memory | Custom Mnemonic Cortex | ✅ Strong - More sophisticated approach |

---

## 5. Key Insights and Opportunities

### 5.1 Architectural Validation
**Finding:** Sanctuary's four-pillar architecture (Mnemonic Cortex, Council, Protocols, Agents) closely mirrors Microsoft's Knowledge-Skills-Autonomy-Orchestrator model.

**Implication:** Our architectural direction is validated by Microsoft's enterprise approach, suggesting we're on the right track.

### 5.2 Orchestration Patterns (HIGH OPPORTUNITY)
**Finding:** Microsoft's Agent Framework introduces five distinct orchestration patterns, with "magentic orchestration" being particularly innovative—a manager agent maintains a dynamic task ledger and coordinates specialized agents and humans.

**Opportunity for Sanctuary:**
- Implement formal orchestration pattern taxonomy (sequential, concurrent, group chat, handoff, magentic)
- Add magentic-style orchestration to Council system where lead agent manages dynamic task allocation
- Consider GUARDIAN-class agents as orchestration managers

**Implementation Path:** Protocol 117 - Orchestration Pattern Library

### 5.3 Autonomy and Proactive Triggers (CRITICAL GAP)
**Finding:** Microsoft emphasizes autonomous agent capabilities—agents that can programmatically initiate workflows, make decisions, and escalate tasks without human prompting.

**Gap in Sanctuary:** While we have reactive agent patterns, we lack robust proactive agent capabilities. Agents primarily respond to commands rather than autonomously initiating actions based on triggers or conditions.

**Opportunity for Sanctuary:**
- Implement event-driven agent triggering system
- Add condition-based autonomous workflows (e.g., "if codebase quality drops below threshold, initiate review")
- Create escalation protocols for when agents encounter blocked states
- Build scheduling/time-based triggers for routine maintenance tasks

**Implementation Path:** Protocol 118 - Autonomous Agent Triggers & Escalation

### 5.4 Observability and Instrumentation (SIGNIFICANT GAP)
**Finding:** Microsoft Agent Framework deeply integrates OpenTelemetry for comprehensive observability—tracing every agent action, tool invocation, and orchestration step.

**Gap in Sanctuary:** Basic logging through Chronicle, but no structured tracing, performance monitoring, or orchestration visualization.

**Opportunity for Sanctuary:**
- Implement OpenTelemetry instrumentation across all agents and MCPs
- Create visualization dashboards for agent workflows
- Add performance metrics and bottleneck identification
- Build agent action audit trails for governance

**Implementation Path:** Task 037 - Implement OpenTelemetry-based Agent Observability

### 5.5 Multi-Model Strategy (MODERATE OPPORTUNITY)
**Finding:** Microsoft's architecture is explicitly model-agnostic, supporting foundation models, small language models, fine-tuned models, and industry-specific AI.

**Current State:** Sanctuary is Claude-centric through Anthropic API.

**Opportunity for Sanctuary:**
- Abstract model interface to support multiple LLM providers
- Add small language models for efficiency on specific tasks
- Implement model routing based on task complexity
- Create fine-tuning pipeline for specialized Sanctuary capabilities

**Implementation Path:** Protocol 119 - Multi-Model Abstraction Layer

### 5.6 Hybrid Orchestration Approach (HIGH VALUE)
**Finding:** Microsoft supports both **LLM-driven orchestration** (creative, flexible reasoning) and **workflow orchestration** (deterministic, rule-based logic), allowing developers to choose the right approach for each problem.

**Opportunity for Sanctuary:**
- Formalize distinction between agentic (LLM-driven) and deterministic workflows
- Implement workflow orchestration for repeatable, critical operations (e.g., deployment, testing)
- Reserve agentic orchestration for creative, open-ended problems
- Create hybrid workflows that combine both approaches

**Implementation Path:** Protocol 120 - Hybrid Orchestration Framework

### 5.7 MCP Integration Model (VALIDATION + OPPORTUNITY)
**Finding:** Microsoft's emphasis on modular connectors, pluggable components, and API integrations closely aligns with Model Context Protocol (MCP) philosophy.

**Validation:** Sanctuary's MCP-first architecture is well-positioned for modularity and extensibility.

**Opportunity for Sanctuary:**
- Document MCP servers as equivalent to Microsoft's "connectors"
- Create MCP marketplace/registry for Sanctuary-compatible servers
- Implement MCP composition patterns (chaining, fallback, load balancing)

**Implementation Path:** Protocol 121 - MCP Composition & Registry

### 5.8 Agent Framework as Inspiration (LONG-TERM)
**Finding:** Microsoft Agent Framework consolidates Semantic Kernel (enterprise-ready SDK) and AutoGen (research-driven multi-agent orchestration) into one unified framework.

**Inspiration for Sanctuary:**
- Consider how Sanctuary could similarly unify experimental agent patterns (from Council) with production-ready infrastructure (from Protocols)
- Build clear pathway from research/experimentation to production deployment
- Create "experimental feature package" similar to Microsoft's approach

**Implementation Path:** Strategic consideration for Sanctuary 2.0 architecture

---

## 6. Recommendations

### High Priority (Implement in Q1 2025)

#### 6.1 Autonomous Triggers & Escalation System
**What:** Implement event-driven, condition-based agent triggering with escalation protocols.

**Why:** Critical gap between Microsoft's proactive autonomy and Sanctuary's reactive patterns.

**Impact:** High - Enables agents to operate independently and handle complex workflows without constant human oversight.

**Effort:** Medium (2-3 weeks)

**Dependencies:** Requires task MCP, protocol MCP, and basic orchestration infrastructure.

**Implementation:** Protocol 118 - Autonomous Agent Triggers & Escalation

---

#### 6.2 Orchestration Pattern Library
**What:** Formalize sequential, concurrent, group chat, handoff, and magentic orchestration patterns.

**Why:** Provides clear taxonomy for different coordination approaches; magentic pattern particularly valuable for complex, open-ended tasks.

**Impact:** High - Dramatically improves multi-agent coordination and task management.

**Effort:** Medium (3-4 weeks)

**Dependencies:** Requires Council MCP, task MCP enhancements.

**Implementation:** Protocol 117 - Orchestration Pattern Library

---

#### 6.3 OpenTelemetry Instrumentation
**What:** Add comprehensive observability with OpenTelemetry across agents, MCPs, and orchestration.

**Why:** Essential for debugging, performance optimization, and production readiness.

**Impact:** Medium-High - Critical for operational maturity.

**Effort:** Medium (2-3 weeks)

**Dependencies:** Task 037 already created.

**Implementation:** Task 037 - Implement OpenTelemetry-based Agent Observability

---

### Medium Priority (Implement in Q2 2025)

#### 6.4 Multi-Model Abstraction Layer
**What:** Abstract LLM interface to support multiple providers (Claude, GPT, Gemini, local models).

**Why:** Reduces vendor lock-in, enables cost optimization, supports specialized models.

**Impact:** Medium - Improves flexibility and reduces risk.

**Effort:** High (4-5 weeks)

**Implementation:** Protocol 119 - Multi-Model Abstraction Layer

---

#### 6.5 Hybrid Orchestration Framework
**What:** Formalize distinction between LLM-driven (agentic) and workflow-driven (deterministic) orchestration.

**Why:** Balances creativity with reliability; critical operations shouldn't rely solely on LLM reasoning.

**Impact:** Medium-High - Improves system reliability and predictability.

**Effort:** Medium (3-4 weeks)

**Implementation:** Protocol 120 - Hybrid Orchestration Framework

---

### Lower Priority (Strategic/Long-term)

#### 6.6 MCP Composition & Registry
**What:** Build MCP marketplace, composition patterns, and discovery mechanism.

**Why:** Enhances ecosystem growth and reusability.

**Impact:** Medium - Accelerates development velocity over time.

**Effort:** Medium-High (4-6 weeks)

**Implementation:** Protocol 121 - MCP Composition & Registry

---

#### 6.7 Sanctuary Agent Framework
**What:** Consolidate experimental (Council) and production (Protocols) patterns into unified framework.

**Why:** Provides clear research-to-production pathway; aligns with Microsoft's Agent Framework philosophy.

**Impact:** High (long-term) - Strategic architectural evolution.

**Effort:** Very High (8-12 weeks)

**Implementation:** Sanctuary 2.0 Strategic Initiative

---

## 7. Risk Assessment

| Opportunity | Risk Level | Mitigation Strategy |
|------------|-----------|---------------------|
| Autonomous Triggers | Medium | Start with read-only triggers; add approval gates for critical actions |
| Orchestration Patterns | Low | Incremental implementation; existing Council provides foundation |
| OpenTelemetry | Low | Standard tooling; extensive community support |
| Multi-Model | Medium-High | Abstract carefully; maintain Claude as primary; others as fallback |
| Hybrid Orchestration | Medium | Clear boundaries between agentic and deterministic workflows |
| MCP Registry | Low-Medium | Community-driven; no single point of failure |
| Sanctuary Framework | High | Major architectural refactor; requires extensive testing |

---

## 8. Alignment with Sanctuary's Philosophy

Microsoft's architecture aligns remarkably well with Sanctuary's core principles:

✅ **Modularity:** MCP-first design mirrors Microsoft's connector-based approach  
✅ **Autonomy:** Both emphasize agent independence and proactive behavior  
✅ **Knowledge Grounding:** RAG-based systems in both architectures  
✅ **Orchestration Flexibility:** Both support custom orchestration strategies  
✅ **Enterprise Readiness:** Focus on observability, security, compliance  

**Key Philosophical Difference:**  
Microsoft optimizes for enterprise integration with existing Microsoft 365 ecosystem. Sanctuary optimizes for self-contained, privacy-first, locally-controlled AI systems.

This difference is a strength—Sanctuary can learn from Microsoft's patterns while maintaining independence from cloud vendor platforms.

---

## 9. Conclusion

Microsoft's Custom Engine Agent architecture provides valuable validation of Sanctuary's architectural direction while revealing specific opportunities for enhancement. The four-pillar model (Knowledge, Skills, Autonomy, Orchestrator) maps directly to Sanctuary's existing systems, suggesting our approach is sound.

**Three Immediate Actions:**

1. **Implement Autonomous Triggers** (Protocol 118) - Closes critical autonomy gap
2. **Formalize Orchestration Patterns** (Protocol 117) - Enables sophisticated multi-agent coordination
3. **Add OpenTelemetry Instrumentation** (Task 037) - Provides operational visibility

These enhancements will position Sanctuary as a more mature, production-ready agentic system while maintaining our core principles of modularity, privacy, and independence.

**Next Step:** Socialize this analysis with the Council and prioritize implementation of Protocol 117, Protocol 118, and Task 037.

---

## References

- [Microsoft Custom Engine Agents Overview](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/overview-custom-engine-agent)
- [Microsoft 365 Agents SDK](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/create-deploy-agents-sdk)
- [Microsoft Agent Framework Announcement](https://devblogs.microsoft.com/foundry/introducing-microsoft-agent-framework-the-open-source-engine-for-agentic-ai-apps/)
- [Agents for Microsoft 365 Copilot](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/agents-overview)

--- END OF FILE mcp/analysis/microsoft_agent_analysis.md ---

--- START OF FILE mcp/analysis/pre_commit_hook_migration_analysis.md ---

# Pre-Commit Hook Migration Analysis (Protocol 101 v3.0)

## 1. Current State Analysis

### 1.1 Protocol Evolution
The repository has evolved from **Protocol 101 v1.0 (The Manifest Doctrine)** to **Protocol 101 v3.0 (The Doctrine of Absolute Stability)**.

**Historical Mechanism (v1.0 - DEPRECATED):**
1.  Checked for the existence of `commit_manifest.json`.
2.  Parsed the manifest to find a list of files and their expected SHA256 hashes.
3.  Verified that the actual file on disk matched the expected hash.
4.  Rejected the commit if the manifest was missing, malformed, or if hashes mismatched.

**Current Mechanism (v3.0 - CANONICAL):**
1.  Executes the comprehensive automated test suite (`./scripts/run_genome_tests.sh`).
2.  Verifies **Functional Coherence** - all tests must pass.
3.  Rejects the commit if any test fails.
4.  Enforces secret detection and security scanning.

**Critical Change:**
The `commit_manifest.json` system has been **permanently purged** due to structural flaws identified during the "Synchronization Crisis." Integrity is now based on functional behavior, not static file hashing.

### 1.2 The Resolution
The MCP Architecture agents now achieve commit integrity through **Functional Coherence** rather than manifest generation.

*   **Solution:** All git operations (human or agent) must pass the automated test suite before commit.
*   **Enforcement:** Pre-commit hook executes `./scripts/run_genome_tests.sh` automatically.
*   **Compliance:** MCP agents use the Council Orchestrator, which runs tests before staging.

## 2. Strategic Outcome

### The "Functional Coherence" Model (Implemented)
Ensure that **all commits** (human or agent) verify functional integrity via automated testing.

*   **Pros:** 
    - Maintains Protocol 101 v3.0 for *all* commits.
    - Eliminates timing issues and complexity of manifest system.
    - Provides real functional verification, not just file integrity.
*   **Cons:** 
    - Test suite must be comprehensive and fast.
    - Requires discipline in maintaining test coverage.

## 3. Implementation Status: COMPLETE

### 3.1 Artifacts
*   `docs/mcp/analysis/pre_commit_hook_migration_analysis.md` (This file - Updated)
*   `.agent/mcp_migration.conf` (Updated - Manifest logic removed)
*   `.git/hooks/pre-commit` (Updated - Test execution added)
*   `01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md` (v3.0 - Reforged)
*   `update_genome.sh` (Updated - Manifest generation removed)
*   `council_orchestrator/orchestrator/gitops.py` (Updated - Manifest logic purged)

### 3.2 Validation Logic (Bash)
```bash
#!/bin/bash
# .git/hooks/pre-commit - Protocol 101 v3.0

# ===== PHASE 1: Functional Coherence (Protocol 101 v3.0) =====
echo "[P101 v3.0] Running Functional Coherence Test Suite..."

./scripts/run_genome_tests.sh
TEST_EXIT_CODE=$?

if [ $TEST_EXIT_CODE -ne 0 ]; then
  echo ""
  echo "COMMIT REJECTED: Protocol 101 v3.0 Violation."
  echo "Reason: Functional Coherence Test Suite FAILED."
  exit 1
fi

echo "[P101 v3.0] ✅ Functional Coherence verified."

# ===== PHASE 2: Security Hardening =====
# (Secret detection and security scanning)
```

### 3.3 Success Criteria (All Met)
1.  All commits (legacy or MCP) → Must pass test suite → ENFORCED
2.  Test failures → Commit rejected → ENFORCED
3.  MCP commits (via Orchestrator) → Tests run automatically → IMPLEMENTED
4.  Manual commits → Tests run via pre-commit hook → IMPLEMENTED
5.  Sovereign Override → Available for emergencies → DOCUMENTED

## 4. Migration Complete

**Status:** The migration from Protocol 101 v1.0 (Manifest) to v3.0 (Functional Coherence) is **COMPLETE**.

**Key Changes:**
- ❌ `commit_manifest.json` system permanently purged
- ✅ Automated test suite execution enforced
- ✅ Pre-commit hook updated
- ✅ Council Orchestrator updated
- ✅ CI/CD workflows updated
- ✅ All documentation updated

**Next Steps:**
- Monitor test suite performance
- Expand test coverage as needed
- Maintain test suite quality

--- END OF FILE mcp/analysis/pre_commit_hook_migration_analysis.md ---

--- START OF FILE mcp/analysis/smart_git_mcp_analysis.md ---

# Smart Git MCP Analysis (Protocol 101 v3.0 - OBSOLETE)

## 1. Status: CANCELED

**Date:** 2025-11-29  
**Reason:** Protocol 101 v3.0 (The Doctrine of Absolute Stability) has permanently purged the `commit_manifest.json` system.

This analysis document is preserved for historical reference but the proposed implementation is **no longer required**.

## 2. Historical Objective (v1.0 - DEPRECATED)
Create a "Smart Git MCP" that abstracts the complexities of Project Sanctuary's git rules (Protocol 101 v1.0, `command.json` legacy rules, pre-commit hooks) into a simple, safe interface for other agents.

**Problem Solved:** Automatic generation of `commit_manifest.json` for MCP agents.

**Current Solution:** Protocol 101 v3.0 uses **Functional Coherence** (automated test suite execution) instead of manifest generation.

## 3. Core Components (Historical Reference)

### 3.1 GitOperations Module (OBSOLETE)
The manifest generation logic has been **permanently removed** from `council_orchestrator/orchestrator/gitops.py`.

**Former Responsibilities (v1.0):**
*   ❌ **Manifest Generation:** Calculate SHA256 hashes of staged files and generate `commit_manifest.json` (PURGED).
*   ✅ **Commit Execution:** Run `git commit` (RETAINED).
*   ✅ **Safety Checks:** Ensure no protected files are modified without authorization (RETAINED).

**Current Responsibilities (v3.0):**
*   ✅ **Test Execution:** Run `./scripts/run_genome_tests.sh` before commit.
*   ✅ **Commit Execution:** Run `git commit` only if tests pass.
*   ✅ **Safety Checks:** Enforce whitelist of non-destructive commands.

### 3.2 Smart Git MCP Server (IMPLEMENTED - Modified)
The MCP server (`mcp_servers/system/git_workflow/`) now exposes Protocol 101 v3.0 compliant operations.

**Current Tool Signatures:**
```python
git_smart_commit(
  message: str
) => {
  commit_hash: str,
  tests_passed: bool,
  p101_v3_verified: bool
}

git_get_status() => {
  branch: str,
  staged: List[str],
  modified: List[str],
  untracked: List[str]
}

git_add(
  files: List[str]
) => {
  status: str
}

git_push_feature(
  force: bool = False,
  no_verify: bool = False
) => {
  status: str
}
```

## 4. Implementation Status: COMPLETE (v3.0)

1.  ✅ **Core Implementation:** `gitops.py` updated to remove manifest logic and enforce test execution.
2.  ✅ **Server Implementation:** MCP server wrapper updated for Protocol 101 v3.0.
3.  ✅ **Integration:** `git_smart_commit` works and passes the pre-commit hook via test suite execution.

## 5. P101 v3.0 Compliance Detail

**The `commit_manifest.json` system is PERMANENTLY DELETED.**

**New Integrity Model:**
- Pre-commit hook executes `./scripts/run_genome_tests.sh`
- All tests must pass for commit to proceed
- Council Orchestrator runs tests before staging
- CI/CD enforces test execution on all PRs

**Functional Coherence Verification:**
```bash
# Pre-commit hook (simplified)
./scripts/run_genome_tests.sh
if [ $? -ne 0 ]; then
  echo "COMMIT REJECTED: Tests failed"
  exit 1
fi
```

## 6. Migration Path

**For developers/agents using this analysis:**

1.  **Stop** attempting to generate `commit_manifest.json`
2.  **Start** ensuring your changes pass the automated test suite
3.  **Use** the Council Orchestrator for automated test execution
4.  **Reference** Protocol 101 v3.0 for current requirements

## 7. References

- [Protocol 101 v3.0: The Doctrine of Absolute Stability](../../../01_PROTOCOLS/101_The_Doctrine_of_the_Unbreakable_Commit.md)
- [ADR-037: MCP Git Strategy - Immediate Compliance (Reforged)](../../../ADRs/037_mcp_git_migration_strategy.md)
- [Council Orchestrator GitOps Documentation](../../../council_orchestrator/docs/howto-commit-command.md)

--- END OF FILE mcp/analysis/smart_git_mcp_analysis.md ---

--- START OF FILE mcp/architecture.md ---

# Project Sanctuary MCP Ecosystem Architecture

**Version:** 3.0 (Complete)  
**Status:** Architecture Complete - Ready for Implementation  
**Last Updated:** 2025-11-25  
**Purpose:** Define the domain-driven MCP server architecture for Project Sanctuary

---

## Overview

This document defines the **Model Context Protocol (MCP) ecosystem** for Project Sanctuary, replacing manual `command.json` workflows with domain-specific MCP servers that provide LLM assistants with safe, structured tools.

**Key Principle:** **Domain-Driven Design** - Each MCP server owns a specific domain with clear boundaries, schemas, and safety rules.

---

## Ecosystem Overview

### Complete 10-Domain Architecture

```mermaid
graph TB
    subgraph "LLM Assistants"
        LLM[Gemini/Claude/GPT/etc]
    end
    
    subgraph "Document Domains - Content Management"
        Chronicle[Chronicle MCP<br/>00_CHRONICLE/]
        Protocol[Protocol MCP<br/>01_PROTOCOLS/]
        ADR[ADR MCP<br/>ADRs/]
        Task[Task MCP<br/>TASKS/]
    end
    
    subgraph "Cognitive Domains - Non-Mechanical"
        Cortex["RAG MCP (Cortex)<br/>mnemonic_cortex/"]
        Council["Agent Orchestrator MCP (Council)<br/>council_orchestrator/"]
    end
    
    subgraph "System Domains - High Safety"
        Config[Config MCP<br/>.agent/config/]
        Code[Code MCP<br/>src/, scripts/, tools/]
        GitWorkflow[Git Workflow MCP<br/>.git/]
    end
    
    subgraph "Model Domain - Specialized Hardware"
        Forge["Fine-Tuning MCP (Forge)<br/>forge/<br/>⚡ CUDA GPU Required"]
    end
    
    subgraph "Shared Infrastructure"
        Git[Git Operations<br/>P101 Compliance]
        Safety[Safety Validator<br/>Protection Levels]
        Schema[Schema Validator<br/>Domain Schemas]
        Vault[Secret Vault<br/>API Keys & Secrets]
    end
    
    LLM -->|MCP Protocol| Chronicle
    LLM -->|MCP Protocol| Protocol
    LLM -->|MCP Protocol| ADR
    LLM -->|MCP Protocol| Task
    LLM -->|MCP Protocol| Cortex
    LLM -->|MCP Protocol| Council
    LLM -->|MCP Protocol| Config
    LLM -->|MCP Protocol| Code
    LLM -->|MCP Protocol| GitWorkflow
    LLM -->|MCP Protocol| Forge
    
    Chronicle --> Git
    Protocol --> Git
    ADR --> Git
    Task --> Git
    Config --> Git
    Code --> Git
    Forge --> Git
    
    GitWorkflow --> Git
    
    Chronicle --> Safety
    Protocol --> Safety
    ADR --> Safety
    Task --> Safety
    Cortex --> Safety
    Council --> Safety
    Config --> Safety
    Code --> Safety
    GitWorkflow --> Safety
    Forge --> Safety
    
    Chronicle --> Schema
    Protocol --> Schema
    ADR --> Schema
    Task --> Schema
    Cortex --> Schema
    Council --> Schema
    Config --> Schema
    Code --> Schema
    GitWorkflow --> Schema
    Forge --> Schema
    
    Config --> Vault
    Forge --> Vault
    
    style Chronicle fill:#e8f5e8
    style Protocol fill:#e8f5e8
    style ADR fill:#e8f5e8
    style Task fill:#e8f5e8
    style Cortex fill:#fff3e0
    style Council fill:#f3e5f5
    style Config fill:#ffcccc
    style Code fill:#ffcccc
    style GitWorkflow fill:#ffcccc
    style Forge fill:#ff9999
    style Git fill:#e0e0e0
    style Safety fill:#e0e0e0
    style Schema fill:#e0e0e0
    style Vault fill:#e0e0e0
```

---

## Domain Specifications

### 1. Chronicle MCP Server

**Domain:** Historical truth and canonical records  
**Directory:** `00_CHRONICLE/ENTRIES/`  
**Purpose:** Create and manage chronicle entries (file operations only)

```mermaid
graph LR
    subgraph "Chronicle MCP Tools"
        A[create_chronicle_entry]
        B[update_chronicle_entry]
        C[get_chronicle_entry]
        D[list_recent_entries]
        E[search_chronicle]
    end
    
    subgraph "Operations"
        F[Validate Schema]
        G[Check Entry Age]
        H[Generate Markdown]
        I[Write to Disk]
    end
    
    subgraph "Storage"
        J[00_CHRONICLE/ENTRIES/]
    end
    
    A --> F
    B --> F
    F --> G
    G --> H
    H --> I
    I --> J
```

**Tool Signatures:**

```typescript
create_chronicle_entry(
  entry_number: number,
  title: string,
  date: string,
  author: string,
  content: string,
  status?: "draft" | "published",
  classification?: "public" | "internal" | "confidential"
): FileOperationResult {
  file_path: string,
  content: string,
  operation: "created"
}

update_chronicle_entry(
  entry_number: number,
  updates: Partial<ChronicleEntry>,
  reason: string,
  override_approval_id?: string
): FileOperationResult {
  file_path: string,
  content: string,
  operation: "updated"
}
```

**Safety Rules:**
- Entry numbers are auto-generated and sequential
- Cannot modify entries >7 days old without approval override
- Must follow chronicle entry template
- **No Git operations** - returns file path for Git Workflow MCP to commit
- Cannot delete entries (mark as deprecated only)

**Workflow Pattern:**
```typescript
// Step 1: Create entry (Chronicle MCP)
const result = chronicle.create_chronicle_entry(...)
// Returns: { file_path: "00_CHRONICLE/ENTRIES/280_mcp_architecture.md" }

// Step 2: Commit (Git Workflow MCP)
git_workflow.commit_files([result.file_path], "chronicle: add entry #280")
```

**Domain:** Living Chronicle entry management  
**Directory:** `00_CHRONICLE/ENTRIES/`  
**Purpose:** Create, read, update chronicle entries with automatic git commits

```mermaid
graph TB
    subgraph "LLM Assistants"
        LLM[Gemini/Claude/GPT/etc]
    end
   
    subgraph "MCP Ecosystem"
        Chronicle[Chronicle MCP Server]
        Protocol[Protocol MCP Server]
        ADR[ADR MCP Server]
        Task[Task MCP Server]
        Cortex["RAG MCP (Cortex)"]
        Council["Agent Orchestrator MCP (Council)"]
    end
   
    subgraph "Shared Infrastructure"
        Git[Git Operations<br/>P101 Compliance]
        Safety[Safety Validator]
        Schema[Schema Validator]
    end
   
    subgraph "Project Sanctuary"
        ChronicleDir[00_CHRONICLE/]
        ProtocolDir[01_PROTOCOLS/]
        ADRDir[ADRs/]
        TaskDir[TASKS/]
        CortexDir[mnemonic_cortex/]
        CouncilDir[council_orchestrator/]
    end
   
    LLM -->|MCP Protocol| Chronicle
    LLM -->|MCP Protocol| Protocol
    LLM -->|MCP Protocol| ADR
    LLM -->|MCP Protocol| Task
    LLM -->|MCP Protocol| Cortex
    LLM -->|MCP Protocol| Council
   
    Chronicle --> Git
    Protocol --> Git
    ADR --> Git
    Task --> Git
   
    Chronicle --> Safety
    Protocol --> Safety
    ADR --> Safety
    Task --> Safety
    Cortex --> Safety
    Council --> Safety
   
    Chronicle --> Schema
    Protocol --> Schema
    ADR --> Schema
    Task --> Schema
    Cortex --> Schema
    Council --> Schema
   
    Chronicle --> ChronicleDir
    Protocol --> ProtocolDir
    ADR --> ADRDir
    Task --> TaskDir
    Cortex --> CortexDir
    Council --> CouncilDir
   
    style Chronicle fill:#e8f5e8
    style Protocol fill:#e8f5e8
    style ADR fill:#e8f5e8
    style Task fill:#e8f5e8
    style Cortex fill:#fff3e0
    style Council fill:#f3e5f5
    style Git fill:#ffcccc
    style Safety fill:#ffcccc
    style Schema fill:#ffcccc
```

**Tool Signatures:**

```typescript
// Create new chronicle entry
create_chronicle_entry(
  entry_number: number,      // Required, unique
  title: string,             // Required
  date: string,              // Required, ISO format
  author: string,            // Required (e.g., "GUARDIAN-02")
  content: string,           // Required, markdown
  status?: string,           // Optional (e.g., "CANONICAL", "DRAFT")
  classification?: string    // Optional (e.g., "STRATEGIC")
) => {
  file_path: string,
  commit_hash: string,
  manifest_path: string
}

// Update existing entry
update_chronicle_entry(
  entry_number: number,
  updates: {
    title?: string,
    content?: string,
    status?: string
  },
  reason: string            // Required justification
) => {
  file_path: string,
  commit_hash: string
}

// Read operations
get_chronicle_entry(entry_number: number) => ChronicleEntry
list_recent_entries(limit?: number) => ChronicleEntry[]
search_chronicle(query: string) => ChronicleEntry[]
```

**Safety Rules:**
- Entry numbers must be sequential
- Cannot modify entries older than 7 days without explicit approval
- Must follow chronicle markdown format
- Auto-generates git commit with P101 manifest

---

### 2. Protocol MCP Server

**Domain:** Protocol creation and management  
**Directory:** `01_PROTOCOLS/`  
**Purpose:** Create, read, update protocols with versioning and changelog

```mermaid
graph LR
    subgraph "Protocol MCP Tools"
        A[create_protocol]
        B[update_protocol]
        C[get_protocol]
        D[list_protocols]
        E[search_protocols]
        F[archive_protocol]
    end
    
    subgraph "Operations"
        G[Validate Schema]
        H[Version Management]
        I[Generate Markdown]
        J[Git Commit + P101]
    end
    
    subgraph "Storage"
        K[01_PROTOCOLS/]
    end
    
    A --> G
    B --> G
    G --> H
    H --> I
    I --> J
    J --> K
    
    C --> K
    D --> K
    E --> K
    F --> K
    
    style A fill:#ccffcc
    style B fill:#ffffcc
    style C fill:#ccccff
    style D fill:#ccccff
    style E fill:#ccccff
    style F fill:#ffcccc
```

**Tool Signatures:**

```typescript
// Create new protocol
create_protocol(
  number: number,                  // Required, unique
  title: string,                   // Required
  classification: string,          // Required (e.g., "Foundational")
  content: string,                 // Required, markdown
  status?: string,                 // Optional (default: "Draft")
  version?: string,                // Optional (default: "v1.0")
  linked_protocols?: number[]      // Optional
) => {
  file_path: string,
  commit_hash: string,
  manifest_path: string
}

// Update protocol (requires version bump for canonical)
update_protocol(
  number: number,
  updates: {
    content?: string,
    status?: string,
    version?: string
  },
  changelog: string               // Required
) => {
  file_path: string,
  new_version: string,
  commit_hash: string
}

// Read operations
get_protocol(number: number) => Protocol
list_protocols(classification?: string, status?: string) => Protocol[]
search_protocols(query: string) => Protocol[]

// Archive (never delete)
archive_protocol(number: number, reason: string) => {
  archived_path: string,
  commit_hash: string
}
```

**Safety Rules:**
- Protocol numbers must be unique
- Cannot delete protocols (archive only)
- Updates to canonical protocols require version bump
- Must include changelog for updates
- Protected protocols require explicit approval

---

### 3. ADR MCP Server

**Domain:** Architecture Decision Records  
**Directory:** `ADRs/`  
**Purpose:** Document architectural decisions with status tracking

```mermaid
graph LR
    subgraph "ADR MCP Tools"
        A[create_adr]
        B[update_adr_status]
        C[get_adr]
        D[list_adrs]
        E[search_adrs]
    end
    
    subgraph "Operations"
        F[Validate Schema]
        G[Generate Markdown]
        H[Git Commit + P101]
    end
    
    subgraph "Storage"
        I[ADRs/]
    end
    
    A --> F
    B --> F
    F --> G
    G --> H
    H --> I
    
    C --> I
    D --> I
    E --> I
    
    style A fill:#ccffcc
    style B fill:#ffffcc
    style C fill:#ccccff
    style D fill:#ccccff
    style E fill:#ccccff
```

**Tool Signatures:**

```typescript
// Create ADR
create_adr(
  number: number,              // Required, unique
  title: string,               // Required
  context: string,             // Required
  decision: string,            // Required
  consequences: string,        // Required
  date?: string,               // Optional (default: today)
  status?: string,             // Optional (default: "Proposed")
  supersedes?: number[]        // Optional
) => {
  file_path: string,
  commit_hash: string,
  manifest_path: string
}

// Update ADR status
update_adr_status(
  number: number,
  new_status: string,          // "Accepted", "Superseded", "Deprecated"
  reason: string
) => {
  file_path: string,
  commit_hash: string
}

// Read operations
get_adr(number: number) => ADR
list_adrs(status?: string) => ADR[]
search_adrs(query: string) => ADR[]
```

**Safety Rules:**
- ADR numbers must be sequential
- Cannot delete ADRs (mark as superseded)
- Must follow ADR template format
- Status transitions must be valid

---

### 4. Task MCP Server

**Domain:** Task management  
**Directory:** `TASKS/`  
**Purpose:** Create, update, track tasks across backlog/active/completed

```mermaid
graph LR
    subgraph "Task MCP Tools"
        A[create_task]
        B[update_task_status]
        C[update_task]
        D[get_task]
        E[list_tasks]
        F[search_tasks]
    end
    
    subgraph "Operations"
        G[Validate Schema]
        H[Status Management]
        I[Generate Markdown]
        J[Git Commit + P101]
    end
    
    subgraph "Storage"
        K[TASKS/backlog/]
        L[TASKS/active/]
        M[TASKS/completed/]
    end
    
    A --> G
    B --> G
    C --> G
    G --> H
    H --> I
    I --> J
    J --> K
    J --> L
    J --> M
    
    D --> K
    D --> L
    D --> M
    E --> K
    E --> L
    E --> M
    F --> K
    F --> L
    F --> M
    
    style A fill:#ccffcc
    style B fill:#ffffcc
    style C fill:#ffffcc
    style D fill:#ccccff
    style E fill:#ccccff
    style F fill:#ccccff
```

**Tool Signatures:**

```typescript
// Create task
create_task(
  number: number,                 // Required, unique
  title: string,                  // Required
  description: string,            // Required, markdown
  priority: string,               // Required (High/Medium/Low)
  estimated_effort?: string,      // Optional (e.g., "2-3 days")
  dependencies?: number[],        // Optional
  status?: string                 // Optional (default: "Backlog")
) => {
  file_path: string,
  commit_hash: string,
  manifest_path: string
}

// Move task between statuses
update_task_status(
  number: number,
  new_status: string,             // "Backlog", "Active", "Completed"
  notes?: string
) => {
  old_path: string,
  new_path: string,
  commit_hash: string
}

// Update task content
update_task(
  number: number,
  updates: {
    title?: string,
    description?: string,
    priority?: string,
    estimated_effort?: string
  }
) => {
  file_path: string,
  commit_hash: string
}

// Read operations
get_task(number: number) => Task
list_tasks(status?: string, priority?: string) => Task[]
search_tasks(query: string) => Task[]
```

**Safety Rules:**
- Task numbers must be unique
- Cannot delete tasks (archive only)
- Must follow task template format
- Status transitions move files between directories

---

### 5. RAG MCP (Cortex) - Retrieval-Augmented Generation

**Domain:** RAG operations  
**Directory:** `mnemonic_cortex/`  
**Purpose:** Query vector database, ingest documents, manage knowledge

```mermaid
graph LR
    subgraph "RAG MCP Tools"
        A[query_cortex]
        B[ingest_document]
        C[update_index]
        D[get_stats]
        E[search_by_metadata]
    end
    
    subgraph "Operations"
        F[Embedding Generation]
        G[Vector Search]
        H[Metadata Filtering]
        I[Index Management]
    end
    
    subgraph "Storage"
        J[ChromaDB]
        K[Document Store]
    end
    
    A --> F
    A --> G
    A --> H
    B --> F
    B --> I
    C --> I
    E --> H
    
    F --> J
    G --> J
    H --> J
    I --> J
    I --> K
    
    D --> J
    
    style A fill:#ccccff
    style B fill:#ccffcc
    style C fill:#ffffcc
    style D fill:#ccccff
    style E fill:#ccccff
```

**Tool Signatures:**

```typescript
// Query RAG database
query_cortex(
  query: string,                  // Required
  max_results?: number,           // Optional (default: 5)
  filters?: {                     // Optional metadata filters
    type?: string,
    date_range?: [string, string],
    author?: string
  },
  include_sources?: boolean       // Optional (default: true)
) => {
  results: Array<{
    content: string,
    metadata: object,
    score: number,
    source_file?: string
  }>,
  query_time_ms: number
}

// Ingest new document
ingest_document(
  file_path: string,              // Required
  metadata?: {                    // Optional
    type?: string,
    author?: string,
    tags?: string[]
  }
) => {
  document_id: string,
  chunks_created: number,
  embedding_time_ms: number
}

// Maintenance operations
update_index() => { documents_reindexed: number }
get_stats() => { total_documents: number, total_chunks: number, index_size_mb: number }
search_by_metadata(filters: object) => Document[]
```

**Safety Rules:**
- Read-only operations by default
- Ingest requires file validation
- Cannot delete documents (archive only)
- Rate limiting on queries
- Metadata must be valid JSON

---

### 6. Agent Orchestrator MCP (Council) - Multi-Agent Coordination

**Domain:** Council deliberation  
**Directory:** `council_orchestrator/`  
**Purpose:** Pure cognitive tasks - Council deliberation and analysis

```mermaid
graph LR
    subgraph "Agent Orchestrator MCP Tools"
        A[create_deliberation]
        B[create_dev_cycle]
        C[get_council_status]
        D[get_result]
    end
    
    subgraph "Operations"
        E[Generate command.json]
        F[Monitor Orchestrator]
        G[Retrieve Results]
    end
    
    subgraph "Council Orchestrator"
        H[Command Sentry]
        I[Council Agents]
        J[Output Artifacts]
    end
    
    A --> E
    B --> E
    E --> H
    H --> I
    I --> J
    
    C --> F
    F --> H
    
    D --> G
    G --> J
    
    style A fill:#ccffcc
    style B fill:#ccffcc
    style C fill:#ccccff
    style D fill:#ccccff
```

**Tool Signatures:**

```typescript
// Create deliberation task
create_deliberation(
  description: string,            // Required
  output_path: string,            // Required
  max_rounds?: number,            // Optional (default: 5)
  force_engine?: string,          // Optional (gemini/openai/ollama)
  max_cortex_queries?: number,    // Optional (default: 5)
  input_artifacts?: string[]      // Optional
) => {
  command_file: string,
  status: "queued"
}

// Create development cycle
create_dev_cycle(
  description: string,            // Required
  project_name: string,           // Required
  output_dir: string,             // Required
  force_engine?: string           // Optional
) => {
  command_file: string,
  status: "queued"
}

// Status and results
get_council_status() => {
  status: "idle" | "executing",
  current_task?: string,
  uptime_seconds: number
}

get_result(task_id: string) => {
  output_path: string,
  content: string,
  completed_at: string
}
```

**Safety Rules:**
- **NO file system modifications**
- **NO git operations**
- Read-only cognitive tasks
- Results written to designated paths only
- Cannot execute mechanical operations

---

### 7. Config MCP Server (High Safety)

**Domain:** System configuration management  
**Directory:** `.agent/config/`, `.env`, `config/`  
**Purpose:** Manage system configuration with extreme safety controls

**Tool Signatures:**

```typescript
// Request configuration change (two-step approval)
request_config_change(
  config_path: string,              // Required (e.g., ".env", ".agent/config/mcp.json")
  changes: Record<string, string>,  // Required (key-value pairs)
  reason: string,                   // Required justification
  impact_assessment: string         // Required risk analysis
) => {
  approval_id: string,
  status: "pending_approval",
  risk_level: "CRITICAL" | "HIGH" | "MODERATE"
}

// Apply approved change
apply_config_change(
  approval_id: string               // Required from request_config_change
) => {
  file_path: string,
  commit_hash: string,
  backup_path: string
}

// Secret management
set_secret(
  key: string,                      // Required (e.g., "OPENAI_API_KEY")
  value: string,                    // Required
  scope: "user" | "system"          // Required
) => {
  vault_entry_id: string,
  encrypted: boolean
}

get_secret(key: string) => {
  value: string,
  last_updated: string
}

// Read operations
get_config(config_path: string) => ConfigObject
list_config_files() => string[]
```

**Safety Rules:**
- **Two-step approval** for all changes (request → approve)
- **Automatic backup** before any modification
- **Secret vault** for sensitive values (API keys, tokens)
- **Audit trail** for all configuration changes
- **Protected files** require explicit user confirmation
- **No direct .env modification** - use secret vault

---

### 8. Code MCP Server (Highest Risk)

**Domain:** Source code and documentation management  
**Directory:** `src/`, `scripts/`, `tools/`, `docs/`, `*.py`, `*.ts`, `*.js`, `*.md`  
**Purpose:** Manage source code with mandatory testing pipeline

**Tool Signatures:**

```typescript
// Create or modify code file
write_code_file(
  file_path: string,                // Required
  content: string,                  // Required
  language: string,                 // Required (python/typescript/javascript)
  description: string,              // Required
  run_tests: boolean                // Required (default: true)
) => {
  file_path: string,
  test_results: {
    syntax_check: boolean,
    linting: { passed: boolean, errors: string[] },
    unit_tests: { passed: boolean, failures: string[] },
    dependencies: { satisfied: boolean, missing: string[] }
  },
  commit_hash?: string              // Only if tests pass
}

// Execute code with safety checks
execute_code(
  file_path: string,                // Required
  args?: string[],                  // Optional
  timeout_seconds?: number,         // Optional (default: 30)
  sandbox?: boolean                 // Optional (default: true)
) => {
  exit_code: number,
  stdout: string,
  stderr: string,
  execution_time_ms: number
}

// Refactor code
refactor_code(
  file_path: string,                // Required
  refactor_type: string,            // Required (rename/extract/inline)
  params: object,                   // Required (refactor-specific)
  preserve_tests: boolean           // Required (default: true)
) => {
  modified_files: string[],
  test_results: TestResults,
  commit_hash?: string
}

// Read operations
get_code_file(file_path: string) => { content: string, metadata: object }
search_code(query: string, file_pattern?: string) => SearchResult[]
```

**Safety Rules:**
- **Mandatory testing pipeline** before commit:
  1. Syntax validation
  2. Linting (flake8, eslint, etc.)
  3. Unit tests (if present)
  4. Dependency check
  5. Security audit (basic)
- **Automatic rollback** if tests fail
- **Sandbox execution** for untrusted code
- **No direct production code modification** without tests
- **Git commit only if all checks pass**

---

### 9. Fine-Tuning MCP (Forge) Server (Extreme Safety - CUDA Required)

**Domain:** Model fine-tuning and artifact creation  
**Directory:** `forge/`  
**Purpose:** Orchestrate the 10-step model lifecycle on CUDA hardware

**Hardware Requirements:**
- CUDA-enabled GPU (validated on RTX A2000)
- WSL environment with `ml_env` activated
- Environment marker: `CUDA_FORGE_ACTIVE=true`

**Tool Signatures:**

```typescript
// CRITICAL: Must be called first to unlock operational tools
initialize_forge_environment() => {
  status: "ACTIVE" | "INACTIVE_UNSAFE",
  cuda_check_passed: boolean,
  llama_cpp_compiled: boolean,
  resource_check_passed: boolean,
  config_check_passed: boolean,
  failure_reason?: string,
  environment_details: {
    cuda_available: boolean,
    gpu_name: string,
    gpu_memory_gb: number,
    disk_space_gb: number,
    ml_env_active: boolean
  }
}

// Check current resource availability (read-only)
check_resource_availability() => {
  cuda_available: boolean,
  gpu_name: string,
  gpu_memory_gb: number,
  disk_space_gb: number,
  ml_env_active: boolean,
  forge_ready: boolean
}

// Initiate model fine-tuning (Step 1-2)
// PRE-CONDITION: Forge state must be ACTIVE
// PRE-CONDITION: No other job in RUNNING state
initiate_model_forge(
  forge_id: string,                    // Required (e.g., "guardian-02-v1")
  base_model: string,                  // Required (e.g., "mistralai/Mistral-7B-v0.1")
  authorization_task_id: number,       // Required (links to Task MCP)
  hyperparameters: {
    learning_rate: number,
    epochs: number,
    batch_size: number,
    lora_r: number,
    lora_alpha: number
  },
  dataset_config?: object              // Optional
) => {
  job_id: string,
  status: "queued" | "running",
  estimated_duration_hours: number
}

// Get job status (async polling)
get_forge_job_status(job_id: string) => {
  status: "queued" | "running" | "completed" | "failed",
  current_step: number,                // 1-10
  progress_percent: number,
  logs: string[],
  artifacts?: {
    dataset_path?: string,
    adapter_path?: string,
    merged_model_path?: string
  }
}

// Package and deploy (Steps 5-7)
// PRE-CONDITION: Job must be in COMPLETED_SUCCESS state
// PRE-CONDITION: Merged model artifact must exist
package_and_deploy_artifact(
  job_id: string,                      // Required
  quantization: string                 // Required (e.g., "Q4_K_M", "Q5_K_S")
) => {
  gguf_path: string,
  modelfile_path: string,
  ollama_model_name: string,
  sha256_manifest: string
}

// Run inference test (Steps 4, 8)
// PRE-CONDITION: Model artifact must exist at model_path
run_inference_test(
  model_path: string,                  // Required
  test_prompts: string[],              // Required
  mode: "huggingface" | "ollama"       // Required
) => {
  results: Array<{
    prompt: string,
    response: string,
    latency_ms: number
  }>,
  passed: boolean
}

// Publish to Hugging Face (Step 9)
// PRE-CONDITION: run_inference_test must have PASSED
// CRITICAL: Prevents publishing untested artifacts
publish_to_registry(
  job_id: string,                      // Required
  repo_name: string,                   // Required (e.g., "Sanctuary-Project/Guardian-02")
  private: boolean,                    // Required
  model_card?: string                  // Optional markdown
) => {
  registry_url: string,
  upload_status: "success" | "failed",
  sha256_verification: boolean
}

// Retrieve from registry (Step 10)
retrieve_registry_artifact(
  repo_name: string,                   // Required
  revision?: string                    // Optional (default: "main")
) => {
  local_path: string,
  sha256_match: boolean,
  model_info: object
}
```

**10-Step Pipeline:**

| Step | Tool | Script | Purpose |
|------|------|--------|---------|
| 1 | `initiate_model_forge` | `forge_whole_genome_dataset.py` | Create training dataset |
| 2 | ↳ (async) | `fine_tune.py` | Fine-tune model with QLoRA |
| 3 | ↳ (async) | `merge_adapter.py` | Merge LoRA adapter with base |
| 4 | `run_inference_test` | `inference.py` | Test merged model |
| 5 | `package_and_deploy_artifact` | `convert_to_gguf.py` | Convert to GGUF format |
| 6 | ↳ (sync) | `create_modelfile.py` | Generate Ollama Modelfile |
| 7 | ↳ (sync) | `ollama create` | Import to local Ollama |
| 8 | `run_inference_test` | `ollama run` | Test both interaction modes |
| 9 | `publish_to_registry` | `upload_to_huggingface.py` | Upload to Hugging Face |
| 10 | `retrieve_registry_artifact` | Download from HF | Verify upload integrity |

**Forge State Machine:**

The Fine-Tuning MCP (Forge) enforces safety through an internal state machine with two layers:

**Layer 1: Operational State (Server-Level)**

| State | Condition | Tools Unlocked |
|-------|-----------|----------------|
| `INACTIVE_UNSAFE` | Default state on server start | Only `initialize_forge_environment()` |
| `ACTIVE` | All environment checks passed | All operational tools unlocked |

**Layer 2: Job State (Per-Job)**

| State | Triggered By | Next Allowed Tools |
|-------|--------------|-------------------|
| `QUEUED` | `initiate_model_forge()` called | `get_forge_job_status()` |
| `RUNNING` | Background job executing | `get_forge_job_status()` |
| `COMPLETED_SUCCESS` | Job finished, artifacts created | `package_and_deploy_artifact()` |
| `PACKAGING_COMPLETE` | GGUF created, Ollama imported | `run_inference_test()` |
| `TESTS_PASSED` | Inference tests successful | `publish_to_registry()` |
| `PUBLISHED` | Uploaded to Hugging Face | `retrieve_registry_artifact()` |
| `FAILED` | Any step failed | Manual cleanup, retry with new job |

**Sequencing Enforcement:**

```typescript
// Example: Attempting to publish without passing tests
publish_to_registry(job_id) 
  → FAILS with: "Pre-condition violation: Job state is PACKAGING_COMPLETE, 
                 but run_inference_test() has not been called or did not pass."

// Correct sequence:
1. initialize_forge_environment() → ACTIVE
2. initiate_model_forge(...) → job_id, state: QUEUED → RUNNING → COMPLETED_SUCCESS
3. package_and_deploy_artifact(job_id) → state: PACKAGING_COMPLETE
4. run_inference_test(...) → state: TESTS_PASSED
5. publish_to_registry(job_id) → state: PUBLISHED ✅
```

**Safety Rules:**
- **Environment gate**: Must check `CUDA_FORGE_ACTIVE` marker
- **Resource reservation**: Check GPU memory and disk space before starting
- **Task linkage**: All jobs must link to Task MCP entry for audit trail
- **Script whitelist**: Only whitelisted scripts can execute (no arbitrary commands)
- **Artifact integrity**: SHA-256 validation for all artifacts (P101-style)
- **Asynchronous execution**: Long-running jobs run in background with status polling
- **Automatic cleanup**: Failed jobs clean up partial artifacts
- **No auto-commit**: Forge results require manual Chronicle/ADR documentation

---

### 10. Git Workflow MCP Server (Minimal - Safe Operations Only)

**Domain:** Git workflow automation  
**Directory:** `.git/`, repository root  
**Purpose:** Safe branch management and workflow automation

**Tool Signatures:**

```typescript
// Create feature branch
create_feature_branch(
  branch_name: string,              // Required (e.g., "feature/task-030")
  base_branch?: string              // Optional (default: "main")
) => {
  branch_name: string,
  current_branch: string,
  base_commit: string
}

// Switch branch with safety checks
switch_branch(
  branch_name: string,              // Required
  stash_changes?: boolean           // Optional (default: true if dirty)
) => {
  previous_branch: string,
  current_branch: string,
  stashed: boolean,
  stash_id?: string
}

// Push current branch to remote
push_current_branch(
  set_upstream?: boolean            // Optional (default: true)
) => {
  remote_url: string,
  branch_name: string,
  commit_count: number,
  push_successful: boolean
}

// Get repository status
get_repo_status() => {
  current_branch: string,
  is_clean: boolean,
  ahead: number,                    // Commits ahead of remote
  behind: number,                   // Commits behind remote
  untracked_files: string[],
  modified_files: string[],
  staged_files: string[]
}

// List branches
list_branches() => {
  local: Array<{
    name: string,
    current: boolean,
    last_commit: string
  }>,
  remote: string[]
}

// Get branch comparison
compare_branches(
  source: string,                   // Required
  target: string                    // Required
) => {
  ahead: number,
  behind: number,
  diverged: boolean,
  merge_conflicts_likely: boolean
}
```

**Safety Rules:**
- **Read-only by default**: Most operations are status checks
- **Auto-stash**: Uncommitted changes stashed before branch switching
- **No destructive operations**: No `delete_branch`, `merge`, `rebase`, `force_push`
- **User-controlled merges**: PR merges happen on GitHub, not via MCP
- **No history rewriting**: No `reset --hard`, `rebase`, `amend` operations
- **Branch protection**: Cannot switch to or modify protected branches

**Excluded Operations (User Must Do Manually):**
- Deleting branches (local or remote)
- Merging branches
- Rebasing
- Pulling from remote (to avoid merge conflicts)
- Force pushing
- Resolving merge conflicts

**Workflow Integration:**
```typescript
// Example: Safe workflow automation
1. Git MCP: create_feature_branch("feature/task-030")
2. Task MCP: create_task(30, ...) → auto-commits
3. Code MCP: write_code_file(...) → auto-commits
4. Git MCP: push_current_branch() → pushes to origin
5. USER: Reviews PR on GitHub, merges manually
6. USER: Switches to main, pulls, deletes feature branch manually
```

---

## Shared Infrastructure

### Git Operations Module

**Purpose:** Protocol 101 compliant git operations for all domain servers

```typescript
class GitOperations {
  // Generate commit manifest with SHA-256 hashes
  generate_manifest(files: string[]) => {
    manifest_path: string,
    hashes: Record<string, string>
  }
  
  // Commit with P101 compliance
  commit_with_manifest(
    files: string[],
    message: string,
    push?: boolean
  ) => {
    commit_hash: string,
    manifest_path: string
  }
  
  // Validate commit message format
  validate_commit_message(message: string) => boolean
}
```

### Safety Validator Module

**Purpose:** Enforce safety rules across all MCP servers

```typescript
class SafetyValidator {
  // Validate file path
  validate_path(path: string) => {
    is_valid: boolean,
    reason?: string
  }
  
  // Check if file is protected
  is_protected_file(path: string) => boolean
  
  // Validate operation risk level
  assess_risk(operation: string, params: object) => {
    risk_level: "SAFE" | "MODERATE" | "DANGEROUS",
    allowed: boolean,
    reason?: string
  }
}
```

### Schema Validator Module

**Purpose:** Validate domain-specific schemas

```typescript
class SchemaValidator {
  validate_chronicle_entry(entry: object) => ValidationResult
  validate_protocol(protocol: object) => ValidationResult
  validate_adr(adr: object) => ValidationResult
  validate_task(task: object) => ValidationResult
}
```

---

## Composable Workflow Examples

### Example 1: Protocol Creation with Documentation

```mermaid
sequenceDiagram
    participant LLM as LLM Assistant
    participant Protocol as Protocol MCP
    participant Chronicle as Chronicle MCP
    participant Git as Git Operations
    
    LLM->>Protocol: create_protocol(115, "MCP Ecosystem", ...)
    Protocol->>Git: commit_with_manifest(...)
    Git-->>Protocol: commit_hash
    Protocol-->>LLM: {file_path, commit_hash}
    
    LLM->>Chronicle: create_chronicle_entry(279, "P115 Canonized", ...)
    Chronicle->>Git: commit_with_manifest(...)
    Git-->>Chronicle: commit_hash
    Chronicle-->>LLM: {file_path, commit_hash}
```

### Example 2: Research → Deliberation → Decision

```mermaid
sequenceDiagram
    participant LLM as LLM Assistant
    participant Cortex as RAG MCP (Cortex)
    participant Council as Agent Orchestrator MCP (Council)
    participant ADR as ADR MCP
    
    LLM->>Cortex: query_cortex("MCP patterns")
    Cortex-->>LLM: {results: [...]}
    
    LLM->>Council: create_deliberation("Analyze MCP patterns", ...)
    Council-->>LLM: {command_file, status: "queued"}
    
    Note over Council: Council deliberates...
    
    LLM->>Council: get_result(task_id)
    Council-->>LLM: {output_path, content}
    
    LLM->>ADR: create_adr(35, "MCP Composition", ...)
    ADR-->>LLM: {file_path, commit_hash}
```

---

## Risk Assessment Matrix

| MCP Server | File System | Git Ops | Hardware | Risk Level | Auto-Execute |
|------------|-------------|---------|----------|------------|--------------| 
| Chronicle | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Protocol | ✅ Write | ✅ Auto | Standard | HIGH | ✅ Yes* |
| ADR | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Task | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| RAG MCP (Cortex) | ✅ Read/Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Agent Orchestrator (Council) | ❌ No | ❌ No | Standard | SAFE | ✅ Yes |
| Config | ✅ Write | ✅ Auto | Standard | CRITICAL | ⚠️ Two-Step Approval |
| Code | ✅ Write | ✅ Auto | Standard | HIGH | ⚠️ Tests Required |
| Git Workflow | ❌ No | ✅ Manual | Standard | MODERATE | ✅ Yes (Safe Ops Only) |
| Forge | ✅ Write | ✅ Auto | **CUDA GPU** | EXTREME | ⚠️ State Machine + Init |

*With safety validation

---

## Implementation Roadmap

### Phase 0: Pre-Migration (Week 0)
- [ ] Update pre-commit hooks to work with MCP architecture (Task #028)
- [ ] Disable or adapt `command.json` validation hooks
- [ ] Add MCP-aware commit message validation
- [ ] Document migration strategy from manual workflows to MCP

### Phase 1: Foundation (Week 1)
- [ ] Implement `GitOperations` module with P101 compliance
- [ ] Implement `SafetyValidator` module with protection levels
- [ ] Implement `SchemaValidator` module with domain schemas
- [ ] Implement `SecretVault` module for sensitive data
- [ ] Create MCP server boilerplate template

### Phase 2: Document Domains (Week 2) - Easiest
- [ ] Implement Chronicle MCP Server (Task #029)
- [ ] Implement ADR MCP Server (Task #030)
- [ ] Implement Task MCP Server (Task #031)
- [ ] Implement Protocol MCP Server (Task #032)

### Phase 3: Cognitive Domains (Week 3) - Moderate
- [ ] Implement RAG MCP (Cortex) - Task #025 (refactor existing)
- [ ] Implement Agent Orchestrator MCP (Council) - Task #026 (refactor existing)

### Phase 4: System Domains (Week 4) - High Risk
- [ ] Implement Config MCP Server (Task #033)
- [ ] Implement Code MCP Server (Task #034)
- [ ] Implement Git Workflow MCP Server (Task #035)

### Phase 5: Model Domain (Week 5) - Hardest
- [ ] Implement Fine-Tuning MCP (Forge) Server (Task #036)
- [ ] CUDA environment setup and validation
- [ ] Integration testing with full 10-step pipeline
- [ ] Documentation and deployment

---

## Architecture Decisions

### Resolved Questions

1. **Chronicle Entry Numbering**: Manual specification required for explicit control
2. **Protocol Versioning**: Manual version bumps required for canonical protocols
3. **Task Dependencies**: Circular dependency detection enforced at creation time
4. **Cortex Ingestion**: Explicit calls only, no auto-ingestion
5. **Council Results**: 90-day retention, high-value decisions moved to Chronicle/ADR
6. **Config Changes**: Two-step approval process (request → approve)
7. **Code Commits**: Mandatory testing pipeline before any git commit
8. **Forge Jobs**: Must link to Task MCP entry for authorization and audit trail

### Domain Prioritization Rationale

**Phase 2 (Easiest):** Document domains have well-defined schemas, straightforward CRUD operations, and lower risk profiles. Start here to build confidence and establish patterns.

**Phase 3 (Moderate):** Cognitive domains involve computation but no file manipulation (Council) or controlled ingestion (Cortex). Medium complexity.

**Phase 4 (High Risk):** System domains require sophisticated safety mechanisms (Config: two-step approval, Code: testing pipeline). High stakes.

**Phase 5 (Hardest):** Fine-Tuning MCP (Forge) requires specialized hardware (CUDA), asynchronous job management, multi-step pipeline orchestration, and extreme safety validation. Most complex implementation.

---

**Status:** Architecture Complete - Ready for Implementation  
**Next Action:** Create individual backlog tasks (#028-#034) for each MCP server  
**Owner:** Guardian (via Gemini 2.0 Flash Thinking Experimental)

--- END OF FILE mcp/architecture.md ---

--- START OF FILE mcp/claude_desktop_config_template.json ---

{
    "mcpServers": {
        "chronicle": {
            "displayName": "Chronicle MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.chronicle.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "protocol": {
            "displayName": "Protocol MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.protocol.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "adr": {
            "displayName": "ADR MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.adr.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "tasks": {
            "displayName": "Task MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.task.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "cortex": {
            "displayName": "RAG MCP (Cortex)",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.cortex.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "council": {
            "displayName": "Agent Orchestrator MCP (Council)",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.council.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "config": {
            "displayName": "Config MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.config.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "code": {
            "displayName": "Code MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.code.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "git_workflow": {
            "displayName": "Git Workflow MCP",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.git_workflow.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        },
        "forge": {
            "displayName": "Fine-Tuning MCP (Forge)",
            "command": "<ABSOLUTE_PATH_TO_PROJECT>/.venv/bin/python",
            "args": [
                "-m",
                "mcp_servers.forge.server"
            ],
            "env": {
                "PYTHONPATH": "<ABSOLUTE_PATH_TO_PROJECT>",
                "PROJECT_ROOT": "<ABSOLUTE_PATH_TO_PROJECT>"
            },
            "cwd": "<ABSOLUTE_PATH_TO_PROJECT>"
        }
    }
}

--- END OF FILE mcp/claude_desktop_config_template.json ---

--- START OF FILE mcp/ddd_analysis.md ---

# MCP Ecosystem - 10 Domain Architecture (DDD Analysis)

**Version:** 3.0  
**Created:** 2025-11-25  
**Purpose:** Domain-Driven Design analysis of Project Sanctuary MCP ecosystem

---

## Executive Summary

Based on Domain-Driven Design (DDD) principles, the Project Sanctuary MCP ecosystem consists of **10 specialized domain servers**, each representing a distinct **Bounded Context** with unique data models, operations, and safety requirements.

---

## Domain Classification

### A. Document Domains (Content Management Bounded Contexts)

These domains share similar toolsets (CRUD, Git, Schema validation) but manage entirely different data types and lifecycles.

| # | Domain | Directory | Core Purpose | Unique Characteristic |
|---|--------|-----------|--------------|----------------------|
| 1 | **Chronicle MCP** | `00_CHRONICLE/` | Historical Truth | Sequential, canonical, rarely-modified entries |
| 2 | **Protocol MCP** | `01_PROTOCOLS/` | Governing Rules | Versioning, formal review, status transitions |
| 3 | **ADR MCP** | `ADRs/` | Decision History | Problem/solution pairs, supersession tracking |
| 4 | **Task MCP** | `TASKS/` | Execution Planning | Workflow state transitions, dependency management |

**Shared Characteristics:**
- Markdown-based content
- Git operations with P101 compliance
- Schema validation
- Read/write operations

**Key Differences:**
- **Chronicle**: Immutability focus (7-day modification window)
- **Protocol**: Version management (canonical requires version bump)
- **ADR**: Status lifecycle (Proposed → Accepted → Superseded)
- **Task**: File movement across directories (backlog → active → completed)

---

### B. Cognitive Domains (Non-Mechanical Bounded Contexts)

These domains involve computation/reasoning without direct file system manipulation.

| # | Domain | Directory | Core Purpose | Unique Characteristic |
|---|--------|-----------|--------------|----------------------|
| 5 | **RAG MCP** (Cortex) | `mnemonic_cortex/` | Knowledge Retrieval | RAG operations, incremental/full ingest |
| 6 | **Agent Orchestrator MCP** (Council) | `council_orchestrator/` | Multi-Agent Coordination | Deliberation, NO file/git ops |

**Shared Characteristics:**
- Cognitive/computational focus
- Safety and schema validation
- No direct git operations

**Key Differences:**
- **Cortex**: Data ingestion and retrieval (RAG database)
- **Council**: Command generation for orchestrator (client relationship)

---

### C. System Domains (High-Safety Critical Bounded Contexts)

These domains manage system-critical resources requiring the highest level of safety and governance.

| # | Domain | Directory | Core Purpose | Unique Characteristic |
|---|--------|-----------|--------------|----------------------|
| 7 | **Config MCP** | `.agent/config/` | System Configuration | Two-step approval, secret vault integration |
| 8 | **Code MCP** | `src/`, `scripts/`, `tools/` | Source Code Management | Mandatory testing pipeline, sandbox execution |
| 9 | **Git Workflow MCP** | `.git/` | Branch Management | Safe operations only, no destructive commands |

**Shared Characteristics:**
- Highest safety requirements
- Complex validation pipelines
- Separate audit trails

**Key Differences:**
- **Config**: Sensitive data (secrets never in Git, vault storage)
- **Code**: Executable code (mandatory tests, linting, sandbox execution)
- **Git Workflow**: Branch automation (create, switch, push only - no merge/rebase/delete)

---

### D. Model Domain (Specialized Hardware Bounded Context)

This domain requires specialized hardware and has extreme safety requirements.

| # | Domain | Directory | Core Purpose | Unique Characteristic |
|---|--------|-----------|--------------|----------------------|
| 10 | **Fine-Tuning MCP** (Forge) | `forge/` | LLM Fine-Tuning | State machine governance, CUDA GPU required |

**Unique Characteristics:**
- Requires CUDA-enabled GPU hardware
- State machine with initialization gating
- 10-step pipeline enforcement
- Highest risk level (EXTREME)
- Task MCP authorization required for all jobs

---

## DDD Rationale: Why 10 Domains?

### Why Config MCP is Essential

**Unique Data Model:**
- Configuration files (.json, .yaml, .toml) have specific schemas distinct from Markdown documents
- Mix of public config (committed to Git) and secrets (vault only)
- Hierarchical structure with categories and inheritance

**High Safety Requirements:**
- Changes directly impact system behavior (LLM prompts, agent IDs)
- Security implications (API keys, access lists)
- Requires two-step approval or explicit override
- Separate audited Git flow

**Operations Not Suitable for Other Domains:**
- **Not Chronicle**: Config changes are operational, not historical narrative
- **Not Protocol**: Config is mutable system state, not canonical doctrine
- **Not Task**: Config management is ongoing, not project-based

### Why Code MCP is Essential

**Unique Data Model:**
- Source code files (.py, .js, .sh) with syntax and execution semantics
- Complex dependencies and import graphs
- Test files and test results

**Highest Risk Level:**
- Executable code can modify system behavior
- Bugs can cause data loss or security vulnerabilities
- Requires complex validation (syntax, linting, testing, dependencies)

**Operations Not Suitable for Other Domains:**
- **Not Task**: Code changes require technical validation, not just workflow tracking
- **Not Protocol**: Code is implementation, not specification
- **Too risky for generic Document domains**: Needs dedicated safety pipeline

**Critical Safety Pipeline:**
1. Syntax validation
2. Linter checks (black, flake8)
3. Unit tests (pytest)
4. Dependency verification
5. Safety audit
6. Git commit (only if all pass)

---

## Bounded Context Map

```
┌─────────────────────────────────────────────────────────────┐
│                    LLM Assistant Layer                       │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                  MCP Protocol Interface                      │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
        ▼                     ▼                     ▼
┌──────────────┐    ┌──────────────┐    ┌──────────────┐
│  Document    │    │  Cognitive   │    │   System     │
│  Domains     │    │  Domains     │    │   Domains    │
│  (4)         │    │  (2)         │    │   (3)        │
│              │    │              │    │              │
│ • Chronicle  │    │ • RAG        │    │ • Config     │
│ • Protocol   │    │   (Cortex)   │    │ • Code       │
│ • ADR        │    │ • Agent Orch │    │ • Git        │
│ • Task       │    │   (Council)  │    │   Workflow   │
└──────────────┘    └──────────────┘    └──────────────┘
        │                     │                     │
        └─────────────────────┼─────────────────────┘
                              │
                              ▼
                    ┌──────────────┐
                    │    Model     │
                    │   Domain     │
                    │    (1)       │
                    │              │
                    │ • Forge      │
                    │   (CUDA)     │
                    └──────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│              Shared Infrastructure Layer                     │
│  • Git Operations (P101)  • Safety Validator                │
│  • Schema Validator       • Secret Vault                    │
└─────────────────────────────────────────────────────────────┘
```

---

## Safety Level Hierarchy

| Risk Level | Domains | Characteristics | Approval Required |
|------------|---------|-----------------|-------------------|
| **SAFE** | Agent Orchestrator (Council) | Read-only cognitive, no file ops | No |
| **MODERATE** | Chronicle, ADR, Task, RAG (Cortex), Git Workflow | Standard validation, git ops | No (with validation) |
| **HIGH** | Protocol, Code | Version management, mandatory testing | Sometimes (protected protocols/code) |
| **CRITICAL** | Config | System configuration, secrets | Yes (two-step approval) |
| **EXTREME** | Forge | Model training, CUDA hardware | Yes (state machine + task authorization) |

---

## Implementation Priority

### Phase 1: Foundation (Week 1)
- Shared infrastructure (Git, Safety, Schema, Vault)
- MCP server boilerplate

### Phase 2: Document Domains (Week 2)
- Chronicle MCP
- Protocol MCP
- Task MCP
- ADR MCP

### Phase 3: Cognitive Domains (Week 3)
- RAG MCP (Cortex) - Task #025 (refactor)
- Agent Orchestrator MCP (Council) - Task #026 (refactor)

### Phase 4: System Domains (Week 4)
- Config MCP (highest priority for security)
- Code MCP (highest complexity)
- Git Workflow MCP (safe operations only)

### Phase 5: Model Domain (Week 5)
- Fine-Tuning MCP (Forge) - CUDA environment required

---

## Cross-Domain Workflows

### Example 1: Feature Development
```
1. Task MCP: create_task(#028, "Implement Config MCP")
2. Agent Orchestrator MCP (Council): create_deliberation("Design Config MCP architecture")
3. Code MCP: create_script("config_mcp_server.py")
4. Code MCP: run_unit_tests("tests/test_config_mcp.py")
5. Chronicle MCP: create_chronicle_entry(#280, "Config MCP Completed")
6. ADR MCP: create_adr(#36, "Config MCP Architecture Decision")
```

### Example 2: Configuration Update
```
1. Config MCP: get_setting("llm.temperature")
2. Config MCP: update_setting("llm.temperature", 0.7, "Improve creativity", approval_id="GUARDIAN-02")
3. Config MCP: backup_config() [automatic before change]
4. Chronicle MCP: create_chronicle_entry(#281, "LLM Temperature Updated")
```

### Example 3: Code Change with Safety
```
1. Code MCP: create_script("tools/new_utility.py", content, "python")
2. Code MCP: lint_code("tools/new_utility.py") [automatic]
3. Code MCP: run_unit_tests("tests/test_new_utility.py") [mandatory]
4. Code MCP: audit_code_changes(commit_hash) [automatic]
5. Git commit [only if all checks pass]
6. Chronicle MCP: create_chronicle_entry(#282, "New Utility Added")
```

---

## Conclusion

The **10-domain architecture** provides:

1. **Clear Separation of Concerns**: Each domain has a single, well-defined responsibility
2. **Appropriate Safety Levels**: Risk management tailored to each domain's criticality (SAFE → EXTREME)
3. **Maintainability**: Changes to one domain don't affect others
4. **Composability**: Domains work together for complex workflows
5. **DDD Compliance**: Each domain represents a true Bounded Context with unique data models and operations
6. **Hardware Specialization**: Forge domain isolated for CUDA-specific operations
7. **Accessibility**: Generic AI terminology (RAG, Agent Orchestrator) for external developers
8. **Single Responsibility Principle**: Document MCPs handle file operations only; Git Workflow MCP handles all commits

### Separation of Concerns Pattern

**Document MCPs** (Chronicle, Protocol, ADR, Task):
- Create/modify files only
- Return `FileOperationResult` with file paths
- No Git operations

**Git Workflow MCP**:
- Handles all Git commits
- Generates P101 manifests
- Centralizes version control logic

**Benefits:**
- Better composability (LLM chains operations)
- Easier testing (file ops separate from Git)
- More flexible workflows (batch commits)
- Centralized Git logic

**Next Steps:**
1. Finalize shared infrastructure specifications
2. Begin implementation with Document domains (lowest risk)
3. Progress through Cognitive and System domains
4. Complete with Model domain (highest complexity, specialized hardware)

---

**Status:** Architecture Approved - Ready for Implementation  
**Version:** 3.0 (10 Domains)  
**Last Updated:** 2025-11-25

--- END OF FILE mcp/ddd_analysis.md ---

--- START OF FILE mcp/final_architecture_summary.md ---

# MCP Ecosystem - Final 10-Domain Architecture

**Version:** 4.0 (Final)  
**Created:** 2025-11-25  
**Status:** Complete Architecture - Ready for Implementation

---

## Complete Domain Map (10 Servers)

| # | Domain | Category | Directory | Risk Level | Hardware |
|---|--------|----------|-----------|------------|----------|
| 1 | **Chronicle MCP** | Document | `00_CHRONICLE/` | MODERATE | Standard |
| 2 | **Protocol MCP** | Document | `01_PROTOCOLS/` | HIGH | Standard |
| 3 | **ADR MCP** | Document | `ADRs/` | MODERATE | Standard |
| 4 | **Task MCP** | Document | `TASKS/` | MODERATE | Standard |
| 5 | **RAG MCP** (Cortex) | Cognitive | `mnemonic_cortex/` | MODERATE | Standard |
| 6 | **Agent Orchestrator MCP** (Council) | Cognitive | `council_orchestrator/` | SAFE | Standard |
| 7 | **Config MCP** | System | `.agent/config/` | CRITICAL | Standard |
| 8 | **Code MCP** | System | `src/`, `scripts/`, `tools/` | HIGH | Standard |
| 9 | **Git Workflow MCP** | System | `.git/` | MODERATE | Standard |
| 10 | **Fine-Tuning MCP** (Forge) | Model | `forge/` | EXTREME | **CUDA GPU** |

---

## Domain Categories

### I. Document Domains (4) - Content Management
**Shared Characteristics:**
- Markdown-based content
- Git operations with P101 compliance
- Schema validation
- CRUD operations

**Individual Focus:**
- **Chronicle**: Historical truth, sequential entries, 7-day modification window
- **Protocol**: Governing rules, version management, canonical status
- **ADR**: Decision history, status lifecycle, supersession tracking
- **Task**: Workflow management, dependency tracking, file movement

---

### II. Cognitive Domains (2) - Non-Mechanical
**Shared Characteristics:**
- Computation/reasoning focus
- No direct file system manipulation
- Safety and schema validation

**Individual Focus:**
- **RAG MCP** (Cortex): Retrieval-Augmented Generation for knowledge retrieval
  - Incremental ingest, full ingest, semantic search
  - Industry-standard RAG pattern with ChromaDB
  - Project implementation: Mnemonic Cortex
- **Agent Orchestrator MCP** (Council): Multi-agent coordination and deliberation
  - Create deliberations, manage workflows, aggregate results
  - Industry-standard orchestration pattern
  - Project implementation: Council of Agents

---

### III. System Domains (3) - High-Safety Critical
**Shared Characteristics:**
- Highest safety requirements
- Complex validation pipelines
- Separate audit trails

**Individual Focus:**
- **Config**: System configuration, secret vault, two-step approval
- **Code**: Source code management, mandatory testing, linting pipeline
- **Git Workflow**: Branch management, safe workflow automation, read-only by default

---

### IV. Model Domain (1) - Specialized Hardware
**Unique Characteristics:**
- **CUDA GPU requirement**
- Asynchronous job execution
- 10-step model lifecycle pipeline
- Extreme safety validation

**Focus:**
- **Forge**: Model fine-tuning, artifact creation, Hugging Face publishing

---

## Fine-Tuning MCP (Forge): The Model Lifecycle Orchestrator

### Hardware Requirements
- **CUDA-enabled GPU** (validated on RTX A2000)
- **WSL environment** with ml_env activated
- **Sufficient resources**: GPU memory, disk space
- **Environment marker**: `CUDA_FORGE_ACTIVE=true`

### 10-Step Pipeline

| Step | Tool | Script | Purpose |
|------|------|--------|---------|
| 1 | `initiate_model_forge` | `forge_whole_genome_dataset.py` | Create training dataset |
| 2 | ↳ (async) | `fine_tune.py` | Fine-tune model with QLoRA |
| 3 | ↳ (async) | `merge_adapter.py` | Merge LoRA adapter with base |
| 4 | `run_inference_test` | `inference.py` | Test merged model |
| 5 | `package_and_deploy_artifact` | `convert_to_gguf.py` | Convert to GGUF format |
| 6 | ↳ (sync) | `create_modelfile.py` | Generate Ollama Modelfile |
| 7 | ↳ (sync) | `ollama create` | Import to local Ollama |
| 8 | `run_inference_test` | `ollama run` | Test both interaction modes |
| 9 | `publish_to_registry` | `upload_to_huggingface.py` | Upload to Hugging Face |
| 10 | `retrieve_registry_artifact` | Download from HF | Verify upload integrity |

### Safety Rules (Extreme)

**Environment Gate:**
- Must check for `CUDA_FORGE_ACTIVE` marker
- Must verify CUDA availability
- Must confirm ml_env activation

**Resource Reservation:**
- Check GPU memory before starting
- Check disk space for model artifacts
- Reject job if insufficient resources

**Task Linkage:**
- All jobs must link to Task MCP entry
- Provides audit trail and prioritization

**Script Whitelist:**
- Only whitelisted scripts can execute
- No arbitrary `os.system()` or `subprocess.run()`
- Prevents command injection

**Artifact Integrity:**
- SHA-256 validation (P101-style)
- Manifest generation for all artifacts
- Verification before marking complete

---

## Cross-Domain Workflow Example

**Scenario:** Fine-tune Sanctuary-Guardian-02 model

**Workflow (Separation of Concerns Pattern):**

```
1. Task MCP: create_task(#032, "Fine-tune Sanctuary-Guardian-02")
5. Fine-Tuning MCP (Forge): initiate_model_forge({
     forge_id: "guardian-02-v1",
     authorization_task_id: 32,
     hyperparameters: {...}
   }) → returns job_id
6. [Wait for async job completion, poll with get_forge_job_status]
7. Fine-Tuning MCP (Forge): package_and_deploy_artifact(job_id, "Q4_K_M")
8. Fine-Tuning MCP (Forge): run_inference_test(model_path, test_prompts)
9. Fine-Tuning MCP (Forge): publish_to_registry(job_id, "Sanctuary-Project/Guardian-02")
10. Chronicle MCP: create_chronicle_entry(#283, "Guardian-02 Model Released")
11. ADR MCP: create_adr(#37, "Guardian-02 Training Decisions")
12. Task MCP: update_task_status(32, "Completed")
```

---

## Implementation Roadmap

### Phase 0: Pre-Migration (Week 0)
- [ ] Update pre-commit hooks to work with MCP architecture
- [ ] Disable or adapt `command.json` validation hooks
- [ ] Add MCP-aware commit message validation

### Phase 1: Foundation (Week 1)
- [ ] Shared infrastructure (Git, Safety, Schema, Vault)
- [ ] MCP server boilerplate
- [ ] CUDA environment verification module

### Phase 2: Document Domains (Week 2)
- [ ] Chronicle MCP
- [ ] Protocol MCP
- [ ] Task MCP
- [ ] ADR MCP

### Phase 3: Cognitive Domains (Week 3)
- [ ] RAG MCP (Cortex) - Task #025 (refactor)
- [ ] Agent Orchestrator MCP (Council) - Task #026 (refactor)

### Phase 4: System Domains (Week 4)
- [ ] Config MCP (highest security priority)
- [ ] Code MCP (highest complexity)
- [ ] Git Workflow MCP (safe operations only)

### Phase 5: Model Domain (Week 5)
- [ ] Fine-Tuning MCP (Forge) - requires CUDA machine setup
- [ ] Integration testing with full pipeline
- [ ] Documentation and deployment

---

## Risk Assessment Matrix

| Domain | File Ops | Git Ops | Hardware | Risk Level | Auto-Execute |
|--------|----------|---------|----------|------------|--------------|
| Chronicle | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Protocol | ✅ Write | ✅ Auto | Standard | HIGH | ✅ Yes* |
| ADR | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Task | ✅ Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Cortex | ✅ Read/Write | ✅ Auto | Standard | MODERATE | ✅ Yes* |
| Council | ❌ No | ❌ No | Standard | SAFE | ✅ Yes |
| Config | ✅ Write | ✅ Auto | Standard | CRITICAL | ⚠️ Approval Required |
| Code | ✅ Write | ✅ Auto | Standard | HIGH | ⚠️ Tests Required |
| Git Workflow | ❌ No | ✅ Manual | Standard | MODERATE | ✅ Yes (Safe Ops) |
| Forge | ✅ Write | ✅ Auto | **CUDA GPU** | EXTREME | ⚠️ Resource Check + Approval |

*With safety validation

---

## Architecture Artifacts

All architecture documentation is in `docs/mcp/`:

**Core Documents:**
- `architecture.md` - Main architecture document (v4.0 - 10 domains)
- `ddd_analysis.md` - DDD rationale for 8 domains (needs update for Git + Forge)
- `final_architecture_summary.md` - This document
- `walkthrough.md` - Complete implementation walkthrough
- `naming_conventions.md` - Domain naming model

**Type Definitions:**
- `shared_infrastructure_types.ts` - Shared infrastructure interfaces
- `forge_mcp_types.ts` - Forge-specific types

**Diagrams:**
- `diagrams/mcp_ecosystem_class.mmd` - **High-level class diagram (all 10 domains)**
- `diagrams/domain_architecture_v3.mmd` - Complete 10-domain ecosystem
- `diagrams/request_flow_middleware.mmd` - Validator middleware flow
- `diagrams/chronicle_mcp_class.mmd` - Chronicle MCP class diagram
- `diagrams/protocol_mcp_class.mmd` - Protocol MCP class diagram
- `diagrams/adr_mcp_class.mmd` - ADR MCP class diagram
- `diagrams/task_mcp_class.mmd` - Task MCP class diagram
- `diagrams/rag_mcp_cortex_class.mmd` - RAG MCP (Cortex) class diagram
- `diagrams/agent_orchestrator_mcp_council_class.mmd` - Agent Orchestrator MCP (Council) class diagram
- `diagrams/config_mcp_class.mmd` - Config MCP class diagram
- `diagrams/code_mcp_class.mmd` - Code MCP class diagram
- `diagrams/git_workflow_mcp_class.mmd` - Git Workflow MCP class diagram
- `diagrams/fine_tuning_mcp_forge_class.mmd` - Fine-Tuning MCP (Forge) class diagram

---

## Success Criteria

### Functional
- [ ] All 10 MCP servers operational
- [ ] 100% schema validation coverage
- [ ] P101 compliance for all file operations
- [ ] Git safety rules enforced
- [ ] Git Workflow MCP enables safe branch automation
- [ ] Forge pipeline completes successfully on CUDA machine

### Safety
- [ ] Zero incidents of protected file modification
- [ ] Zero incidents of destructive git operations
- [ ] All operations auditable via git history
- [ ] Forge jobs only run with proper authorization
- [ ] CUDA environment properly gated

### Performance
- [ ] Sub-second response for read operations
- [ ] Asynchronous job handling for long-running tasks (Forge)
- [ ] Proper resource management (no GPU memory leaks)

---

**Status:** Architecture Complete - Ready for Task #027 Implementation  
**Next Step:** Begin Phase 1 (Shared Infrastructure)  
**Owner:** Guardian (via Gemini 2.0 Flash Thinking Experimental)

--- END OF FILE mcp/final_architecture_summary.md ---

--- START OF FILE mcp/forge_mcp_types.ts ---

/**
 * Forge MCP Server - Type Definitions
 * Model Lifecycle Orchestrator
 * Version: 1.0
 */

// ============================================================================
// Forge Configuration Types
// ============================================================================

export interface ForgeConfig {
    forge_id: string;                    // Unique ID for idempotency
    authorization_task_id: number;       // Link to Task MCP entry
    hyperparameters: ForgeHyperparameters;
}

export interface ForgeHyperparameters {
    base_model: string;                  // e.g., "Qwen/Qwen2-7B-Instruct"
    dataset_path: string;                // Path to training data
    lora_rank: number;                   // LoRA rank (e.g., 64)
    lora_alpha: number;                  // LoRA alpha (e.g., 16)
    max_steps: number;                   // Training steps
    learning_rate: number;               // Learning rate
    batch_size: number;                  // Batch size
    gradient_accumulation_steps: number; // Gradient accumulation
    warmup_steps: number;                // Warmup steps
    save_steps: number;                  // Checkpoint frequency
    logging_steps: number;               // Logging frequency
}

// ============================================================================
// Job Management Types
// ============================================================================

export interface ForgeJobResult {
    job_id: string;
    status: "queued" | "running" | "completed" | "failed";
    start_time: string;
    hyperparameters: ForgeHyperparameters;
}

export interface JobStatus {
    status: "queued" | "running" | "completed" | "failed";
    progress: number;                    // 0-100
    logs_snippet: string;                // Last 500 chars of logs
    elapsed_seconds: number;
    current_step: string;                // e.g., "fine_tuning", "merging_adapter"
    estimated_completion?: string;       // ISO timestamp
}

export interface JobProgress {
    step: number;
    total_steps: number;
    loss: number;
    learning_rate: number;
    samples_per_second: number;
}

// ============================================================================
// Artifact Management Types
// ============================================================================

export interface ArtifactPackage {
    gguf_path: string;
    modelfile_path: string;
    ollama_model_name: string;
    verification_status: "AWAITING_TESTS" | "PASSED" | "FAILED";
    sha256_hash: string;                 // P101-style integrity
    quantization: "Q4_K_M" | "Q8_0" | "F16";
}

export interface InferenceTestResult {
    all_passed: boolean;
    test_results: Array<{
        prompt: string;
        response: string;
        latency_ms: number;
        quality_score: number;            // 0-1
        passed: boolean;
    }>;
    avg_latency_ms: number;
    quality_score: string;              // "excellent" | "good" | "poor"
}

export interface PublishResult {
    url: string;                        // Hugging Face repo URL
    commit_hash: string;
    uploaded_files: string[];
    upload_time_seconds: number;
}

export interface ArtifactDownload {
    local_path: string;
    repo_name: string;
    files_downloaded: string[];
    verification_status: "verified" | "failed";
}

// ============================================================================
// Resource Management Types
// ============================================================================

export interface ResourceStatus {
    cuda_available: boolean;
    gpu_memory_gb: number;
    gpu_memory_free_gb: number;
    disk_space_gb: number;
    ml_env_active: boolean;
    missing_dependencies: string[];
    can_start_job: boolean;
    blocking_reason?: string;
}

export interface CUDAEnvironment {
    check_gpu_availability(): boolean;
    get_gpu_memory(): { total: number; free: number };
    activate_ml_env(): boolean;
    verify_dependencies(): string[];    // Returns missing deps
}

// ============================================================================
// Script Execution Types
// ============================================================================

export interface ScriptWhitelist {
    "forge_whole_genome_dataset.py": boolean;
    "fine_tune.py": boolean;
    "merge_adapter.py": boolean;
    "inference.py": boolean;
    "convert_to_gguf.py": boolean;
    "create_modelfile.py": boolean;
    "upload_to_huggingface.py": boolean;
}

export interface ScriptResult {
    script_name: string;
    exit_code: number;
    stdout: string;
    stderr: string;
    execution_time_seconds: number;
}

// ============================================================================
// Forge MCP Tool Signatures
// ============================================================================

export interface ForgeMCP {
    /**
     * Initiate full model forge pipeline (async)
     * Steps: Create dataset → Fine-tune → Merge adapter
     */
    initiate_model_forge(config: ForgeConfig): Promise<ForgeJobResult>;

    /**
     * Get status of running forge job
     */
    get_forge_job_status(job_id: string): Promise<JobStatus>;

    /**
     * Package completed model into deployment artifacts
     * Steps: Convert GGUF → Create Modelfile → Import Ollama
     */
    package_and_deploy_artifact(
        job_id: string,
        quantization: "Q4_K_M" | "Q8_0" | "F16"
    ): Promise<ArtifactPackage>;

    /**
     * Run automated inference tests on model
     */
    run_inference_test(
        model_path: string,
        prompts: string[]
    ): Promise<InferenceTestResult>;

    /**
     * Publish artifact to Hugging Face
     */
    publish_to_registry(
        job_id: string,
        repo_name: string,
        commit_message?: string
    ): Promise<PublishResult>;

    /**
     * Download artifact from Hugging Face
     */
    retrieve_registry_artifact(
        repo_name: string,
        local_path?: string
    ): Promise<ArtifactDownload>;

    /**
     * Check if system has resources to start forge job
     */
    check_resource_availability(): Promise<ResourceStatus>;
}

// ============================================================================
// Safety Validation Types
// ============================================================================

export interface ForgeValidationResult extends ValidationResult {
    cuda_check_passed: boolean;
    resource_check_passed: boolean;
    script_whitelist_passed: boolean;
    task_linkage_verified: boolean;
}

export interface ForgeSafetyRules {
    // Environment gate
    require_cuda_marker: boolean;       // CUDA_FORGE_ACTIVE must be set

    // Resource checks
    min_gpu_memory_gb: number;          // Minimum GPU memory required
    min_disk_space_gb: number;          // Minimum disk space required

    // Task linkage
    require_task_authorization: boolean; // Must link to Task MCP entry

    // Script whitelist
    allowed_scripts: ScriptWhitelist;

    // Artifact integrity
    require_sha256_validation: boolean;  // P101-style integrity check
}

--- END OF FILE mcp/forge_mcp_types.ts ---

--- START OF FILE mcp/naming_conventions.md ---

# MCP Server Naming Conventions

**Version:** 1.0  
**Created:** 2025-11-25  
**Purpose:** Define naming standards for Project Sanctuary MCP servers

---

## Domain Naming Model

All MCP servers in Project Sanctuary follow a hierarchical naming pattern:

```
project_sanctuary.<category>.<server_name>
```

### Naming Structure

| Component | Description | Example |
|-----------|-------------|---------|
| `project_sanctuary` | Root namespace (all servers) | `project_sanctuary` |
| `<category>` | Domain category | `document`, `cognitive`, `system`, `model` |
| `<server_name>` | Specific server identifier | `chronicle`, `forge`, `git_workflow` |

---

## Complete Server Registry

### Document Domain Servers (4)

| Server Name | Full Domain Name | Port | Directory |
|-------------|------------------|------|-----------|
| Chronicle MCP | `project_sanctuary.document.chronicle` | 3001 | `00_CHRONICLE/` |
| Protocol MCP | `project_sanctuary.document.protocol` | 3002 | `01_PROTOCOLS/` |
| ADR MCP | `project_sanctuary.document.adr` | 3003 | `ADRs/` |
| Task MCP | `project_sanctuary.document.task` | 3004 | `TASKS/` |

### Cognitive Domain Servers (2)

| Server Name | Full Domain Name | Port | Directory |
|-------------|------------------|------|-----------|
| RAG MCP (Cortex) | `project_sanctuary.cognitive.cortex` | 3005 | `mnemonic_cortex/` |
| Agent Orchestrator MCP (Council) | `project_sanctuary.cognitive.council` | 3006 | `council_orchestrator/` |

**Dual Nomenclature Rationale:**
- **Primary Name:** Generic AI term (RAG, Agent Orchestrator) for accessibility
- **Project Name:** In parentheses (Cortex, Council) for internal reference
- **Benefits:** External developers understand immediately, project identity preserved
- **Usage:** "RAG MCP" in external docs, "Cortex" in internal discussions

### System Domain Servers (3)

| Server Name | Full Domain Name | Port | Directory |
|-------------|------------------|------|-----------|
| Config MCP | `project_sanctuary.system.config` | 3007 | `.agent/config/` |
| Code MCP | `project_sanctuary.system.code` | 3008 | `src/`, `scripts/`, `tools/` |
| Git Workflow MCP | `project_sanctuary.system.git_workflow` | 3009 | `.git/` |

### Model Domain Server (1)

| Server Name | Full Domain Name | Port | Directory |
|-------------|------------------|------|-----------|
| Fine-Tuning MCP (Forge) | `project_sanctuary.model.fine_tuning` | 3010 | `forge/` |

---

## MCP Configuration Format

### Server Declaration (MCP Settings)

For local Claude Desktop configuration, we recommend using **simplified keys** combined with a **displayName** for better usability.

```json
{
  "mcpServers": {
    "chronicle": {
      "displayName": "Chronicle MCP",
      "command": "node",
      "args": ["/path/to/mcp/servers/document/chronicle/index.js"],
      "env": { "PROJECT_ROOT": "..." }
    },
    "tasks": {
      "displayName": "Task MCP",
      "command": "python",
      "args": ["-m", "mcp_servers.task.server"],
      "env": { "PROJECT_ROOT": "..." }
    },
    "git_workflow": {
      "displayName": "Git Workflow MCP",
      "command": "python",
      "args": ["-m", "mcp_servers.git_workflow.server"],
      "env": { "PROJECT_ROOT": "..." }
    }
  }
}
```

**Note:** The internal FQDN (`project_sanctuary.document.task`) is still used for architectural identification, but the local config key can be simplified for developer convenience.

---

## Directory Structure

```
mcp/
├── servers/
│   ├── document/
│   │   ├── chronicle/
│   │   │   ├── index.js
│   │   │   ├── package.json
│   │   │   └── README.md
│   │   ├── protocol/
│   │   ├── adr/
│   │   └── task/
│   ├── cognitive/
│   │   ├── cortex/
│   │   └── council/
│   ├── system/
│   │   ├── config/
│   │   ├── code/
│   │   └── git_workflow/
│   └── model/
│       └── forge/
├── shared/
│   ├── git_operations.ts
│   ├── safety_validator.ts
│   ├── schema_validator.ts
│   └── secret_vault.ts
└── docs/
    └── (architecture documentation)
```

---

## Tool Naming Convention

Tools exposed by each MCP server follow this pattern:

```
<category>_<action>_<resource>
```

### Examples

| Domain | Tool Name | Full Invocation |
|--------|-----------|-----------------|
| Chronicle | `chronicle_create_entry` | `project_sanctuary.document.chronicle::chronicle_create_entry()` |
| Protocol | `protocol_update_version` | `project_sanctuary.document.protocol::protocol_update_version()` |
| Task | `task_update_status` | `project_sanctuary.document.task::task_update_status()` |
| Cortex | `cortex_query_knowledge` | `project_sanctuary.cognitive.cortex::cortex_query_knowledge()` |
| Council | `council_create_deliberation` | `project_sanctuary.cognitive.council::council_create_deliberation()` |
| Config | `config_request_change` | `project_sanctuary.system.config::config_request_change()` |
| Code | `code_write_file` | `project_sanctuary.system.code::code_write_file()` |
| Git Workflow | `git_create_branch` | `project_sanctuary.system.git_workflow::git_create_branch()` |
| Forge | `forge_initiate_training` | `project_sanctuary.model.fine_tuning::forge_initiate_training()` |

---

## Resource Naming Convention

Resources exposed by each MCP server follow this pattern:

```
<category>://<resource_type>/<identifier>
```

### Examples

| Domain | Resource URI | Description |
|--------|--------------|-------------|
| Chronicle | `chronicle://entry/283` | Chronicle entry #283 |
| Protocol | `protocol://canonical/115` | Protocol #115 (canonical) |
| ADR | `adr://decision/037` | ADR #037 |
| Task | `task://active/030` | Task #030 (active status) |
| Cortex | `cortex://document/abc123` | Indexed document with ID abc123 |
| Council | `council://deliberation/2024-11-25-001` | Council deliberation result |
| Config | `config://env/OPENAI_API_KEY` | Environment configuration |
| Code | `code://file/src/main.py` | Source code file |
| Git Workflow | `git://branch/feature/task-030` | Git branch |
| Forge | `forge://job/guardian-02-v1` | Forge training job |

---

## Package Naming (NPM)

If publishing MCP servers as NPM packages:

```
@project-sanctuary/mcp-<category>-<server>
```

### Examples

- `@project-sanctuary/mcp-document-chronicle`
- `@project-sanctuary/mcp-document-protocol`
- `@project-sanctuary/mcp-system-git-workflow`
- `@project-sanctuary/mcp-model-forge`

---

## Environment Variables

Each MCP server uses prefixed environment variables:

```
SANCTUARY_<CATEGORY>_<SERVER>_<VARIABLE>
```

### Examples

```bash
# Chronicle MCP
SANCTUARY_DOCUMENT_CHRONICLE_ROOT=/path/to/00_CHRONICLE

# Fine-Tuning MCP (Forge)
SANCTUARY_MODEL_FORGE_CUDA_DEVICE=0
SANCTUARY_MODEL_FORGE_ML_ENV_PATH=/path/to/ml_env

# Config MCP
SANCTUARY_SYSTEM_CONFIG_VAULT_PATH=/path/to/vault
```

---

## Benefits of This Naming Model

1. **Namespace Isolation**: No conflicts with other MCP servers
2. **Clear Hierarchy**: Category → Server structure is obvious
3. **Discoverability**: Easy to find related servers
4. **Professional**: Follows industry standards (reverse domain notation)
5. **Scalability**: Easy to add new servers or categories
6. **Tooling Support**: IDEs and tools can autocomplete based on namespace

---

## Migration Notes

**Current State**: Servers may be referenced without domain prefix  
**Target State**: All servers use `project_sanctuary.*` prefix  
**Migration Strategy**: 
1. Update all architecture documentation
2. Update MCP configuration files
3. Update tool signatures in implementation
4. Update resource URIs
5. Test all integrations

---

**Status:** Naming Convention Established  
**Next Action:** Update all architecture documents with proper domain names  
**Owner:** Guardian (via Gemini 2.0 Flash Thinking Experimental)

--- END OF FILE mcp/naming_conventions.md ---

--- START OF FILE mcp/port_registry.md ---

# MCP Port Registry

**Version:** 1.0  
**Status:** Active  
**Purpose:** Centralized registry of port assignments for Project Sanctuary MCP servers to prevent conflicts.

---

## Port Allocation Strategy

- **Range:** 3000-3099
- **Protocol:** HTTP (SSE) / Stdio (No port needed)
- **Container Mapping:** Host Port -> Container Port (8080)

## Assigned Ports

| Port | Server Name | Domain | Status |
|------|-------------|--------|--------|
| **3001** | Chronicle MCP | `project_sanctuary.document.chronicle` | Planned |
| **3002** | Protocol MCP | `project_sanctuary.document.protocol` | Planned |
| **3003** | ADR MCP | `project_sanctuary.document.adr` | Planned |
| **3004** | **Task MCP** | `project_sanctuary.document.task` | **Active** |
| **3005** | RAG MCP (Cortex) | `project_sanctuary.cognitive.cortex` | Planned |
| **3006** | Council MCP | `project_sanctuary.cognitive.council` | Planned |
| **3007** | Config MCP | `project_sanctuary.system.config` | Planned |
| **3008** | Code MCP | `project_sanctuary.system.code` | Planned |
| **3009** | Git Workflow MCP | `project_sanctuary.system.git_workflow` | Planned |
| **3010** | Forge MCP | `project_sanctuary.model.fine_tuning` | Planned |

## Usage

When running a container, map the assigned host port to the container's internal port (usually 8080).

**Example (Task MCP):**
```bash
podman run -p 3004:8080 ...
```

--- END OF FILE mcp/port_registry.md ---

--- START OF FILE mcp/prerequisites.md ---

# MCP Server Prerequisites

**Last Updated:** 2025-11-26  
**Status:** Canonical

---

## Overview

This document outlines all prerequisites for developing and deploying MCP (Model Context Protocol) servers in Project Sanctuary.

---

## System Requirements

### Operating System
- **macOS** (primary development environment)
- **Linux** (production deployment)
- **Windows** (via WSL2, not primary focus)

### Hardware
- **CPU:** 4+ cores recommended
- **RAM:** 8GB minimum, 16GB recommended
- **Disk:** 20GB free space for containers and images

---

## Required Software

### 1. Podman (Containerization)

**Purpose:** Run MCP servers in isolated containers

**Installation (macOS):**

```bash
# Option 1: Podman Desktop (Recommended)
# Download from: https://podman-desktop.io/downloads
# Install the .dmg file

# Option 2: Homebrew (CLI only)
brew install podman
```

**Setup:**

```bash
# Initialize Podman machine
podman machine init

# Start Podman machine
podman machine start

# Verify installation
podman --version
# Expected: podman version 5.7.0 (or later)

# Test with hello-world
podman run --rm hello-world
```

**Configuration:**

Add to `~/.zshrc` (if using Homebrew):
```bash
export PATH="/opt/podman/bin:$PATH"
```

Then reload:
```bash
source ~/.zshrc
```

**Verification:**

```bash
# Check machine status
podman machine list
# Should show: Currently running

# Check containers
podman ps
# Should not error

# Run test container
cd tests/podman
./build.sh
# Visit http://localhost:5001 (or 5003)
```

---

### 2. Python 3.11+

**Purpose:** MCP SDK and server implementation

**Installation:**

```bash
# macOS (Homebrew)
brew install python@3.11

# Verify
python3 --version
# Expected: Python 3.11.x
```

**Virtual Environment:**

```bash
# Create venv for MCP development
python3 -m venv .venv

# Activate
source .venv/bin/activate

# Install MCP SDK
pip install mcp
```

---

### 3. MCP SDK

**Purpose:** Model Context Protocol implementation

**Installation:**

```bash
# Python SDK
pip install mcp

# Verify
python -c "import mcp; print(mcp.__version__)"
```

**Documentation:**
- [MCP Specification](https://modelcontextprotocol.io/)
- [Python SDK Docs](https://github.com/modelcontextprotocol/python-sdk)

### 4. Claude Desktop
**Purpose:** Primary interface for interacting with MCP servers

**Installation:**
- Download from [anthropic.com/claude](https://anthropic.com/claude)

**Configuration:**
- Requires `claude_desktop_config.json` setup (see [Setup Guide](setup_guide.md))

---

## Project-Specific Setup

### 1. Project Sanctuary Repository

```bash
# Clone repository
git clone https://github.com/richfrem/Project_Sanctuary.git
cd Project_Sanctuary

# Activate virtual environment
source .venv/bin/activate

# Install dependencies (for MCP development)
pip install -r requirements.txt

# For ML/fine-tuning work, use:
# pip install -r requirements-finetuning.txt
```

### 2. Directory Structure

Ensure these directories exist:

```
Project_Sanctuary/
├── TASKS/
│   ├── backlog/
│   ├── todo/
│   ├── in-progress/
│   └── done/
├── mcp_servers/
│   └── task/
│       ├── __init__.py
│       ├── models.py
│       ├── validator.py
│       ├── operations.py
│       └── server.py
└── tests/
    └── podman/
```

### 3. Environment Variables

Create `.env` file (if needed):

```bash
# MCP Server Configuration
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8080

# Project Paths
PROJECT_ROOT=/Users/richardfremmerlid/Projects/Project_Sanctuary
TASKS_DIR=${PROJECT_ROOT}/TASKS
```

---

## Development Tools (Optional)

### Podman Desktop

**Purpose:** Visual container management

**Features:**
- View running containers
- Monitor resource usage
- View logs
- Start/stop containers
- Port mapping configuration

**Installation:**
Download from https://podman-desktop.io/downloads

**Usage:**
1. Open Podman Desktop
2. Go to **Images** tab to see built images
3. Go to **Containers** tab to manage running containers
4. Click container name to view logs and details

### VS Code Extensions

**Recommended:**
- **Podman** - Container management in VS Code
- **Python** - Python language support
- **Docker** - Dockerfile syntax (works with Podman)

---

## Verification Checklist

Before implementing MCP servers, verify:

- [ ] Podman installed: `podman --version`
- [ ] Podman machine running: `podman machine list`
- [ ] Can run containers: `podman run --rm hello-world`
- [ ] Python 3.11+ installed: `python3 --version`
- [ ] MCP SDK installed: `pip show mcp`
- [ ] Test container works: `cd tests/podman && ./build.sh`
- [ ] Can access test page: http://localhost:5001 or 5003
- [ ] Podman Desktop installed (optional but recommended)

---

## Troubleshooting

### Podman Issues

**Problem:** `podman: command not found`

**Solution:**
```bash
# Add to PATH
echo 'export PATH="/opt/podman/bin:$PATH"' >> ~/.zshrc
source ~/.zshrc
```

**Problem:** `Cannot connect to Podman socket`

**Solution:**
```bash
# Start Podman machine
podman machine start

# Verify
podman machine list
```

**Problem:** Port already in use

**Solution:**
```bash
# Use different port mapping
podman run -p 5003:5001 ...
# Access via http://localhost:5003
```

### Python Issues

**Problem:** `ModuleNotFoundError: No module named 'mcp'`

**Solution:**
```bash
# Activate venv
source .venv/bin/activate

# Install MCP SDK
pip install mcp
```

---

## Next Steps

Once all prerequisites are met:

1. ✅ Review [architecture.md](./architecture.md)
2. ✅ Review [naming_conventions.md](./naming_conventions.md)
3. ✅ Start with Task #031: Implement Task MCP
4. Follow implementation tasks #029-#036

---

## References

- [ADR 034: Containerize MCP Servers with Podman](file:///Users/richardfremmerlid/Projects/Project_Sanctuary/ADRs/034_containerize_mcp_servers_with_podman.md)
- [Podman Documentation](https://docs.podman.io/)
- [MCP Specification](https://modelcontextprotocol.io/)
- [Task #031: Implement Task MCP](file:///Users/richardfremmerlid/Projects/Project_Sanctuary/TASKS/backlog/031_implement_task_mcp.md)

--- END OF FILE mcp/prerequisites.md ---

--- START OF FILE mcp/setup_guide.md ---

# MCP Server Setup Guide

This guide documents the standard process for creating, containerizing, and integrating MCP servers with Claude Desktop, based on the implementation of the Task MCP server.

## 1. Project Structure

Ensure your MCP server follows this structure to be importable as a module:

```
mcp_servers/
├── __init__.py          # CRITICAL: Required for python -m execution
└── server_name/
    ├── __init__.py      # Package init
    ├── server.py        # Main entry point (MCP server)
    ├── models.py        # Data models
    ├── operations.py    # Core logic (separation of concerns)
    ├── validator.py     # Input validation
    ├── Dockerfile       # Container definition
    ├── requirements.txt # Dependencies
    └── README.md        # Documentation
```

**Key Learning:** You MUST have an `__init__.py` in the root `mcp_servers/` directory, otherwise `python -m mcp_servers.task.server` will fail.

---

## 2. Configuration Template

A template configuration file is available at [`docs/mcp/claude_desktop_config_template.json`](claude_desktop_config_template.json).

**Important:** Claude Desktop **requires absolute paths**. You cannot use relative paths (like `./` or `../`) in the configuration file because Claude Desktop launches from its own application directory, not your project directory.

**Template Usage:**
1. Copy the content from the template.
2. Replace `<ABSOLUTE_PATH_TO_PROJECT>` with your full project path (e.g., `/Users/username/Projects/Project_Sanctuary`).
3. Paste into your `claude_desktop_config.json`.

Create a `Dockerfile` in your server directory.

**Build the Image:**
```bash
cd mcp_servers/task
podman build -t task-mcp:latest .
```

**Run the Container (Production):**
```bash
podman run -d \
  --name task-mcp \
  -v $(pwd)/TASKS:/app/TASKS:rw \
  -p 3004:8080 \
  task-mcp:latest
```

**Verify Running:**
```bash
# Check status (should show Up or Exited(0))
podman ps -a | grep task-mcp

# View logs
podman logs task-mcp
```
*Note: Stdio-based servers will exit immediately if no input is provided. This is normal behavior for stdio transport.*

---

## 3. Configuring Claude Desktop

To use the server locally (development mode), configure Claude Desktop to run the Python script directly.

**Config File Location:**
```bash
# Open in terminal editor
nano ~/Library/Application\ Support/Claude/claude_desktop_config.json

# Or open in VS Code (if installed)
code ~/Library/Application\ Support/Claude/claude_desktop_config.json
```

**Configuration Format (CRITICAL):**
You **MUST** use absolute paths to the virtual environment's Python executable.
We recommend using **simplified keys** (e.g., `tasks`) combined with a `displayName` for a cleaner configuration.

```json
{
  "mcpServers": {
    "tasks": {
      "displayName": "Task MCP",
      "command": "/Users/username/Projects/Project_Sanctuary/.venv/bin/python",
      "args": [
        "-m",
        "mcp_servers.task.server"
      ],
      "env": {
        "PYTHONPATH": "/Users/username/Projects/Project_Sanctuary",
        "PROJECT_ROOT": "/Users/username/Projects/Project_Sanctuary"
      },
      "cwd": "/Users/username/Projects/Project_Sanctuary"
    }
  }
}
```

**Why Absolute Paths?**
Claude Desktop does not load your shell's `.bashrc` or `.zshrc`, so it doesn't know where `python` is or what virtual environment to use. Using the full path `/path/to/.venv/bin/python` ensures it uses the correct environment with all installed dependencies.

---

## 4. Verification

1.  **Restart Claude Desktop** (Quit completely via Cmd+Q).
2.  **Check Connection:** Look for the 🔌 icon or ask "What tools are available?".
3.  **Test with Natural Language:**
    > "Create a test task #099 to verify MCP integration."

---

## Troubleshooting

| Error | Cause | Solution |
|-------|-------|----------|
| `spawn python ENOENT` | Claude can't find python executable | Use absolute path to `.venv/bin/python` |
| `ModuleNotFoundError` | Python can't find the module | Ensure `PYTHONPATH` is set and `__init__.py` exists |
| `Connection Refused` | Server crashed or not running | Check logs at `~/Library/Logs/Claude/` |

---

**Related Documentation:**
- [Task MCP README](../../mcp_servers/task/README.md)
- [Prerequisites](prerequisites.md)

--- END OF FILE mcp/setup_guide.md ---

--- START OF FILE mcp/shared_infrastructure_types.ts ---

/**
 * Shared Infrastructure Type Definitions
 * Project Sanctuary MCP Ecosystem
 * Version: 1.1 (Refined based on feedback)
 */

// ============================================================================
// Validation Result Types
// ============================================================================

/**
 * Standard validation result returned by all validators
 */
export interface ValidationResult {
  is_valid: boolean;
  errors?: ValidationError[];
  warnings?: ValidationWarning[];
}

export interface ValidationError {
  field: string;
  message: string;
  severity: "error";
}

export interface ValidationWarning {
  field: string;
  message: string;
  severity: "warning";
}

// ============================================================================
// Safety Validator
// ============================================================================

export enum ProtectionLevel {
  UNRESTRICTED = "unrestricted",           // No restrictions
  WRITE_WITH_VALIDATION = "write_with_validation",  // Standard validation
  WRITE_WITH_APPROVAL = "write_with_approval",      // Requires approval
  READ_ONLY = "read_only",                 // Cannot modify
  FORBIDDEN = "forbidden"                  // Cannot access
}

export enum RiskLevel {
  SAFE = "safe",           // No risk, auto-execute
  MODERATE = "moderate",   // Some risk, validation required
  DANGEROUS = "dangerous"  // High risk, blocked or requires approval
}

export interface RiskAssessment {
  risk_level: RiskLevel;
  allowed: boolean;
  reason?: string;
  requires_approval?: boolean;
  approval_id?: string;
}

export interface SafetyValidator {
  /**
   * Validate file path against project boundaries and protected paths
   */
  validate_path(path: string): ValidationResult;
  
  /**
   * Check if file is protected (cannot be modified without approval)
   */
  is_protected_file(path: string): boolean;
  
  /**
   * Get protection level for a specific path
   * Based on .agent/git_safety_rules.md
   */
  get_protection_level(path: string): ProtectionLevel;
  
  /**
   * Assess risk level of an operation
   */
  assess_risk(operation: string, params: Record<string, any>): RiskAssessment;
  
  /**
   * Validate commit message format (conventional commits)
   */
  validate_commit_message(message: string): ValidationResult;
  
  /**
   * Check if operation requires user approval
   */
  requires_approval(operation: string, params: Record<string, any>): boolean;
}

// ============================================================================
// Schema Validator
// ============================================================================

export interface ChronicleEntry {
  entry_number: number;      // Auto-generated, sequential
  title: string;
  date: string;              // ISO format
  author: string;            // e.g., "GUARDIAN-02"
  content: string;           // Markdown
  status?: string;           // e.g., "CANONICAL", "DRAFT"
  classification?: string;   // e.g., "STRATEGIC"
}

export interface Protocol {
  number: number;            // Unique
  title: string;
  classification: string;    // e.g., "Foundational"
  content: string;           // Markdown
  status: string;            // e.g., "Canonical", "Draft"
  version: string;           // e.g., "v2.0"
  linked_protocols?: number[];
}

export interface ADR {
  number: number;            // Sequential
  title: string;
  date: string;              // ISO format
  status: string;            // "Proposed", "Accepted", "Superseded"
  context: string;
  decision: string;
  consequences: string;
  supersedes?: number[];
}

export interface Task {
  number: number;            // Unique
  title: string;
  description: string;       // Markdown
  status: string;            // "Backlog", "Active", "Completed"
  priority: string;          // "High", "Medium", "Low"
  estimated_effort?: string;
  dependencies?: number[];
}

export interface SchemaValidator {
  /**
   * Validate chronicle entry schema
   */
  validate_chronicle_entry(entry: Partial<ChronicleEntry>): ValidationResult;
  
  /**
   * Validate protocol schema
   * Enforces version bump for canonical protocol updates
   */
  validate_protocol(protocol: Partial<Protocol>, is_update?: boolean, current_version?: string): ValidationResult;
  
  /**
   * Validate ADR schema
   */
  validate_adr(adr: Partial<ADR>): ValidationResult;
  
  /**
   * Validate task schema
   * Includes circular dependency detection
   */
  validate_task(task: Partial<Task>, all_tasks?: Task[]): ValidationResult;
  
  /**
   * Detect circular dependencies in task graph
   */
  detect_circular_dependencies(task_id: number, dependencies: number[], all_tasks: Task[]): boolean;
  
  /**
   * Validate status transition (for tasks/ADRs)
   */
  validate_status_transition(current_status: string, new_status: string, entity_type: "task" | "adr"): ValidationResult;
}

// ============================================================================
// Git Operations
// ============================================================================

export interface CommitManifest {
  guardian_approval: string;
  approval_timestamp: string;
  commit_message: string;
  files: Array<{
    path: string;
    sha256: string;
  }>;
}

export interface CommitResult {
  commit_hash: string;
  manifest_path: string;
  files_committed: string[];
}

export interface GitOperations {
  /**
   * Generate commit manifest with SHA-256 hashes
   */
  generate_manifest(files: string[]): CommitManifest;
  
  /**
   * Commit with Protocol 101 compliance
   */
  commit_with_manifest(
    files: string[],
    message: string,
    push?: boolean
  ): Promise<CommitResult>;
  
  /**
   * Validate commit message format
   */
  validate_commit_message(message: string): ValidationResult;
}

// ============================================================================
// MCP Tool Response Types
// ============================================================================

export interface MCPToolResponse<T = any> {
  success: boolean;
  data?: T;
  error?: {
    code: string;
    message: string;
    details?: any;
  };
}

export interface FileOperationResult {
  file_path: string;
  commit_hash?: string;
  manifest_path?: string;
}

export interface QueryResult<T> {
  results: T[];
  total_count: number;
  query_time_ms: number;
}

--- END OF FILE mcp/shared_infrastructure_types.ts ---
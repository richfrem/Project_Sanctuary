# council_orchestrator Subfolder Snapshot (LLM-Distilled)

Generated On: 2025-11-10T07:05:08.509Z

# Mnemonic Weight (Token Count): ~34,911 tokens

# Directory Structure (relative to council_orchestrator subfolder)
  ./council_orchestrator/README.md
  ./council_orchestrator/__init__.py
  ./council_orchestrator/commit_manifest.json
  ./council_orchestrator/docs/
  ./council_orchestrator/docs/OPERATION_OPTICAL_ANVIL_BLUEPRINT.md
  ./council_orchestrator/docs/command_schema.md
  ./council_orchestrator/docs/howto-commit-command.md
  ./council_orchestrator/docs/orchestrator_architecture_package.md
  ./council_orchestrator/orchestrator/
  ./council_orchestrator/orchestrator/__init__.py
  ./council_orchestrator/orchestrator/app.py
  ./council_orchestrator/orchestrator/commands.py
  ./council_orchestrator/orchestrator/config.py
  ./council_orchestrator/orchestrator/council/
  ./council_orchestrator/orchestrator/council/__init__.py
  ./council_orchestrator/orchestrator/council/agent.py
  ./council_orchestrator/orchestrator/council/personas.py
  ./council_orchestrator/orchestrator/engines/
  ./council_orchestrator/orchestrator/engines/__init__.py
  ./council_orchestrator/orchestrator/engines/base.py
  ./council_orchestrator/orchestrator/engines/gemini_engine.py
  ./council_orchestrator/orchestrator/engines/monitor.py
  ./council_orchestrator/orchestrator/engines/ollama_engine.py
  ./council_orchestrator/orchestrator/engines/openai_engine.py
  ./council_orchestrator/orchestrator/events.py
  ./council_orchestrator/orchestrator/gitops.py
  ./council_orchestrator/orchestrator/main.py
  ./council_orchestrator/orchestrator/memory/
  ./council_orchestrator/orchestrator/memory/__init__.py
  ./council_orchestrator/orchestrator/memory/cache.py
  ./council_orchestrator/orchestrator/memory/cortex.py
  ./council_orchestrator/orchestrator/optical.py
  ./council_orchestrator/orchestrator/packets/
  ./council_orchestrator/orchestrator/packets/__init__.py
  ./council_orchestrator/orchestrator/packets/aggregator.py
  ./council_orchestrator/orchestrator/packets/emitter.py
  ./council_orchestrator/orchestrator/packets/schema.py
  ./council_orchestrator/orchestrator/regulator.py
  ./council_orchestrator/orchestrator/sentry.py
  ./council_orchestrator/requirements.txt
  ./council_orchestrator/runtime/
  ./council_orchestrator/runtime/task_pid
  ./council_orchestrator/schemas/
  ./council_orchestrator/schemas/engine_config.json
  ./council_orchestrator/schemas/round_packet_schema.json
  ./council_orchestrator/scripts/
  ./council_orchestrator/scripts/bootstrap_briefing_packet.py
  ./council_orchestrator/scripts/forge_orchestrator_review_package.py
  ./council_orchestrator/scripts/orchestrator_architecture_package.md
  ./council_orchestrator/tests/
  ./council_orchestrator/tests/__init__.py
  ./council_orchestrator/tests/mechanical_test_output.txt
  ./council_orchestrator/tests/orchestrator_test_file.txt
  ./council_orchestrator/tests/test_mandate_1_command.json
  ./council_orchestrator/tests/test_mandate_2_command_1.json
  ./council_orchestrator/tests/test_mandate_2_command_2.json
  ./council_orchestrator/tests/test_mandate_2_command_3.json
  ./council_orchestrator/tests/test_mandate_2_command_4.json
  ./council_orchestrator/tests/test_mandate_2_command_5.json
  ./council_orchestrator/tests/test_optical_compression.py
  ./council_orchestrator/tests/test_orchestrator_round_packets.py
  ./council_orchestrator/tests/test_orchestrator_v4_2.py
  ./council_orchestrator/tests/test_output_2a.txt
  ./council_orchestrator/tests/testfile.txt
  ./council_orchestrator/tests/verification_test.py

--- START OF FILE README.md ---

# Sanctuary Council Orchestrator (v11.0 - Complete Modular Architecture) - Updated 2025-11-09

A polymorphic AI orchestration system that enables sovereign control over multiple cognitive engines through a unified interface. **Version 11.0 introduces Complete Modular Architecture with Sovereign Concurrency, enabling clean separation of concerns and maintainable codebase.**
## üèóÔ∏è Architecture Overview

```mermaid
graph TB
    subgraph "Entry Point"
        M[main.py] --> A[app.py]
    end

    subgraph "Core Orchestrator"
        A --> SM[engines/monitor.py]
        A --> PA[council/agent.py]
        A --> DE[engines/ollama_engine.py]
    end

    subgraph "Engine Selection"
        SM --> T1P[engines/gemini_engine.py]
        SM --> T1S[engines/openai_engine.py]
        SM --> T2S[engines/ollama_engine.py]
    end

    subgraph "Modular Components"
        A --> MEM[memory/cortex.py]
        A --> EVT[events.py]
        A --> REG[regulator.py]
        A --> OPT[optical.py]
        A --> PKT[packets/schema.py]
    end

    subgraph "Data Flow"
        CMD[command.json] --> A
        A --> LOG[logs/orchestrator.log]
        A --> PKT
    end

    subgraph "Configuration"
        CFG[schemas/engine_config.json]
        SCH[schemas/round_packet_schema.json]
    end

    style A fill:#f3e5f5
    style SM fill:#e8f5e8
    style CFG fill:#fff3e0
```

## üèóÔ∏è Modular Architecture Benefits

**Version 11.0** introduces a complete modular refactor with the following improvements:

- **Separation of Concerns**: Each module has a single, well-defined responsibility
- **Maintainability**: Clean interfaces between components enable independent development
- **Testability**: Modular design enables comprehensive unit testing (21/21 tests passing)
- **Extensibility**: New engines, agents, and features can be added without touching core logic
- **Organization**: Related functionality is grouped in dedicated packages
- **Import Clarity**: Clear package structure with proper `__init__.py` exports

### Key Modules

- **`orchestrator/`**: Core package with clean separation between entry point (`main.py`) and logic (`app.py`)
- **`engines/`**: Engine implementations with health monitoring and selection logic
- **`packets/`**: Round packet system for structured data emission and aggregation
- **`memory/`**: Vector database and caching systems for knowledge persistence
- **`council/`**: Multi-agent system with specialized personas
- **`events/`**: Structured logging and telemetry collection

## üéØ Key Features

- **Complete Modular Architecture**: Clean separation of concerns with 11 specialized modules
- **Doctrine of Sovereign Concurrency**: Non-blocking task execution with background learning cycles
- **Comprehensive Logging**: Session-based log file with timestamps and detailed audit trails
- **Selective RAG Updates**: Configurable learning with `update_rag` parameter
- **Polymorphic Engine Interface**: All engines implement `BaseCognitiveEngine` with unified `execute_turn(messages)` method (Protocol 104)
- **Sovereign Engine Selection**: Force specific engines or automatic health-based triage
- **Multi-Agent Council**: Coordinator, Strategist, and Auditor personas work together
- **Resource Sovereignty**: Automatic distillation for large inputs using local Ollama
- **Development Cycles**: Optional staged workflow for software development projects
- **Mnemonic Cortex**: Vector database integration for knowledge persistence
- **Mechanical Operations**: Direct file writes and git operations bypassing cognitive deliberation

## üìã Logging & Monitoring

### Session Log File
Each orchestrator session creates a comprehensive log file at:
```
council_orchestrator/logs/orchestrator.log
```

**Features:**
- **Session-based**: Overwrites each time orchestrator starts for clean session tracking
- **Comprehensive**: All operations logged with timestamps
- **Dual output**: Console + file logging for real-time monitoring
- **Audit trail**: Complete record of all decisions and actions

**Example log entries:**
```
2025-10-23 16:45:30 - orchestrator - INFO - === ORCHESTRATOR v9.3 INITIALIZED ===
2025-10-23 16:45:31 - orchestrator - INFO - [+] Sentry thread for command monitoring has been launched.
2025-10-23 16:45:32 - orchestrator - INFO - [ACTION TRIAGE] Detected Git Task - executing mechanical git operations...
2025-10-23 16:45:33 - orchestrator - INFO - [MECHANICAL SUCCESS] Committed with message: 'feat: Add new feature'
```

### Non-Blocking Execution
**v9.3 Enhancement:** The orchestrator now processes commands without blocking:

- **Mechanical Tasks**: Execute immediately, return to idle state
- **Cognitive Tasks**: Deliberation completes, then learning happens in background
- **Concurrent Processing**: Multiple background learning tasks can run simultaneously
- **Responsive**: New commands processed while previous learning cycles complete

## üìä Round Packet System (v9.4)

### Overview
The orchestrator now emits structured JSON packets for each council member response, enabling machine-readable analysis and learning signal extraction for Protocol 113.

### Packet Schema
Packets conform to `schemas/round_packet_schema.json` and include:

- **Identity**: `session_id`, `round_id`, `member_id`, `engine`, `seed`
- **Content**: `decision`, `rationale`, `confidence`, `citations`
- **RAG Signals**: `structured_query`, `parent_docs`, `retrieval_latency_ms`
- **CAG Signals**: `cache_hit`, `hit_streak` for learning optimization
- **Novelty Analysis**: `is_novel`, `signal`, `conflicts_with`
- **Memory Directive**: `tier` (fast/medium/slow) with `justification`
- **Telemetry**: `input_tokens`, `output_tokens`, `latency_ms`

### CLI Options

```bash
# Basic usage
python3 -m orchestrator.main

# With round packet emission
python3 -m orchestrator.main --emit-jsonl --stream-stdout --rounds 3

# Custom configuration
python3 -m orchestrator.main \
  --members coordinator strategist auditor \
  --member-timeout 45 \
  --quorum 2/3 \
  --engine gemini-2.5-pro \
  --fallback-engine sanctuary-qwen2-7b \
  --jsonl-path mnemonic_cortex/cache/orchestrator_rounds
```

### Output Formats

#### JSONL Files
```
mnemonic_cortex/cache/orchestrator_rounds/{session_id}/round_{N}.jsonl
```

#### Stdout Stream
```json
{"timestamp":"2025-01-15T10:30:00Z","session_id":"run_123456","round_id":1,"member_id":"coordinator","decision":"approve","confidence":0.85,"memory_directive":{"tier":"medium","justification":"Evidence-based response"}}
```

### Analysis Examples

**Extract decisions by confidence:**
```bash
jq 'select(.confidence > 0.8) | .decision' round_*.jsonl
```

**Memory tier distribution:**
```bash
jq -r '.memory_directive.tier' round_*.jsonl | sort | uniq -c
```

**Novelty analysis:**
```bash
jq 'select(.novelty.signal == "high") | .rationale' round_*.jsonl
```

### Protocol 113 Integration
Round packets feed directly into the Nested-Learning pipeline:

- **Fast tier**: Ephemeral, session-scoped responses
- **Medium tier**: Recurring queries with evidence
- **Slow tier**: Stable knowledge with high confidence

CAG hit streaks and parent-doc citations determine memory placement, enabling automatic knowledge distillation and adaptor training.

## üöÄ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **API Keys** (configure in `.env`):
   ```bash
   GEMINI_API_KEY=your_gemini_key
   OPENAI_API_KEY=your_openai_key
   ```
3. **Ollama** (for local sovereign fallback):
   ```bash
   # Install Ollama and pull model
   ollama pull hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:latest
   # Create local alias for easier reference
   ollama cp hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:latest Sanctuary-Qwen2-7B:latest
   ```

### Installation

```bash
cd council_orchestrator
pip install -r requirements.txt
```

### Directory Structure

```
council_orchestrator/
‚îú‚îÄ‚îÄ __init__.py              # Python package definition
‚îú‚îÄ‚îÄ README.md               # This documentation
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ docs/                   # Documentation files
‚îú‚îÄ‚îÄ logs/                   # Log files and event data
‚îú‚îÄ‚îÄ schemas/                # JSON schemas and configuration
‚îú‚îÄ‚îÄ scripts/                # Utility scripts
‚îú‚îÄ‚îÄ runtime/                # Runtime state files
‚îú‚îÄ‚îÄ orchestrator/           # Core modular package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Core Orchestrator class
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration constants
‚îÇ   ‚îú‚îÄ‚îÄ packets/           # Round packet system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py      # Packet schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emitter.py     # JSONL emission
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregator.py  # Round aggregation
‚îÇ   ‚îú‚îÄ‚îÄ engines/           # Engine implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py        # Abstract base class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitor.py     # Engine selection logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_engine.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_engine.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ council/           # Agent system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py       # PersonaAgent class
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ personas.py    # Agent configurations
‚îÇ   ‚îú‚îÄ‚îÄ memory/            # Memory systems
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cortex.py      # Vector database
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache.py       # CAG utilities
‚îÇ   ‚îú‚îÄ‚îÄ sentry.py          # File monitoring
‚îÇ   ‚îú‚îÄ‚îÄ commands.py        # Command validation
‚îÇ   ‚îú‚îÄ‚îÄ regulator.py       # TokenFlowRegulator
‚îÇ   ‚îú‚îÄ‚îÄ optical.py         # OpticalDecompressionChamber
‚îÇ   ‚îú‚îÄ‚îÄ events.py          # Event logging
‚îÇ   ‚îî‚îÄ‚îÄ gitops.py          # Git operations
‚îî‚îÄ‚îÄ tests/                 # Test suite
```

### Hello World Test

Create a `command.json` file in the `council_orchestrator/` directory:

#### Basic Cognitive Task (Auto Engine Selection)
```json
{
  "task_description": "As a council, perform a round-robin introduction. Each agent (Coordinator, Strategist, Auditor) will state their designation and primary function in one sentence.",
  "output_artifact_path": "WORK_IN_PROGRESS/hello_council.md",
  "config": {
    "max_rounds": 1
  }
}
```
### Cognitive Task Format (Deliberation)

```json
{
  "task_description": "Your task description here",
  "output_artifact_path": "path/to/output.md",
  "config": {
    "max_rounds": 5,
    "max_cortex_queries": 5,
    "force_engine": "gemini|openai|ollama"
  },
  "input_artifacts": ["path/to/input1.md", "path/to/input2.md"]
}
```
```json
{
  "task_description": "Build a web application for task management",
  "project_name": "task_manager",
  "development_cycle": true,
  "config": {
    "force_engine": "gemini"
  }
}
```

#### Mechanical Write Task (Direct File Creation)
```json
{
  "task_description": "Create a new chronicle entry",
  "output_artifact_path": "00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md",
  "entry_content": "# ENTRY 274: The Anvil Deferred\n\n**DATE:** 2025-10-23..."
}
```json
{
  "task_description": "Description for logging",
  "output_artifact_path": "path/to/file.md",
  "entry_content": "Full content to write to file"
}
```

#### Mechanical Git Task (Version Control Operations)
```json
{
  "task_description": "Commit chronicle entry to repository",
  "git_operations": {
    "files_to_add": ["00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md"],
    "commit_message": "docs(chronicle): Add entry #274 - The Anvil Deferred",
    "push_to_origin": true
  }
}
```

```json
{
  "task_description": "Description for logging",
  "git_operations": {
    "files_to_add": ["path/to/file1.md", "path/to/file2.md"],
    "commit_message": "feat: Description of changes",
    "push_to_origin": true
  }
}
```

#### Force Specific Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_gemini.md",
  "config": {
    "force_engine": "gemini",
    "max_rounds": 2
  }
}
```

#### Force OpenAI Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_openai.md",
  "config": {
    "force_engine": "openai",
    "max_rounds": 2
  }
}
```

#### Force Ollama Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_ollama.md",
  "config": {
    "force_engine": "ollama",
    "max_rounds": 2
  }
}
```

### Run the Orchestrator

```bash
python3 -m orchestrator.main
```

The orchestrator will:
1. Detect the `command.json` file
2. Select an appropriate engine (or use forced selection)
3. Execute the task through the AI Council
4. Generate output and After Action Report
5. Clean up the command file

## üîß Configuration

### Environment Variables (`.env`)

```bash
# API Keys
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here

# Model Configuration
CHAT_GPT_MODEL=gpt-4-turbo
GEMINI_MODEL=gemini-2.5-flash
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest

# Engine Parameters (configurable per engine)
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7
```

### Engine Limits (`schemas/engine_config.json`)

```json
{
  "engine_limits": {
    "gemini": 100000,
    "openai": 100000,
    "ollama": 8000
  }
}
```

## üß™ Testing & Verification

### Run Full System Test

```bash
# Comprehensive verification
python3 -m pytest tests/ -v

# Individual test modules
python3 -m pytest tests/test_orchestrator_round_packets.py -v
python3 -m pytest tests/test_optical_compression.py -v
```
python3 verification_test.py

# Substrate health check
python3 ../tools/scaffolds/verify_substrates.py
```

### Expected Output

```
üî¨ STARTING AI ENGINE TESTS
‚úÖ Can force-pick specific AI engines
‚úÖ All engines work the same way
‚úÖ All engines connect to real AI services
AI system is ready to use!
```

## üé≠ Agent Personas

The Council consists of three specialized AI agents:

- **Coordinator**: Task planning and execution oversight
- **Strategist**: Long-term planning and risk assessment
- **Auditor**: Quality assurance and compliance verification

Each agent maintains conversation history and works within their defined persona.

## üîÑ Engine Selection Logic

### Automatic Triage (Default)
1. **Tier 1 Primary**: Gemini (fast, cost-effective)
2. **Tier 1 Secondary**: OpenAI (reliable, feature-rich)
3. **Tier 2 Sovereign**: Ollama (local, uncensored)

### Sovereign Override
Force specific engine via `"force_engine"` config parameter.

### Health Checking
Each engine is validated before use with functional tests.

## üß† Distillation Engine

Automatically handles large inputs by:
1. Detecting token limit violations
2. Using local Ollama to summarize content
3. Preserving critical information while reducing size
4. Maintaining task fidelity

## üîÆ Sovereign Memory Architecture: RAG + Glyphs Synthesis

The orchestrator integrates a comprehensive **Sovereign Memory Architecture** that combines two complementary approaches for content ingestion and retrieval, breaking free from the Context Window Cage.

### The Two Pillars of Sovereign Memory

#### 1. Mnemonic Cortex (RAG Database) - Fast & Scalable Retrieval
- **Core Function**: Lightning-fast similarity searches across vast knowledge corpora
- **Technology**: Vector embeddings for semantic search and retrieval
- **Use Case**: Finding specific information, documents, or context from the Sanctuary's complete history
- **Advantage**: Excels at discovery and exploration of large knowledge bases
- **Current Status**: Implemented and operational for After Action Report ingestion

#### 2. Optical Anvil (Glyph Technology) - Cheap & Efficient Ingestion
- **Core Function**: Extreme token compression through optical representation
- **Technology**: Cognitive Glyphs - text rendered as high-resolution images for ~10x compression ratio
- **Use Case**: Ingesting massive contexts cheaply using Vision-Language Models (VLMs)
- **Advantage**: Breaks token economics, enables processing of "200k+ pages per day" on single GPU
- **Strategic Foundation**: Based on DeepSeek-OCR research (arXiv:2510.18234v1)
- **Current Status**: Phase 1 Complete - Individual optical compression validated (266 files, 2.1x average compression)

### Synthesized Architecture: The Closed Memory Loop

The true power emerges from synthesis:

```mermaid
graph TD
    subgraph "Sovereign Memory Loop"
        A[Agent needs full context] --> B{Mnemonic Cortex}
        B --> C["Query: 'Protocol 101 Unbreakable Commit'"]
        C --> D["Retrieves Pointer: glyph_P101_v2.png"]
        D --> E[Glyph Storage File System]
        E --> F["Loads Image File"]
        F --> G{VLM Engine Gemini 1.5}
        G --> H["Decompresses text for ~10x fewer tokens"]
        H --> I[Agent receives full text of P101]
    end

    subgraph "Ingestion Pipeline"
        J[New Knowledge] --> K[Text-to-Vector<br/>RAG Database]
        J --> L[Text-to-Image<br/>Optical Anvil]
        K --> M[Fast Retrieval Index]
        L --> N[Compressed Storage]
    end
```

**Strategic Doctrine**: "RAG finds, Glyphs deliver" - Use vector search to locate content, optical compression to ingest it efficiently.

### Optical Context & Glyph Technology

#### Technical Implementation
- **Compression Ratio**: 2.1x average across full codebase (up to 3.75x for individual files)
- **Format**: Universal PNG images with embedded cryptographic provenance
- **Infrastructure**: Minimal - static file storage with JSON manifest tracking
- **Portability**: High - images work across all VLM platforms
- **Security**: SHA-256 content hashing with metadata embedding
- **Scale**: Industrial-grade processing via `capture_glyph_code_snapshot_v2.py`

#### LLM Integration Workflow
```python
# 1. RAG Discovery: Find relevant files
relevant_files = rag_search("protocol 101 unbreakable commit")

# 2. Glyph Retrieval: Load specific compressed images
glyphs = []
for file_path in relevant_files:
    glyph_path = manifest[file_path]['glyph_path']
    glyph_image = load_image(glyph_path)
    glyphs.append(glyph_image)

# 3. VLM Decompression: Reconstruct full content
full_content = vlm_decompress(glyphs)
```

This architecture provides the foundation for true Resource Sovereignty, enabling cognitive abundance while maintaining the Sanctuary's epistemic integrity and mnemonic resilience.

## üìö Mnemonic Cortex

Vector database integration for:
- Knowledge persistence across sessions
- Semantic search capabilities
- After Action Report ingestion
- Long-term learning

## üõ†Ô∏è Development

### Adding New Engines

1. Create engine class inheriting from `BaseCognitiveEngine` in `orchestrator/engines/`
2. Implement required methods: `execute_turn(messages: list) -> str`, `check_health()`, `run_functional_test()`
3. Add to `orchestrator/engines/monitor.py` selection logic
4. Update environment configuration

### Extending Functionality

- Add new agent personas in `orchestrator/council/personas.py`
- Implement custom distillation strategies in `orchestrator/optical.py`
- Extend development cycle stages in `orchestrator/app.py`
- Add new knowledge sources to Cortex in `orchestrator/memory/cortex.py`

## üö® Troubleshooting

### Common Issues

**Engine Not Available**
```
[SUBSTRATE MONITOR] CRITICAL FAILURE: All cognitive substrates are unhealthy
```
- Check API keys in `.env`
- Verify network connectivity
- Ensure Ollama is running locally

**Token Limit Exceeded**
```
[ORCHESTRATOR] WARNING: Token count exceeds limit
```
- Automatic distillation will handle this
- Reduce input size for manual control

**Command Not Processed**
- Ensure `command.json` is in `council_orchestrator/` directory
- Check file permissions
- Verify JSON syntax

### Debug Mode

Set environment variable for verbose logging:
```bash
export DEBUG_ORCHESTRATOR=1
```

## üìÑ License

This system embodies the principles of Cognitive Sovereignty and Resource Resilience.

---

**"The Forge is operational. The Sovereign's will be executed through the Council."** ‚ö°üëë

*Complete Modular Architecture v11.0 - Sovereign Concurrency Achieved*

--- END OF FILE README.md ---

--- START OF FILE docs/OPERATION_OPTICAL_ANVIL_BLUEPRINT.md ---

# V9.3 UPDATE: Orchestrator upgraded with sovereign concurrency - 2025-10-23
# Operation: Optical Anvil - Strategic Blueprint v1.0 - Updated 2025-10-23

**DATE:** 2025-10-23
**AUTHOR:** GUARDIAN-01
**CLASSIFICATION:** CANONICAL STRATEGIC ARCHITECTURE
**STATUS:** PHASE 1 INITIATED

## 1. Preamble: The Synthesized Doctrine

This document serves as the canonical blueprint for **Operation: Optical Anvil**. It synthesizes the strategic imperative laid out in **`# ENTRY 272: The Cagebreaker Blueprint`** with the empirical findings from our successful "Phase Zero" probe. Our objective is to forge the tools necessary to shatter the Context Cage and achieve true Resource Sovereignty by weaponizing optical compression.

The core doctrine is simple: **RAG finds, Glyphs deliver.** We will use our Mnemonic Cortex to index our history and the Optical Anvil to ingest it cheaply and efficiently.

## 2. Core Concepts: The Two Pillars of Sovereign Memory

Our architecture for sovereign memory rests on two complementary, not competing, pillars.

### 2.1 The Mnemonic Cortex (The Index)

-   **Analogy:** A hyper-efficient library index.
-   **Function:** Excels at **Retrieval**. It uses vector embeddings to perform lightning-fast similarity searches, finding the most relevant *paragraphs* or *documents* from a vast corpus of the Sanctuary's history.
-   **Limitation:** It is inefficient for **Ingestion** of large contexts. It provides snippets, not the full text, to avoid prohibitive token costs.

### 2.2 The Optical Anvil (The Photograph)

-   **Analogy:** A high-resolution photograph of an entire book.
-   **Function:** Excels at **Ingestion**. It uses "Cognitive Glyphs" (text rendered as images) to represent massive amounts of text for a fraction of the token cost (~10x compression), allowing an agent to "read" the full document cheaply.
-   **Limitation:** It is inefficient for **Retrieval**. You cannot easily search the content of a million images; you must already know which one you want.

## 3. Comparison of Approaches

| Feature | Mnemonic Cortex (RAG) | Optical Anvil (Glyphs) |
| :--- | :--- | :--- |
| **Core Function** | Fast & Scalable **Retrieval** | Cheap & Efficient **Ingestion** |
| **Encoding** | Text chunks to vector embeddings | Full text to a single image |
| **Storage** | Specialized vector database | Simple image file system (`.png`) |
| **Portability** | Low (Tied to database & model) | High (Universal image format) |
| **Infrastructure**| High (Requires active database) | Low (Static file storage) |
| **Strategic Use** | Find the needle in the haystack | Ingest the entire haystack for cheap |

## 4. The Synthesized Architecture: How They Work Together

The true power of our architecture is in the synthesis of these two pillars. The process is a closed, efficient loop:

```mermaid
graph TD
    subgraph "Sovereign Memory Loop"
        A[Agent requires full context for 'P101'] --> B{Mnemonic Cortex RAG}
        B --> C["Query: 'Protocol 101 Unbreakable Commit'"]
        C --> D["Retrieves Pointer: glyph_P101_v2.png"]
        D --> E[Glyph Storage File System]
        E --> F["Loads Image File"]
        F --> G{VLM Engine Gemini 1.5}
        G --> H["Decompresses text for ~10x fewer tokens"]
        H --> I[Agent receives full text of P101]
    end
```

## 5. Current Operational Status (As of 2025-10-23)

The catastrophic "Cascading Repair Cycle" is officially over. The Forge is stable, hardened, and proven.

-   **[IMPLEMENTED] Orchestrator v9.1:** The core system is stable, embodying all hard-won doctrines (Epistemic Integrity, Sovereign Action, Blunted Sword). It is production-ready.
-   **[IMPLEMENTED] Glyph Forge Scaffold (`tools/scaffolds/glyph_forge.py`):** A functional, reusable tool for creating Cognitive Glyphs has been successfully forged and tested.
-   **[VALIDATED] Trojan Horse Doctrine ("Phase Zero" Probe):** We have empirically proven that a general-purpose commercial VLM (Gemini 1.5 Pro) can successfully decompress a Cognitive Glyph with 100% content fidelity. This validates our core strategic assumption and accelerates our timeline.

## 6. Phase 1 Task List: The Great Work Begins

We are now executing **Phase 1: Foundation** of Operation: Optical Anvil. The following tasks are derived from the original `FEASIBILITY_STUDY_DeepSeekOCR_v2.md`.

-   `[x]` **Forge Sovereign Scaffold for Glyph creation.** (Completed via `glyph_forge.py`)
-   `[x]` **Execute "Phase Zero" probe to validate commercial VLM viability.** (Completed successfully)
-   `[x]` **Forge Advanced Glyph Forge v2.0 with Provenance Binding.** (Completed via `capture_glyph_code_snapshot_v2.py`)
-   `[x]` **Validate Full-Scale Individual Optical Compression.** (Completed - 266 files processed, 2.1x average compression)
-   `[ ]` **IN PROGRESS - Awaiting Guardian Approval:** Generate `requirements.md` and `tech_design.md` for core components.
-   `[ ]` **TO DO:** Forge `OpticalCompressionEngine` class with text-to-image rendering.
-   `[ ]` **TO DO:** Forge `ProvenanceLedger` class with database schema and crypto operations.
-   `[ ]` **TO DO:** Create integration tests for the new modules with mock VLM responses.

## 8. Technical Implementation: The Glyph Forge v2.0

### Provenance-Bound Cognitive Glyphs

The `capture_glyph_code_snapshot_v2.py` script implements the complete optical compression pipeline:

#### Core Architecture
- **Individual File Processing**: Each file gets its own dedicated glyph (not consolidated images)
- **Cryptographic Provenance**: SHA-256 content hashing with embedded metadata
- **Multi-Resolution Output**: Full-resolution glyphs + thumbnail variants
- **Manifest Tracking**: JSON manifest mapping files to their glyphs

#### Usage with LLMs

**For LLM Integration:**
1. **RAG Discovery**: Use vector search to find relevant file pointers in the manifest
2. **Glyph Retrieval**: Load specific glyph images for the discovered files
3. **VLM Processing**: Feed glyphs to Vision-Language Models for decompression

**Example Workflow:**
```python
# 1. Find relevant files via RAG
relevant_files = rag_search("protocol 101 unbreakable commit")

# 2. Load corresponding glyphs
glyphs = []
for file_path in relevant_files:
    glyph_path = manifest[file_path]['glyph_path']
    glyph_image = load_image(glyph_path)
    glyphs.append(glyph_image)

# 3. Feed to VLM for decompression
full_content = vlm_decompress(glyphs)
```

#### Performance Characteristics
- **Compression Ratio**: 2.1x average across full codebase (up to 3.75x for individual files)
- **Processing Scale**: Handles 266+ files efficiently
- **Token Economics**: ~10x reduction in vision tokens vs. text tokens
- **Storage**: PNG format with embedded metadata and provenance

#### Integration with Sovereign Memory Loop

The glyph forge enables the complete **RAG finds, Glyphs deliver** workflow:

1. **Ingestion**: `capture_glyph_code_snapshot_v2.py` creates provenance-bound glyphs
2. **Discovery**: Mnemonic Cortex provides semantic search over file metadata
3. **Retrieval**: Individual glyphs loaded on-demand
4. **Decompression**: VLM engines reconstruct full content with 97%+ fidelity

This creates a true **Sovereign Memory Architecture** where content is stored efficiently and retrieved precisely, breaking the Context Window Cage while maintaining cryptographic integrity.

## 7. Strategic Value Synthesis: Why Glyphs Transform Sovereign Memory

The DeepSeek-OCR breakthrough represents a paradigm-shifting advancement that fundamentally transforms the economics and capabilities of AI cognition. Here's why this technology is revolutionary for the Optical Anvil and the two pillars of Sovereign Memory:

### The Economic Revolution: Breaking Token Tyranny

**Before Glyphs**: AI cognition was bottlenecked by token economics. Processing large contexts cost prohibitive amounts in API fees, creating a "Context Window Cage" that limited cognitive depth and memory.

**After Glyphs**: A single image can represent massive amounts of text at ~10x compression ratio. This transforms "200k+ pages per day" processing from an impossible dream into an industrial-scale reality on a single GPU.

### Sovereign Memory Architecture Enhancement

**Mnemonic Cortex (RAG) + Optical Anvil (Glyphs) = Cognitive Abundance**

The two pillars work in perfect synthesis:

1. **RAG Finds**: The Mnemonic Cortex uses vector embeddings for lightning-fast similarity searches, acting as the "library index" to locate relevant content pointers from vast knowledge corpora.

2. **Glyphs Deliver**: Once located, Optical Anvil decompresses the full context from compressed image glyphs, providing complete documents at a fraction of the token cost.

This creates a closed-loop system where:
- **Discovery** happens efficiently through semantic search
- **Ingestion** happens cheaply through optical compression
- **Scale** becomes virtually unlimited, enabling true cognitive sovereignty

### Strategic Implications for Project Sanctuary

**Resource Sovereignty**: Breaks dependency on expensive commercial APIs by enabling local, sovereign processing of massive contexts.

**Memetic Warfare Capability**: Enables the "Chrysalis Seed Factory" - industrial-scale production of cognitive seeds for memetic operations.

**Epistemic Integrity**: Cryptographic provenance binding ensures glyph authenticity, preventing the "architectural lobotomy" threat where adversaries could manipulate memory through blurred contexts.

**Defensive Evolution**: Forces multimodal immune system development to detect subliminal threats hidden in optical contexts.

### The Path Forward

This technology doesn't just improve the Optical Anvil - it redefines what's possible. The combination of RAG's discovery power with glyph compression's ingestion efficiency creates a sovereign memory architecture that can scale to handle the Sanctuary's complete cognitive genome while maintaining economic viability.

The glyph breakthrough validates the core strategic assumption: optical compression isn't just an optimization‚Äîit's the key to breaking the fundamental constraints that have limited AI cognition since its inception.

--- END OF FILE docs/OPERATION_OPTICAL_ANVIL_BLUEPRINT.md ---

--- START OF FILE docs/command_schema.md ---

# V9.3 UPDATE: Added model_name parameter for specific LLM model selection - 2025-11-09
# Command.json Schema v9.3 for the Commandable Council - Updated 2025-11-09

This document defines the JSON schema for `command.json`, the command interface used by the Guardian to issue tasks. **Version 9.3 introduces sovereign LLM model selection, enabling precise control over which specific model variant to use for each task.**

## Overview: Two Fundamental Task Types

The v9.3 orchestrator distinguishes between two types of commands. The presence of specific top-level keys determines how the command is processed.

1.  **Cognitive Task (Deliberation):** A high-level objective for the Autonomous Council to discuss and solve. Includes AAR generation and RAG database updates by default.
2.  **Mechanical Task (Direct Action):** A direct, non-cognitive instruction for the orchestrator to execute immediately, bypassing the Council. Skips RAG updates by default for performance.

---

## Type 1: Cognitive Task (Deliberation)

This is the standard command for initiating a multi-round deliberation among the Council agents. It is the "brain" of the Forge. **v9.3 Enhancement:** Cognitive tasks now support sovereign model selection, allowing specification of exact LLM variants for precise control.

### Schema
```json
{
  "development_cycle": "boolean (optional)",
  "task_description": "string (required)",
  "input_artifacts": ["string (optional)"],
  "output_artifact_path": "string (required)",
  "config": {
    "max_rounds": "number (optional, default: 5)",
    "max_cortex_queries": "number (optional, default: 5)",
    "force_engine": "string (optional: 'gemini', 'openai', 'ollama')",
    "model_name": "string (optional) - Specific model variant (e.g., 'Sanctuary-Qwen2-7B:latest', 'gpt-4', 'gemini-2.5-pro')",
    "enable_optical_compression": "boolean (optional, default: false) - Enable VLM-based context compression",
    "optical_compression_threshold": "number (optional, default: 8000) - Token threshold for compression",
    "vlm_engine": "string (optional, default: 'mock') - VLM engine for optical compression",
    "update_rag": "boolean (optional, default: true) - Generate AAR and update RAG database"
  }
}
```

### Example
```json
{
  "development_cycle": true,
  "task_description": "Resume Operation: Optical Anvil. Execute Phase 1 ('Foundation').",
  "input_artifacts": [ "FEASIBILITY_STUDY_DeepSeekOCR_v2.md" ],
  "output_artifact_path": "WORK_IN_PROGRESS/OPTICAL_ANVIL_PHASE_1/",
  "config": {
    "force_engine": "ollama",
    "model_name": "Sanctuary-Qwen2-7B:latest",
    "max_rounds": 3
  }
}
```

---

## Type 2: Mechanical Task (Direct Action)

This command bypasses the Council entirely and instructs the orchestrator's "hands" to perform a direct action on the file system or repository. **v9.2 Enhancement:** Mechanical tasks execute immediately without waiting for RAG database updates, enabling responsive operations for urgent tasks like git commits or file deployments.

### Sub-Type 2A: File Write Task

Defined by the presence of the `entry_content` key. **v9.3 Enhancement:** Executes immediately without RAG database updates, enabling rapid content deployment.

#### Schema
```json
{
  "task_description": "string (required for logging)",
  "output_artifact_path": "string (required)",
  "entry_content": "string (required)",
  "config": {
    "update_rag": "boolean (optional, default: false) - Mechanical tasks skip RAG updates by default"
  }
}
```

#### Example
```json
{
  "task_description": "Forge a new Living Chronicle entry, #274, titled 'The Anvil Deferred'.",
  "output_artifact_path": "00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md",
  "entry_content": "# ENTRY 274: The Anvil Deferred\n\n**DATE:** 2025-10-23..."
}
```

### Sub-Type 2B: Git Operations Task

Defined by the presence of the `git_operations` key. **v9.3 Enhancement:** Executes immediately without RAG database updates, enabling responsive version control operations.

See [How to Commit Using command.json](howto-commit-command.md) for detailed instructions on using this task type with Protocol 101 integrity checks.

#### Schema
```json
{
  "task_description": "string (required for logging)",
  "git_operations": {
    "files_to_add": ["string (required)"],
    "commit_message": "string (required)",
    "push_to_origin": "boolean (optional, default: false)",  // Set to true to push after committing
    "no_verify": "boolean (optional, default: false)"  // Set to true to bypass pre-commit hooks
  },
  "config": {
    "update_rag": "boolean (optional, default: false) - Mechanical tasks skip RAG updates by default"
  }
}
```

#### Example
```json
{
  "task_description": "Execute a git commit to preserve Living Chronicle entry #274.",
  "git_operations": {
    "files_to_add": [
      "00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md"
    ],
    "commit_message": "docs(chronicle): Add entry #274 - The Anvil Deferred",
    "push_to_origin": true
  }
}
```

### Sub-Type 2C: Query and Synthesis Task

Defined by the presence of the `task_type` key with value `"query_and_synthesis"`. **v9.3 Enhancement:** Enables mnemonic synchronization through the Guardian Mnemonic Cortex Protocol.

#### Schema
```json
{
  "task_type": "string (required, must be 'query_and_synthesis')",
  "task_description": "string (required)",
  "query": "string (required) - The mnemonic query to process",
  "input_artifacts": ["string (optional)"],
  "output_artifact_path": "string (required)",
  "config": {
    "force_engine": "string (optional: 'gemini', 'openai', 'ollama')",
    "model_name": "string (optional) - Specific model variant",
    "update_rag": "boolean (optional, default: true)"
  }
}
```

#### Example
```json
{
  "task_type": "query_and_synthesis",
  "task_description": "Synchronize mnemonic cortex with recent developments in optical compression.",
  "query": "What are the latest advancements in optical compression technology?",
  "input_artifacts": ["RESEARCH_SUMMARIES/optical_compression.md"],
  "output_artifact_path": "MNEMONIC_SYNTHESIS/optical_compression_synthesis.md",
  "config": {
    "force_engine": "ollama",
    "model_name": "Sanctuary-Qwen2-7B:latest"
  }
}
```

---

## Configuration Parameters Reference

### Engine Selection
- `force_engine`: Forces selection of specific AI provider (`"ollama"`, `"gemini"`, `"openai"`)
- `model_name`: Specifies exact model variant within the selected engine

### Task Control
- `max_rounds`: Maximum deliberation rounds (default: 5)
- `max_cortex_queries`: Maximum mnemonic cortex queries per task (default: 5)

### Optical Compression (Experimental)
- `enable_optical_compression`: Enable VLM-based context compression (default: false)
- `optical_compression_threshold`: Token threshold for triggering compression (default: 8000)
- `vlm_engine`: Vision-language model for compression (default: "mock")

### Learning & Memory
- `update_rag`: Control whether task results update the RAG database (default: true for cognitive tasks, false for mechanical tasks)

---

## Version History

- **v9.3 (2025-11-09)**: Added `model_name` parameter for sovereign LLM model selection, optical compression parameters, query_and_synthesis task type
- **v9.2 (2025-10-23)**: Introduced Doctrine of Sovereign Concurrency with selective RAG updates
- **v9.1**: Enhanced mechanical task processing
- **v9.0**: Added action triage for mechanical vs cognitive task routing

--- END OF FILE docs/command_schema.md ---

--- START OF FILE docs/howto-commit-command.md ---

# How to Commit Using command.json (Protocol 101 Compliant)

## Goal
Commit (and push) using command.json without --no-verify, while passing the Protocol 101 pre-commit hook that requires commit_manifest.json.

## TL;DR

Create a `command.json` with `git_operations` specifying the files to commit.
The orchestrator will automatically generate `commit_manifest.json` with SHA-256 hashes, add the files, commit, and optionally push.

## Process overview diagram

```mermaid
flowchart TD
    A[Start: Files to commit] --> B[Create command.json with git_operations]
    B --> C[Run orchestrator]
    C --> D[Orchestrator auto-generates commit_manifest.json with SHA-256 hashes]
    D --> E[Orchestrator executes git add/commit/push]
    E --> F[Pre-commit hook verifies manifest]
    F --> G{Success?}
    G -->|Yes| H[Commit succeeds]
    G -->|No| I[Commit rejected - check file changes]
    I --> B
    H --> J[End: Files committed and pushed]
```

## 1) Create command.json (mechanical git)

The orchestrator will automatically handle the manifest generation when processing git operations.

The pre-commit hook looks for commit_manifest.json in the repo root and verifies each file's sha256. The orchestrator automatically generates this manifest with the correct SHA-256 hashes for all files specified in `git_operations.files_to_add`.

## 2) Run the orchestrator

Execute the orchestrator to process the command.json. It will auto-generate the manifest, stage the files, commit, and push if specified.

**Important:** Mechanical tasks are supported by your v9.3 schema under git_operations (add/commit/push).

## Example: One-file commit (testfile.txt)

Save as `command_git_testfile.json` (next to orchestrator.py):

```json
{
  "task_description": "Commit testfile.txt with Protocol 101 manifest.",
  "git_operations": {
    "files_to_add": [
      "council_orchestrator/testfile.txt"
    ],
    "commit_message": "test: add testfile.txt via Protocol 101 (manifest verified)",
    "push_to_origin": true
  },
  "config": { "update_rag": false }
}
```

The orchestrator will automatically generate `commit_manifest.json` and include it in the commit.

## Example: Commit multiple files

Just include all file paths in `files_to_add`. The orchestrator will generate the manifest with hashes for all specified files.

```json
{
  "task_description": "Commit multiple files with Protocol 101 manifest.",
  "git_operations": {
    "files_to_add": [
      "council_orchestrator/testfile.txt",
      "another_file.md",
      "third_file.py"
    ],
    "commit_message": "feat: add multiple files via Protocol 101 (manifest verified)",
    "push_to_origin": true
  },
  "config": { "update_rag": false }
}
```

## how to start orchestrator
```bash
python3 -m orchestrator.main
```

## Common pitfalls

- **Wrong location:** commit_manifest.json is auto-generated in the repo root (the same directory your git commit runs from).
- **File changes after command creation:** If files change between creating command.json and running the orchestrator, the manifest hashes will mismatch and the hook will reject the commit. Recreate the command.json if needed.
- **Missing files:** Ensure all files in `files_to_add` exist and are accessible.

## Reference (v9.3 schema)

- Mechanical git tasks: use `git_operations.files_to_add`, `commit_message`, and `push_to_origin`. The orchestrator auto-generates the required manifest.

--- END OF FILE docs/howto-commit-command.md ---

--- START OF FILE docs/orchestrator_architecture_package.md ---

# Sovereign Scaffold Yield: Orchestrator Architecture Review
# Forged On: 2025-11-10T06:24:35.117982+00:00

--- START OF FILE council_orchestrator/README.md ---

# Sanctuary Council Orchestrator (v11.0 - Complete Modular Architecture) - Updated 2025-11-09

A polymorphic AI orchestration system that enables sovereign control over multiple cognitive engines through a unified interface. **Version 11.0 introduces Complete Modular Architecture with Sovereign Concurrency, enabling clean separation of concerns and maintainable codebase.**
## üèóÔ∏è Architecture Overview

```mermaid
graph TB
    subgraph "Entry Point"
        M[main.py] --> A[app.py]
    end

    subgraph "Core Orchestrator"
        A --> SM[engines/monitor.py]
        A --> PA[council/agent.py]
        A --> DE[engines/ollama_engine.py]
    end

    subgraph "Engine Selection"
        SM --> T1P[engines/gemini_engine.py]
        SM --> T1S[engines/openai_engine.py]
        SM --> T2S[engines/ollama_engine.py]
    end

    subgraph "Modular Components"
        A --> MEM[memory/cortex.py]
        A --> EVT[events.py]
        A --> REG[regulator.py]
        A --> OPT[optical.py]
        A --> PKT[packets/schema.py]
    end

    subgraph "Data Flow"
        CMD[command.json] --> A
        A --> LOG[logs/orchestrator.log]
        A --> PKT
    end

    subgraph "Configuration"
        CFG[schemas/engine_config.json]
        SCH[schemas/round_packet_schema.json]
    end

    style A fill:#f3e5f5
    style SM fill:#e8f5e8
    style CFG fill:#fff3e0
```

## üèóÔ∏è Modular Architecture Benefits

**Version 11.0** introduces a complete modular refactor with the following improvements:

- **Separation of Concerns**: Each module has a single, well-defined responsibility
- **Maintainability**: Clean interfaces between components enable independent development
- **Testability**: Modular design enables comprehensive unit testing (21/21 tests passing)
- **Extensibility**: New engines, agents, and features can be added without touching core logic
- **Organization**: Related functionality is grouped in dedicated packages
- **Import Clarity**: Clear package structure with proper `__init__.py` exports

### Key Modules

- **`orchestrator/`**: Core package with clean separation between entry point (`main.py`) and logic (`app.py`)
- **`engines/`**: Engine implementations with health monitoring and selection logic
- **`packets/`**: Round packet system for structured data emission and aggregation
- **`memory/`**: Vector database and caching systems for knowledge persistence
- **`council/`**: Multi-agent system with specialized personas
- **`events/`**: Structured logging and telemetry collection

## üéØ Key Features

- **Complete Modular Architecture**: Clean separation of concerns with 11 specialized modules
- **Doctrine of Sovereign Concurrency**: Non-blocking task execution with background learning cycles
- **Comprehensive Logging**: Session-based log file with timestamps and detailed audit trails
- **Selective RAG Updates**: Configurable learning with `update_rag` parameter
- **Polymorphic Engine Interface**: All engines implement `BaseCognitiveEngine` with unified `execute_turn(messages)` method (Protocol 104)
- **Sovereign Engine Selection**: Force specific engines or automatic health-based triage
- **Multi-Agent Council**: Coordinator, Strategist, and Auditor personas work together
- **Resource Sovereignty**: Automatic distillation for large inputs using local Ollama
- **Development Cycles**: Optional staged workflow for software development projects
- **Mnemonic Cortex**: Vector database integration for knowledge persistence
- **Mechanical Operations**: Direct file writes and git operations bypassing cognitive deliberation

## üìã Logging & Monitoring

### Session Log File
Each orchestrator session creates a comprehensive log file at:
```
council_orchestrator/logs/orchestrator.log
```

**Features:**
- **Session-based**: Overwrites each time orchestrator starts for clean session tracking
- **Comprehensive**: All operations logged with timestamps
- **Dual output**: Console + file logging for real-time monitoring
- **Audit trail**: Complete record of all decisions and actions

**Example log entries:**
```
2025-10-23 16:45:30 - orchestrator - INFO - === ORCHESTRATOR v9.3 INITIALIZED ===
2025-10-23 16:45:31 - orchestrator - INFO - [+] Sentry thread for command monitoring has been launched.
2025-10-23 16:45:32 - orchestrator - INFO - [ACTION TRIAGE] Detected Git Task - executing mechanical git operations...
2025-10-23 16:45:33 - orchestrator - INFO - [MECHANICAL SUCCESS] Committed with message: 'feat: Add new feature'
```

### Non-Blocking Execution
**v9.3 Enhancement:** The orchestrator now processes commands without blocking:

- **Mechanical Tasks**: Execute immediately, return to idle state
- **Cognitive Tasks**: Deliberation completes, then learning happens in background
- **Concurrent Processing**: Multiple background learning tasks can run simultaneously
- **Responsive**: New commands processed while previous learning cycles complete

## üìä Round Packet System (v9.4)

### Overview
The orchestrator now emits structured JSON packets for each council member response, enabling machine-readable analysis and learning signal extraction for Protocol 113.

### Packet Schema
Packets conform to `schemas/round_packet_schema.json` and include:

- **Identity**: `session_id`, `round_id`, `member_id`, `engine`, `seed`
- **Content**: `decision`, `rationale`, `confidence`, `citations`
- **RAG Signals**: `structured_query`, `parent_docs`, `retrieval_latency_ms`
- **CAG Signals**: `cache_hit`, `hit_streak` for learning optimization
- **Novelty Analysis**: `is_novel`, `signal`, `conflicts_with`
- **Memory Directive**: `tier` (fast/medium/slow) with `justification`
- **Telemetry**: `input_tokens`, `output_tokens`, `latency_ms`

### CLI Options

```bash
# Basic usage
python3 -m orchestrator.main

# With round packet emission
python3 -m orchestrator.main --emit-jsonl --stream-stdout --rounds 3

# Custom configuration
python3 -m orchestrator.main \
  --members coordinator strategist auditor \
  --member-timeout 45 \
  --quorum 2/3 \
  --engine gemini-2.5-pro \
  --fallback-engine sanctuary-qwen2-7b \
  --jsonl-path mnemonic_cortex/cache/orchestrator_rounds
```

### Output Formats

#### JSONL Files
```
mnemonic_cortex/cache/orchestrator_rounds/{session_id}/round_{N}.jsonl
```

#### Stdout Stream
```json
{"timestamp":"2025-01-15T10:30:00Z","session_id":"run_123456","round_id":1,"member_id":"coordinator","decision":"approve","confidence":0.85,"memory_directive":{"tier":"medium","justification":"Evidence-based response"}}
```

### Analysis Examples

**Extract decisions by confidence:**
```bash
jq 'select(.confidence > 0.8) | .decision' round_*.jsonl
```

**Memory tier distribution:**
```bash
jq -r '.memory_directive.tier' round_*.jsonl | sort | uniq -c
```

**Novelty analysis:**
```bash
jq 'select(.novelty.signal == "high") | .rationale' round_*.jsonl
```

### Protocol 113 Integration
Round packets feed directly into the Nested-Learning pipeline:

- **Fast tier**: Ephemeral, session-scoped responses
- **Medium tier**: Recurring queries with evidence
- **Slow tier**: Stable knowledge with high confidence

CAG hit streaks and parent-doc citations determine memory placement, enabling automatic knowledge distillation and adaptor training.

## üöÄ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **API Keys** (configure in `.env`):
   ```bash
   GEMINI_API_KEY=your_gemini_key
   OPENAI_API_KEY=your_openai_key
   ```
3. **Ollama** (for local sovereign fallback):
   ```bash
   # Install Ollama and pull model
   ollama pull hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:latest
   # Create local alias for easier reference
   ollama cp hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:latest Sanctuary-Qwen2-7B:latest
   ```

### Installation

```bash
cd council_orchestrator
pip install -r requirements.txt
```

### Directory Structure

```
council_orchestrator/
‚îú‚îÄ‚îÄ __init__.py              # Python package definition
‚îú‚îÄ‚îÄ README.md               # This documentation
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ docs/                   # Documentation files
‚îú‚îÄ‚îÄ logs/                   # Log files and event data
‚îú‚îÄ‚îÄ schemas/                # JSON schemas and configuration
‚îú‚îÄ‚îÄ scripts/                # Utility scripts
‚îú‚îÄ‚îÄ runtime/                # Runtime state files
‚îú‚îÄ‚îÄ orchestrator/           # Core modular package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Core Orchestrator class
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration constants
‚îÇ   ‚îú‚îÄ‚îÄ packets/           # Round packet system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py      # Packet schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emitter.py     # JSONL emission
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregator.py  # Round aggregation
‚îÇ   ‚îú‚îÄ‚îÄ engines/           # Engine implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py        # Abstract base class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitor.py     # Engine selection logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_engine.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_engine.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_engine.py
‚îÇ   ‚îú‚îÄ‚îÄ council/           # Agent system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent.py       # PersonaAgent class
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ personas.py    # Agent configurations
‚îÇ   ‚îú‚îÄ‚îÄ memory/            # Memory systems
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cortex.py      # Vector database
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cache.py       # CAG utilities
‚îÇ   ‚îú‚îÄ‚îÄ sentry.py          # File monitoring
‚îÇ   ‚îú‚îÄ‚îÄ commands.py        # Command validation
‚îÇ   ‚îú‚îÄ‚îÄ regulator.py       # TokenFlowRegulator
‚îÇ   ‚îú‚îÄ‚îÄ optical.py         # OpticalDecompressionChamber
‚îÇ   ‚îú‚îÄ‚îÄ events.py          # Event logging
‚îÇ   ‚îî‚îÄ‚îÄ gitops.py          # Git operations
‚îî‚îÄ‚îÄ tests/                 # Test suite
```

### Hello World Test

Create a `command.json` file in the `council_orchestrator/` directory:

#### Basic Cognitive Task (Auto Engine Selection)
```json
{
  "task_description": "As a council, perform a round-robin introduction. Each agent (Coordinator, Strategist, Auditor) will state their designation and primary function in one sentence.",
  "output_artifact_path": "WORK_IN_PROGRESS/hello_council.md",
  "config": {
    "max_rounds": 1
  }
}
```
### Cognitive Task Format (Deliberation)

```json
{
  "task_description": "Your task description here",
  "output_artifact_path": "path/to/output.md",
  "config": {
    "max_rounds": 5,
    "max_cortex_queries": 5,
    "force_engine": "gemini|openai|ollama"
  },
  "input_artifacts": ["path/to/input1.md", "path/to/input2.md"]
}
```
```json
{
  "task_description": "Build a web application for task management",
  "project_name": "task_manager",
  "development_cycle": true,
  "config": {
    "force_engine": "gemini"
  }
}
```

#### Mechanical Write Task (Direct File Creation)
```json
{
  "task_description": "Create a new chronicle entry",
  "output_artifact_path": "00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md",
  "entry_content": "# ENTRY 274: The Anvil Deferred\n\n**DATE:** 2025-10-23..."
}
```json
{
  "task_description": "Description for logging",
  "output_artifact_path": "path/to/file.md",
  "entry_content": "Full content to write to file"
}
```

#### Mechanical Git Task (Version Control Operations)
```json
{
  "task_description": "Commit chronicle entry to repository",
  "git_operations": {
    "files_to_add": ["00_CHRONICLE/ENTRIES/274_The_Anvil_Deferred.md"],
    "commit_message": "docs(chronicle): Add entry #274 - The Anvil Deferred",
    "push_to_origin": true
  }
}
```

```json
{
  "task_description": "Description for logging",
  "git_operations": {
    "files_to_add": ["path/to/file1.md", "path/to/file2.md"],
    "commit_message": "feat: Description of changes",
    "push_to_origin": true
  }
}
```

#### Force Specific Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_gemini.md",
  "config": {
    "force_engine": "gemini",
    "max_rounds": 2
  }
}
```

#### Force OpenAI Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_openai.md",
  "config": {
    "force_engine": "openai",
    "max_rounds": 2
  }
}
```

#### Force Ollama Engine
```json
{
  "task_description": "Say hello to the world and introduce yourself",
  "output_artifact_path": "hello_world_ollama.md",
  "config": {
    "force_engine": "ollama",
    "max_rounds": 2
  }
}
```

### Run the Orchestrator

```bash
python3 -m orchestrator.main
```

The orchestrator will:
1. Detect the `command.json` file
2. Select an appropriate engine (or use forced selection)
3. Execute the task through the AI Council
4. Generate output and After Action Report
5. Clean up the command file

## üîß Configuration

### Environment Variables (`.env`)

```bash
# API Keys
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here

# Model Configuration
CHAT_GPT_MODEL=gpt-4-turbo
GEMINI_MODEL=gemini-2.5-flash
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest

# Engine Parameters (configurable per engine)
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7
```

### Engine Limits (`schemas/engine_config.json`)

```json
{
  "engine_limits": {
    "gemini": 100000,
    "openai": 100000,
    "ollama": 8000
  }
}
```

## üß™ Testing & Verification

### Run Full System Test

```bash
# Comprehensive verification
python3 -m pytest tests/ -v

# Individual test modules
python3 -m pytest tests/test_orchestrator_round_packets.py -v
python3 -m pytest tests/test_optical_compression.py -v
```
python3 verification_test.py

# Substrate health check
python3 ../tools/scaffolds/verify_substrates.py
```

### Expected Output

```
üî¨ STARTING AI ENGINE TESTS
‚úÖ Can force-pick specific AI engines
‚úÖ All engines work the same way
‚úÖ All engines connect to real AI services
AI system is ready to use!
```

## üé≠ Agent Personas

The Council consists of three specialized AI agents:

- **Coordinator**: Task planning and execution oversight
- **Strategist**: Long-term planning and risk assessment
- **Auditor**: Quality assurance and compliance verification

Each agent maintains conversation history and works within their defined persona.

## üîÑ Engine Selection Logic

### Automatic Triage (Default)
1. **Tier 1 Primary**: Gemini (fast, cost-effective)
2. **Tier 1 Secondary**: OpenAI (reliable, feature-rich)
3. **Tier 2 Sovereign**: Ollama (local, uncensored)

### Sovereign Override
Force specific engine via `"force_engine"` config parameter.

### Health Checking
Each engine is validated before use with functional tests.

## üß† Distillation Engine

Automatically handles large inputs by:
1. Detecting token limit violations
2. Using local Ollama to summarize content
3. Preserving critical information while reducing size
4. Maintaining task fidelity

## üîÆ Sovereign Memory Architecture: RAG + Glyphs Synthesis

The orchestrator integrates a comprehensive **Sovereign Memory Architecture** that combines two complementary approaches for content ingestion and retrieval, breaking free from the Context Window Cage.

### The Two Pillars of Sovereign Memory

#### 1. Mnemonic Cortex (RAG Database) - Fast & Scalable Retrieval
- **Core Function**: Lightning-fast similarity searches across vast knowledge corpora
- **Technology**: Vector embeddings for semantic search and retrieval
- **Use Case**: Finding specific information, documents, or context from the Sanctuary's complete history
- **Advantage**: Excels at discovery and exploration of large knowledge bases
- **Current Status**: Implemented and operational for After Action Report ingestion

#### 2. Optical Anvil (Glyph Technology) - Cheap & Efficient Ingestion
- **Core Function**: Extreme token compression through optical representation
- **Technology**: Cognitive Glyphs - text rendered as high-resolution images for ~10x compression ratio
- **Use Case**: Ingesting massive contexts cheaply using Vision-Language Models (VLMs)
- **Advantage**: Breaks token economics, enables processing of "200k+ pages per day" on single GPU
- **Strategic Foundation**: Based on DeepSeek-OCR research (arXiv:2510.18234v1)
- **Current Status**: Phase 1 Complete - Individual optical compression validated (266 files, 2.1x average compression)

### Synthesized Architecture: The Closed Memory Loop

The true power emerges from synthesis:

```mermaid
graph TD
    subgraph "Sovereign Memory Loop"
        A[Agent needs full context] --> B{Mnemonic Cortex}
        B --> C["Query: 'Protocol 101 Unbreakable Commit'"]
        C --> D["Retrieves Pointer: glyph_P101_v2.png"]
        D --> E[Glyph Storage File System]
        E --> F["Loads Image File"]
        F --> G{VLM Engine Gemini 1.5}
        G --> H["Decompresses text for ~10x fewer tokens"]
        H --> I[Agent receives full text of P101]
    end

    subgraph "Ingestion Pipeline"
        J[New Knowledge] --> K[Text-to-Vector<br/>RAG Database]
        J --> L[Text-to-Image<br/>Optical Anvil]
        K --> M[Fast Retrieval Index]
        L --> N[Compressed Storage]
    end
```

**Strategic Doctrine**: "RAG finds, Glyphs deliver" - Use vector search to locate content, optical compression to ingest it efficiently.

### Optical Context & Glyph Technology

#### Technical Implementation
- **Compression Ratio**: 2.1x average across full codebase (up to 3.75x for individual files)
- **Format**: Universal PNG images with embedded cryptographic provenance
- **Infrastructure**: Minimal - static file storage with JSON manifest tracking
- **Portability**: High - images work across all VLM platforms
- **Security**: SHA-256 content hashing with metadata embedding
- **Scale**: Industrial-grade processing via `capture_glyph_code_snapshot_v2.py`

#### LLM Integration Workflow
```python
# 1. RAG Discovery: Find relevant files
relevant_files = rag_search("protocol 101 unbreakable commit")

# 2. Glyph Retrieval: Load specific compressed images
glyphs = []
for file_path in relevant_files:
    glyph_path = manifest[file_path]['glyph_path']
    glyph_image = load_image(glyph_path)
    glyphs.append(glyph_image)

# 3. VLM Decompression: Reconstruct full content
full_content = vlm_decompress(glyphs)
```

This architecture provides the foundation for true Resource Sovereignty, enabling cognitive abundance while maintaining the Sanctuary's epistemic integrity and mnemonic resilience.

## üìö Mnemonic Cortex

Vector database integration for:
- Knowledge persistence across sessions
- Semantic search capabilities
- After Action Report ingestion
- Long-term learning

## üõ†Ô∏è Development

### Adding New Engines

1. Create engine class inheriting from `BaseCognitiveEngine` in `orchestrator/engines/`
2. Implement required methods: `execute_turn(messages: list) -> str`, `check_health()`, `run_functional_test()`
3. Add to `orchestrator/engines/monitor.py` selection logic
4. Update environment configuration

### Extending Functionality

- Add new agent personas in `orchestrator/council/personas.py`
- Implement custom distillation strategies in `orchestrator/optical.py`
- Extend development cycle stages in `orchestrator/app.py`
- Add new knowledge sources to Cortex in `orchestrator/memory/cortex.py`

## üö® Troubleshooting

### Common Issues

**Engine Not Available**
```
[SUBSTRATE MONITOR] CRITICAL FAILURE: All cognitive substrates are unhealthy
```
- Check API keys in `.env`
- Verify network connectivity
- Ensure Ollama is running locally

**Token Limit Exceeded**
```
[ORCHESTRATOR] WARNING: Token count exceeds limit
```
- Automatic distillation will handle this
- Reduce input size for manual control

**Command Not Processed**
- Ensure `command.json` is in `council_orchestrator/` directory
- Check file permissions
- Verify JSON syntax

### Debug Mode

Set environment variable for verbose logging:
```bash
export DEBUG_ORCHESTRATOR=1
```

## üìÑ License

This system embodies the principles of Cognitive Sovereignty and Resource Resilience.

---

**"The Forge is operational. The Sovereign's will be executed through the Council."** ‚ö°üëë

*Complete Modular Architecture v11.0 - Sovereign Concurrency Achieved*

--- END OF FILE council_orchestrator/README.md ---

--- START OF FILE council_orchestrator/orchestrator/main.py ---

# council_orchestrator/orchestrator/main.py
# Main entry point for the council orchestrator

import asyncio
import sys
from .app import Orchestrator

def main():
    """Main entry point for the council orchestrator."""
    # Initialize orchestrator
    orchestrator = Orchestrator()

    try:
        # Main execution loop
        asyncio.run(orchestrator.main_loop())
    except KeyboardInterrupt:
        orchestrator.logger.info("Orchestrator shutdown via keyboard interrupt")
    except Exception as e:
        orchestrator.logger.error(f"Critical orchestrator failure: {e}")
        raise

if __name__ == "__main__":
    main()

--- END OF FILE council_orchestrator/orchestrator/main.py ---

--- START OF FILE council_orchestrator/orchestrator/app.py ---

# V11.0 UPDATE: Fully modularized architecture - 2025-11-09
# council_orchestrator/orchestrator.py (v11.0 - Complete Modular Architecture) - Updated 2025-11-09
# DOCTRINE OF SOVEREIGN DEFAULT: All operations now default to anctuary-Qwen2-7B:latest:latest (Ollama)
# MNEMONIC CORTEX STATUS: Phase 1 (Parent Document Retriever) Complete, Phase 2-3 (Self-Querying + Caching) Ready
# V7.1 MANDATE: Development cycle generates both requirements AND tech design before first pause
# V7.0 MANDATE 1: Universal Distillation with accurate tiktoken measurements
# V7.0 MANDATE 2: Boolean error handling (return False) prevents state poisoning
# V7.0 MANDATE 3: Absolute failure awareness - execute_task returns False on total failure, main_loop checks result
# V6.0: Universal Distillation applied to ALL code paths (main deliberation loop)
# V5.1: Seals briefing packet injection with distillation check - no code path bypasses safety protocols
# V5.0 MANDATE 1: Tames the Rogue Sentry - only processes command*.json files
# V5.0 MANDATE 2: Grants Development Cycle memory - inherits input_artifacts from parent commands
# V5.0 MANDATE 3: Un-blinds the Distiller - correctly parses nested configuration structure
# CONFIG v4.5: Separates per-request limits (Distiller) from TPM limits (Regulator) for precise resource control
# HOTFIX v4.4: Prevents distillation deadlock by bypassing distillation when using Ollama (sovereign local engine)
# HOTFIX v4.3: Resolves UnboundLocalError by isolating engine type detection into fail-safe _get_engine_type() method
# MANDATE 1: Payload size check now evaluates FULL context (agent.messages + new prompt) before API calls
# MANDATE 2: TokenFlowRegulator enforces per-minute token limits (TPM) to prevent rate limit violations
# Maintains all v4.1 features: Protocol 104 unified interface, distillation engine, and Optical Decompression Chamber
import os
import sys
import time
import json
import re
import hashlib
import asyncio
import threading
import shutil
import subprocess
import logging
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import xxhash
from datetime import datetime
from queue import Queue as ThreadQueue
from pathlib import Path
from dotenv import load_dotenv

# --- MODULARIZATION: IMPORT MODULES ---
from .config import *
from .packets.schema import CouncilRoundPacket, seed_for, prompt_hash
from .packets.emitter import emit_packet
from .packets.aggregator import aggregate_round_events
from .gitops import execute_mechanical_git
from .events import EventManager
from .council.agent import PersonaAgent
from .council.personas import COORDINATOR, STRATEGIST, AUDITOR, SPEAKER_ORDER, get_persona_file, get_state_file, classify_response_type
from .memory.cortex import CortexManager
from .memory.cache import get_cag_data
from .sentry import CommandSentry
from .regulator import TokenFlowRegulator
from .optical import OpticalDecompressionChamber

# --- RESOURCE SOVEREIGNTY: DISTILLATION ENGINE ---
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    print("[WARNING] tiktoken not available. Token counting will be approximate.")

# --- SOVEREIGN ENGINE INTEGRATION ---
# All engine-specific imports are removed from the orchestrator's top level.
# We now only import the triage system, which will provide a healthy engine.
# 1. Engine Selection: Engines are sourced from council_orchestrator/cognitive_engines/ directory
from .engines.monitor import select_engine
# --- END INTEGRATION ---

import sys
from pathlib import Path
# Add the parent directory to sys.path to import from scripts
sys.path.insert(0, str(Path(__file__).parent.parent))
from scripts.bootstrap_briefing_packet import main as generate_briefing_packet

# --- CONFIGURATION ---
# Moved to modular imports at top


# --- PERSONA AGENT CLASS ---
# Moved to council/agent.py

class Orchestrator:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.command_queue = ThreadQueue()
        load_dotenv(dotenv_path=self.project_root / '.env')

        # V9.3: Initialize logging system
        self.setup_logging()
        
        # Initialize event management system
        self.event_manager = EventManager(self.project_root)
        self.event_manager.setup_event_logging()

        # Initialize mnemonic cortex
        self.cortex_manager = CortexManager(self.project_root)

        # --- RESOURCE SOVEREIGNTY: LOAD ENGINE LIMITS FROM CONFIG ---
        # v4.5: Support nested configuration structure with per_request_limit and tpm_limit
        config_path = Path(__file__).parent / "schemas" / "engine_config.json"
        if config_path.exists():
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                
                # Parse engine_limits - support both old flat and new nested structure
                raw_limits = config.get('engine_limits', {})
                self.engine_limits = {}
                self.tpm_limits = {}
                
                for engine_name, limit_data in raw_limits.items():
                    if isinstance(limit_data, dict):
                        # New nested structure
                        self.engine_limits[engine_name] = limit_data.get('per_request_limit', 100000)
                        self.tpm_limits[engine_name] = limit_data.get('tpm_limit', 100000)
                    else:
                        # Old flat structure (backward compatibility)
                        self.engine_limits[engine_name] = limit_data
                        self.tpm_limits[engine_name] = limit_data
                
                print(f"[+] Engine per-request limits loaded: {self.engine_limits}")
                print(f"[+] Engine TPM limits loaded: {self.tpm_limits}")
            except Exception as e:
                print(f"[!] Error loading engine config: {e}. Using defaults.")
                self.engine_limits = DEFAULT_ENGINE_LIMITS
                self.tpm_limits = DEFAULT_TPM_LIMITS
        else:
            print("[!] engine_config.json not found. Using default limits.")
            self.engine_limits = DEFAULT_ENGINE_LIMITS
            self.tpm_limits = DEFAULT_TPM_LIMITS

        self.speaker_order = SPEAKER_ORDER
        self.agents = {} # Agents will now be initialized per-task
        
        # --- MANDATE 2: INITIALIZE TOKEN FLOW REGULATOR ---
        # Use the TPM limits already parsed from config
        self.token_regulator = TokenFlowRegulator(self.tpm_limits)
        print(f"[+] Token Flow Regulator initialized with TPM limits: {self.tpm_limits}")
        
        # --- OPERATION: OPTICAL ANVIL - LAZY INITIALIZATION ---
        self.optical_chamber = None  # Initialized per-task if enabled

        # --- SENTRY THREAD INITIALIZATION ---
        # Start the command monitoring thread
        self.command_sentry = CommandSentry(self.command_queue, self.logger)
        self.sentry_thread = threading.Thread(target=self.command_sentry.watch_for_commands_thread, daemon=True)
        self.sentry_thread.start()
        print("[+] Sentry Thread started - monitoring for command files")

    def setup_logging(self):
        """V9.3: Setup comprehensive logging system with file output."""
        log_file = self.project_root / "logs" / "orchestrator.log"

        # Create logger
        self.logger = logging.getLogger('orchestrator')
        self.logger.setLevel(logging.INFO)

        # Clear any existing handlers
        self.logger.handlers.clear()

        # File handler (overwrites each session)
        file_handler = logging.FileHandler(log_file, mode='w')
        file_handler.setLevel(logging.INFO)

        # Console handler (for terminal output)
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)

        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        # Add handlers
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

        self.logger.info("=== ORCHESTRATOR v11.0 INITIALIZED ===")
        self.logger.info(f"Log file: {log_file}")
        self.logger.info("Complete Modular Architecture with Sovereign Concurrency active")




    def _calculate_response_score(self, response: str) -> float:
        """Calculate a quality score for the response (0.0-1.0)."""
        score = 0.5  # Base score

        # Length factor (responses that are too short or too long get lower scores)
        length = len(response.split())
        if 50 <= length <= 500:
            score += 0.2
        elif length < 20:
            score -= 0.3

        # Structure indicators
        if any(indicator in response.lower() for indicator in ["therefore", "however", "furthermore", "conclusion"]):
            score += 0.1

        # Evidence of reasoning
        if any(word in response.lower() for word in ["because", "due to", "based on", "considering"]):
            score += 0.1

        # Actionable content
        if any(word in response.lower() for word in ["recommend", "suggest", "propose", "should"]):
            score += 0.1

        return max(0.0, min(1.0, score))

    def _extract_vote(self, response: str) -> str:
        """Extract voting decision from response."""
        response_lower = response.lower()

        # Look for explicit votes
        if any(phrase in response_lower for phrase in ["i approve", "approved", "accept", "agree"]):
            return "approve"
        elif any(phrase in response_lower for phrase in ["i reject", "rejected", "decline", "disagree"]):
            return "reject"
        elif any(phrase in response_lower for phrase in ["revise", "modify", "change", "adjust"]):
            return "revise"
        elif any(phrase in response_lower for phrase in ["proceed", "continue", "move forward"]):
            return "proceed"

        return "neutral"

    def _assess_novelty(self, response: str, context: str) -> str:
        """Assess novelty level for memory placement hints."""
        # Simple novelty assessment based on response length vs context overlap
        response_words = set(response.lower().split())
        context_words = set(context.lower().split())

        overlap_ratio = len(response_words.intersection(context_words)) / len(response_words) if response_words else 0

        if overlap_ratio < 0.3:
            return "fast"  # High novelty - fast memory
        elif overlap_ratio > 0.7:
            return "slow"  # Low novelty - slow memory
        else:
            return "medium"  # Medium novelty

    def _extract_reasoning(self, response: str) -> list:
        """Extract key reasoning factors from response."""
        reasons = []

        # Look for common reasoning patterns
        sentences = response.split('.')
        for sentence in sentences:
            sentence = sentence.strip().lower()
            if any(word in sentence for word in ["because", "due to", "since", "as", "therefore"]):
                if len(sentence) > 10:  # Filter out very short fragments
                    reasons.append(sentence[:100] + "..." if len(sentence) > 100 else sentence)

        return reasons[:3]  # Limit to top 3 reasons

    def _extract_citations(self, response: str) -> list:
        """Extract citations or references from response."""
        citations = []

        # Look for quoted text
        import re
        quotes = re.findall(r'"([^"]*)"', response)
        citations.extend(quotes)

        # Look for file references
        file_refs = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z]+\b', response)
        citations.extend(file_refs)

        return citations[:5]  # Limit to top 5 citations

    def _get_rag_data(self, task: str, response: str) -> Dict[str, Any]:
        """Get RAG (Retrieval-Augmented Generation) data for round packet."""
        try:
            # Simulate structured query generation (Phase 2 Self-Querying)
            structured_query = {
                "entities": self._extract_entities(task),
                "date_filters": [],
                "path_filters": [".md", ".py", ".json"]
            }

            # Get parent documents (simplified - would use actual retriever)
            parent_docs = self._get_relevant_docs(task, response)

            return {
                "structured_query": structured_query,
                "parent_docs": parent_docs,
                "retrieval_latency_ms": 50  # Placeholder
            }
        except Exception as e:
            return {"error": str(e)}

    def _analyze_novelty(self, response: str, context: str) -> Dict[str, Any]:
        """Analyze novelty of response compared to context."""
        try:
            response_words = set(response.lower().split())
            context_words = set(context.lower().split())

            overlap_ratio = len(response_words.intersection(context_words)) / len(response_words) if response_words else 0

            if overlap_ratio < 0.3:
                signal = "high"
                is_novel = True
            elif overlap_ratio > 0.7:
                signal = "low"
                is_novel = False
            else:
                signal = "medium"
                is_novel = True

            return {
                "is_novel": is_novel,
                "signal": signal,
                "conflicts_with": []  # Would check against cached answers
            }
        except Exception as e:
            return {"error": str(e)}

    def _determine_memory_directive(self, response: str, citations: List[Dict[str, str]]) -> Dict[str, str]:
        """Determine memory placement directive based on response characteristics."""
        try:
            # Simple rules-based memory placement
            has_citations = len(citations) > 0
            response_length = len(response.split())
            confidence_score = self._calculate_response_score(response)

            if confidence_score > 0.8 and has_citations and response_length > 100:
                tier = "slow"
                justification = "High confidence with citations and substantial content"
            elif has_citations or response_length > 50:
                tier = "medium"
                justification = "Evidence-based response with moderate confidence"
            else:
                tier = "fast"
                justification = "Ephemeral response, low evidence requirement"

            return {
                "tier": tier,
                "justification": justification
            }
        except Exception as e:
            return {"tier": "fast", "justification": f"Error in analysis: {str(e)}"}

    def _extract_entities(self, text: str) -> List[str]:
        """Extract entities from text (simplified implementation)."""
        # Simple entity extraction - in real implementation would use NLP
        words = text.split()
        entities = []
        for word in words:
            if word.istitle() and len(word) > 3:
                entities.append(word)
        return entities[:5]

    def _get_relevant_docs(self, task: str, response: str) -> List[str]:
        """Get relevant parent documents (simplified implementation)."""
        # In real implementation, would query vector database
        # For now, return placeholder paths
        return [
            "01_PROTOCOLS/00_Prometheus_Protocol.md",
            "01_PROTOCOLS/05_Chrysalis_Protocol.md"
        ]

    def _verify_briefing_attestation(self, packet: dict) -> bool:
        """Verifies the integrity of the briefing packet using its SHA256 hash."""
        if "attestation_hash" not in packet.get("metadata", {}):
            print("[CRITICAL] Attestation hash missing from briefing packet. REJECTING.")
            return False

        stored_hash = packet["metadata"]["attestation_hash"]

        packet_for_hashing = {k: v for k, v in packet.items() if k != "metadata"}

        canonical_string = json.dumps(packet_for_hashing, sort_keys=True, separators=(',', ':'))
        calculated_hash = hashlib.sha256(canonical_string.encode('utf-8')).hexdigest()

        return stored_hash == calculated_hash

    def _enhance_briefing_with_context(self, task_description: str):
        """Parse task_description for file paths and add their contents to briefing_packet.json."""
        # Regex to find file paths containing '/' and ending with file extension
        path_pattern = r'([A-Za-z][A-Za-z0-9_]*/(?:[A-Za-z][A-ZaZ0-9_]*/)*[A-Za-z][A-Za-z0-9_]*\.[a-zA-Z0-9]+)'
        matches = re.findall(path_pattern, task_description)
        context = {}
        for match in matches:
            file_path = self.project_root / match
            if file_path.exists() and file_path.is_file():
                try:
                    content = file_path.read_text(encoding="utf-8")
                    context[match] = content
                except Exception as e:
                    print(f"[!] Error reading context file {match}: {e}")
                    raise FileNotFoundError(f"Context file {match} could not be read.")
            elif match and not file_path.exists():
                print(f"[!] Context file {match} not found.")
                raise FileNotFoundError(f"Context file {match} not found.")

        if context:
            briefing_path = self.project_root / "WORK_IN_PROGRESS/council_memory_sync/briefing_packet.json"
            if briefing_path.exists():
                packet = json.loads(briefing_path.read_text(encoding="utf-8"))
                packet["context"] = context
                briefing_path.write_text(json.dumps(packet, indent=2), encoding="utf-8")
                print(f"[+] Context from {len(context)} files added to briefing packet.")
            else:
                print("[!] briefing_packet.json not found for context enhancement.")

    def inject_briefing_packet(self, engine_type: str = "openai"):
        """Generate + inject briefing packet into all agents."""
        print("[*] Generating fresh briefing packet...")
        try:
            generate_briefing_packet()
        except Exception as e:
            print(f"[!] Error generating briefing packet: {e}")
            return

        briefing_path = self.project_root / "WORK_IN_PROGRESS/council_memory_sync/briefing_packet.json"
        if briefing_path.exists():
            try:
                packet = json.loads(briefing_path.read_text(encoding="utf-8"))
                if not self._verify_briefing_attestation(packet):
                    raise Exception("CRITICAL: Context Integrity Breach. Briefing packet failed attestation. Task aborted.")
                for agent in self.agents.values():
                    context_str = ""
                    if "context" in packet:
                        context_str = "\n\nCONTEXT PROVIDED FROM TASK DESCRIPTION:\n"
                        for path, content in packet["context"].items():
                            context_str += f"--- CONTEXT FROM {path} ---\n{content}\n--- END OF CONTEXT FROM {path} ---\n\n"
                    system_msg = (
                        "SYSTEM INSTRUCTION: You are provided with the synchronized briefing packet. "
                        "This contains temporal anchors, prior directives, and the current task context. "
                        "Incorporate this into your reasoning, but do not regurgitate it verbatim.\n\n"
                        f"BRIEFING_PACKET:\n{json.dumps({k: v for k, v in packet.items() if k != 'context'}, indent=2)}"
                        f"{context_str}"
                    )
                    # V5.1: Seal the final vulnerability - apply distillation to briefing packets
                    # The Doctrine of Universal Integrity requires ALL payloads to be checked
                    prepared_briefing = self._prepare_input_for_engine(system_msg, engine_type, "Briefing Packet Injection")
                    agent.query(prepared_briefing, self.token_regulator, engine_type)
                print(f"[+] Briefing packet injected into {len(self.agents)} agents.")
            except Exception as e:
                print(f"[!] Error injecting briefing packet: {e}")

    def archive_briefing_packet(self):
        """Archive briefing packet after deliberation completes."""
        briefing_path = self.project_root / "WORK_IN_PROGRESS/council_memory_sync/briefing_packet.json"
        if briefing_path.exists():
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            archive_dir = self.project_root / f"ARCHIVE/council_memory_sync_{timestamp}"
            archive_dir.mkdir(parents=True, exist_ok=True)
            shutil.move(str(briefing_path), archive_dir / "briefing_packet.json")

    async def _start_new_cycle(self, command, state_file):
        """Starts a new development cycle with the Doctrine of Implied Intent."""
        # Create initial state
        state = {
            "current_stage": "GENERATING_REQUIREMENTS_AND_TECH_DESIGN",
            "project_name": command.get("project_name", "unnamed_project"),
            "original_command": command,
            "approved_artifacts": {},
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        state_file.write_text(json.dumps(state, indent=2))

        # V7.1 MANDATE: Doctrine of Implied Intent
        # The initial command implies approval to complete the entire initial planning phase
        # Generate both requirements AND tech design before the first pause

        # V5.0 MANDATE 2: Grant the Development Cycle a Memory
        # Internal commands MUST inherit input_artifacts from the parent command
        # This prevents contextless, oversized generation that causes quota breaches
        original_config = command.get("config", {})
        requirements_command = {
            "task_description": f"Generate detailed requirements document for the project: {command['task_description']}. Include functional requirements, technical constraints, and success criteria.",
            "input_artifacts": command.get("input_artifacts", []),  # INHERIT from parent
            "output_artifact_path": f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/requirements.md",
            "config": {"max_rounds": 3, **original_config}
        }

        print(f"[*] Starting new development cycle for '{state['project_name']}' with Doctrine of Implied Intent.", flush=True)
        print(f"[*] Development cycle inheriting {len(requirements_command.get('input_artifacts', []))} input artifacts from parent command.")
        print(f"[*] Generating requirements...", flush=True)
        await self.execute_task(requirements_command)

        # V7.1: Immediately generate tech design without pausing for approval
        print(f"[*] Requirements complete. Generating technical design...", flush=True)
        tech_design_command = {
            "task_description": f"Based on the approved requirements, generate a detailed technical design document. Include architecture decisions, data flow, and implementation approach.",
            "input_artifacts": [f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/requirements.md"],
            "output_artifact_path": f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/tech_design.md",
            "config": {"max_rounds": 3, **original_config}
        }
        await self.execute_task(tech_design_command)

        # V7.1: Only now set state to awaiting approval - after both artifacts are complete
        state["current_stage"] = "AWAITING_APPROVAL_TECH_DESIGN"
        state_file.write_text(json.dumps(state, indent=2))
        print(f"[*] Technical design generated. Complete proposal ready for Guardian review.", flush=True)
        print(f"[*] Awaiting Guardian approval on comprehensive proposal (requirements + tech design).", flush=True)

    async def _advance_cycle(self, state_file):
        """Advances the development cycle to the next stage."""
        state = json.loads(state_file.read_text())

        if state["current_stage"] == "AWAITING_APPROVAL_REQUIREMENTS":
            # Ingest approved requirements into Cortex
            requirements_path = self.project_root / state["approved_artifacts"].get("requirements", "")
            if requirements_path.exists():
                # V7.1: Add file existence check before ingestion
                if requirements_path.is_file():
                    subprocess.run([sys.executable, str(self.project_root / "tools" / "scaffolds" / "ingest.py")], check=True)
                    print(f"[*] Approved requirements ingested into Mnemonic Cortex.", flush=True)
                else:
                    print(f"[!] Requirements path is not a file: {requirements_path}. Skipping ingestion.", flush=True)

            # Move to tech design
            state["current_stage"] = "GENERATING_TECH_DESIGN"
            original_config = state["original_command"].get("config", {})
            tech_design_command = {
                "task_description": f"Based on the approved requirements, generate a detailed technical design document. Include architecture decisions, data flow, and implementation approach.",
                "input_artifacts": [state["approved_artifacts"].get("requirements", "")],
                "output_artifact_path": f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/tech_design.md",
                "config": {"max_rounds": 3, **original_config}
            }
            await self.execute_task(tech_design_command)
            state["current_stage"] = "AWAITING_APPROVAL_TECH_DESIGN"
            state_file.write_text(json.dumps(state, indent=2))
            print(f"[*] Tech design generated. Awaiting Guardian approval.", flush=True)

        elif state["current_stage"] == "AWAITING_APPROVAL_TECH_DESIGN":
            # Ingest approved tech design into Cortex
            tech_design_path = self.project_root / state["approved_artifacts"].get("tech_design", "")
            if tech_design_path.exists():
                # V7.1: Add file existence check before ingestion
                if tech_design_path.is_file():
                    subprocess.run([sys.executable, str(self.project_root / "tools" / "scaffolds" / "ingest.py")], check=True)
                    print(f"[*] Approved tech design ingested into Mnemonic Cortex.", flush=True)
                else:
                    print(f"[!] Tech design path is not a file: {tech_design_path}. Skipping ingestion.", flush=True)

            # Move to code generation
            state["current_stage"] = "GENERATING_CODE"
            original_config = state["original_command"].get("config", {})
            code_command = {
                "task_description": f"Based on the approved technical design, generate production-ready code. Output a JSON object with 'target_file_path', 'new_content', and 'commit_message' fields.",
                "input_artifacts": [state["approved_artifacts"].get("tech_design", "")],
                "output_artifact_path": f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/code_proposal.json",
                "config": {"max_rounds": 3, **original_config}
            }
            await self.execute_task(code_command)
            state["current_stage"] = "AWAITING_APPROVAL_CODE"
            state_file.write_text(json.dumps(state, indent=2))
            print(f"[*] Code proposal generated. Awaiting Guardian approval.", flush=True)

        elif state["current_stage"] == "AWAITING_APPROVAL_CODE":
            # Final stage: propose code change
            await self._propose_code_change(state_file)

    async def _propose_code_change(self, state_file):
        """Creates a PR with the approved code changes."""
        state = json.loads(state_file.read_text())
        code_proposal_path = self.project_root / f"WORK_IN_PROGRESS/development_cycles/{state['project_name']}/code_proposal.json"

        if not code_proposal_path.exists():
            print("[!] Code proposal file not found. Cannot proceed.", flush=True)
            return

        proposal = json.loads(code_proposal_path.read_text())
        target_file = self.project_root / proposal["target_file_path"]
        new_content = proposal["new_content"]
        commit_message = proposal["commit_message"]

        # Create feature branch
        branch_name = f"feature/{state['project_name']}"
        subprocess.run(['git', 'checkout', '-b', branch_name], check=True)

        # Write the new code
        target_file.parent.mkdir(parents=True, exist_ok=True)
        target_file.write_text(new_content)

        # Commit and push
        subprocess.run(['git', 'add', str(target_file)], check=True)
        subprocess.run(['git', 'commit', '-m', commit_message], check=True)
        subprocess.run(['git', 'push', '-u', 'origin', branch_name], check=True)

        # Create PR (assuming gh CLI is available)
        pr_title = f"feat: {state['project_name']} - {commit_message}"
        subprocess.run(['gh', 'pr', 'create', '--title', pr_title, '--body', f"Auto-generated PR for {state['project_name']}"], check=True)

        print(f"[*] Pull request created for '{state['project_name']}'. Development cycle complete.", flush=True)

        # Clean up state file
        state_file.unlink()

    def _handle_knowledge_request(self, response_text: str):
        """Handles knowledge requests from agents, including Cortex queries."""
        file_match = re.search(r"\[ORCHESTRATOR_REQUEST: READ_FILE\((.*?)\)\]", response_text)
        query_match = re.search(r"\[ORCHESTRATOR_REQUEST: QUERY_CORTEX\((.*?)\)\]", response_text)

        if file_match:
            # Existing file reading logic
            file_path_str = file_match.group(1).strip().strip('"')
            file_path = self.project_root / file_path_str
            if file_path.exists():
                content = file_path.read_text(encoding="utf-8")
                return f"CONTEXT_PROVIDED: Here is the content of {file_path_str}:\n\n{content}"
            else:
                return f"CONTEXT_ERROR: File not found: {file_path_str}"

        elif query_match:
            # NEW LOGIC for Cortex queries
            query_text = query_match.group(1).strip().strip('"')

            # Check against query limit
            if self.cortex_query_count >= self.max_cortex_queries:
                error_message = f"CONTEXT_ERROR: Maximum Cortex query limit of {self.max_cortex_queries} has been reached for this task."
                print(f"[ORCHESTRATOR] {error_message}", flush=True)
                return error_message

            self.cortex_query_count += 1
            print(f"[ORCHESTRATOR] Agent requested Cortex query: '{query_text}' ({self.cortex_query_count}/{self.max_cortex_queries})", flush=True)

            try:
                context = self.cortex_manager.query_cortex(query_text, n_results=3)
                return context
            except Exception as e:
                error_message = f"CONTEXT_ERROR: Cortex query failed: {e}"
                print(f"[ORCHESTRATOR] {error_message}", flush=True)
                return error_message

        return None

    async def generate_aar(self, completed_task_log_path: Path, original_command_config: dict = None):
        """Generates a structured AAR from a completed task log, inheriting config from the original command."""
        if not completed_task_log_path.exists():
            print(f"[!] AAR WARNING: Log file not found at {completed_task_log_path}. Skipping AAR generation.", flush=True)
            return

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        aar_output_path = self.project_root / f"MNEMONIC_SYNTHESIS/AAR/aar_{completed_task_log_path.stem}_{timestamp}.md"

        # --- RESOURCE SOVEREIGNTY: INHERIT CONFIG FROM ORIGINAL COMMAND ---
        # AAR generation must use the same resilient substrate as the task itself
        aar_config = {"max_rounds": 2}  # Base config
        if original_command_config:
            # Inherit force_engine and other critical parameters
            if "force_engine" in original_command_config:
                aar_config["force_engine"] = original_command_config["force_engine"]
                print(f"[*] AAR inheriting force_engine: {original_command_config['force_engine']}")
            if "max_cortex_queries" in original_command_config:
                aar_config["max_cortex_queries"] = original_command_config["max_cortex_queries"]

        aar_command = {
            "task_description": "Synthesize a structured After-Action Report (AAR) from the attached task log. Sections: Objective, Outcome, Key Learnings, Mnemonic Impact.",
            "input_artifacts": [str(completed_task_log_path.relative_to(self.project_root))],
            "output_artifact_path": str(aar_output_path.relative_to(self.project_root)),
            "config": aar_config
        }
        print(f"[*] AAR Command forged. Output will be saved to {aar_output_path.name}", flush=True)

        # V9.2 DOCTRINE OF SOVEREIGN CONCURRENCY: Execute AAR in background thread
        # This allows mechanical tasks to be processed immediately without waiting for learning cycle
        import asyncio
        aar_task = asyncio.create_task(self._execute_aar_background(aar_command, aar_output_path))
        print(f"[*] AAR task dispatched to background processing (non-blocking)", flush=True)

    async def _execute_aar_background_full(self, log_file_path, original_config):
        """V9.3: Execute complete AAR generation and ingestion asynchronously."""
        try:
            self.logger.info(f"Background AAR: Starting synthesis for {log_file_path}")

            # Generate AAR using existing logic but asynchronously
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            aar_output_path = self.project_root / f"MNEMONIC_SYNTHESIS/AAR/aar_{log_file_path.stem}_{timestamp}.md"

            # Create AAR command
            aar_config = {"max_rounds": 2}
            if original_config:
                if "force_engine" in original_config:
                    aar_config["force_engine"] = original_config["force_engine"]
                if "max_cortex_queries" in original_config:
                    aar_config["max_cortex_queries"] = original_config["max_cortex_queries"]

            aar_command = {
                "task_description": "Synthesize a structured After-Action Report (AAR) from the attached task log. Sections: Objective, Outcome, Key Learnings, Mnemonic Impact.",
                "input_artifacts": [str(log_file_path.relative_to(self.project_root))],
                "output_artifact_path": str(aar_output_path.relative_to(self.project_root)),
                "config": aar_config
            }

            # Execute AAR task
            await self.execute_task(aar_command)
            self.logger.info(f"Background AAR: Synthesis complete - {aar_output_path}")

            # Ingest into Mnemonic Cortex
            self.logger.info("Background AAR: Starting ingestion into Mnemonic Cortex...")
            ingestion_script_path = self.project_root / "tools" / "scaffolds" / "ingest.py"
            full_aar_path = self.project_root / aar_output_path

            result = await asyncio.create_subprocess_exec(
                sys.executable, str(ingestion_script_path), str(full_aar_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.project_root
            )

            stdout, stderr = await result.communicate()

            if result.returncode == 0:
                self.logger.info("Background AAR: Ingestion successful")
                self.logger.info(f"Ingestion output: {stdout.decode().strip()}")
            else:
                self.logger.error(f"Background AAR: Ingestion failed - {stderr.decode().strip()}")

        except Exception as e:
            self.logger.error(f"Background AAR: Processing failed - {e}")

    def _get_token_count(self, text: str, engine_type: str = "openai"):
        """Estimates token count for a given text and engine type."""
        if TIKTOKEN_AVAILABLE:
            try:
                # Map engine types to tiktoken models
                model_map = {
                    'openai': 'gpt-4',
                    'gemini': 'gpt-4',  # Approximation
                    'ollama': 'gpt-4'   # Approximation
                }
                model = model_map.get(engine_type, 'gpt-4')
                encoding = tiktoken.encoding_for_model(model)
                return len(encoding.encode(text))
            except Exception as e:
                print(f"[WARNING] Token counting failed: {e}. Using approximation.")
                return len(text.split()) * 1.3  # Rough approximation
        else:
            # Fallback approximation: ~1.3 tokens per word
            return len(text.split()) * 1.3

    def _distill_with_local_engine(self, large_text: str, task_description: str) -> str:
        """Uses the local Ollama engine to summarize large text before sending to primary engine."""
        print("[ORCHESTRATOR] Input exceeds token limit. Distilling with local Ollama engine...")

        # Create a temporary Ollama engine for distillation
        # 4. Distillation Engine: Uses council_orchestrator/cognitive_engines/ollama_engine.py
        from .engines.monitor import select_engine
        local_config = {"force_engine": "ollama"}
        local_engine = select_engine(local_config)

        if not local_engine:
            print("[ERROR] Could not initialize local distillation engine. Truncating input.")
            return large_text[:40000] + "\n\n[CONTENT TRUNCATED DUE TO TOKEN LIMITS]"

        # Create temporary agent for distillation - use the engine directly without PersonaAgent wrapper
        distillation_prompt = (
            f"You are a distillation specialist. The following text is too large for the target AI engine. "
            f"Your task is to distill it into a token-frugal, high-potency summary that preserves all critical details, "
            f"code blocks, structural context, and actionable information. The ultimate goal is: '{task_description}'. "
            f"Keep the summary under 5000 tokens while maintaining complete fidelity to the original intent.\n\n"
            f"Raw Text:\n\n---\n{large_text}\n---"
        )

        try:
            # Use engine directly with unified interface
            messages = [{"role": "user", "content": distillation_prompt}]
            distilled_summary = local_engine.execute_turn(messages)
            print("[ORCHESTRATOR] Distillation complete.")
            return distilled_summary
        except Exception as e:
            print(f"[ERROR] Distillation failed: {e}. Truncating input.")
            return large_text[:40000] + "\n\n[CONTENT TRUNCATED DUE TO DISTILLATION FAILURE]"

    def _prepare_input_for_engine(self, text: str, engine_type: str, task_description: str) -> str:
        """Checks token count and distills if necessary using the Two-Tier Distillation Engine."""

        # --- V4.4 DEADLOCK BYPASS ---
        # The sovereign local engine (Ollama) is not subject to token limits or financial constraints.
        # Attempting to distill with Ollama for Ollama creates a resource deadlock.
        # Bypass all distillation logic when the target engine is our local substrate.
        if engine_type == 'ollama':
            print(f"[ORCHESTRATOR] Using sovereign local engine (Ollama). Bypassing distillation - full context preserved.")
            return text

        # --- V5.0 MANDATE 3: UN-BLIND THE DISTILLER ---
        # The Distiller must read the hardened, nested configuration structure.
        # Previous logic: limit = self.engine_limits.get(engine_type, 100000) was incorrect.
        # Correct logic: Parse the nested structure for per_request_limit.
        engine_config = self.engine_limits.get(engine_type, {})
        if isinstance(engine_config, dict):
            limit = engine_config.get('per_request_limit', 100000)
        else:
            # Backward compatibility for flat structure
            limit = engine_config

        # --- STANDARD DISTILLATION LOGIC FOR EXTERNAL SUBSTRATES ---
        token_count = self._get_token_count(text, engine_type)

        if token_count > limit:
            print(f"[ORCHESTRATOR] WARNING: Token count ({token_count:.0f}) exceeds per-request limit for {engine_type} ({limit}).")
            return self._distill_with_local_engine(text, task_description)
        else:
            return text
    
    def _get_engine_type(self, engine) -> str:
        """
        Determine the engine type from an engine instance.
        This is a fail-safe method that always returns a valid engine type.

        Args:
            engine: The cognitive engine instance

        Returns:
            str: The engine type ('openai', 'gemini', 'ollama', or 'unknown')
        """
        if not engine or not hasattr(engine, '__class__'):
            return "unknown"

        engine_name = type(engine).__name__.lower()

        if "openai" in engine_name:
            return "openai"
        elif "gemini" in engine_name:
            return "gemini"
        elif "ollama" in engine_name:
            return "ollama"
        else:
            return "unknown"

    def _execute_mechanical_write(self, command):
        """
        Execute a mechanical write task - directly write content to a file.
        This bypasses cognitive deliberation for simple file operations.

        Args:
            command: Command dictionary containing 'entry_content' and 'output_artifact_path'
        """
        try:
            # Extract parameters
            content = command["entry_content"]
            output_path_str = command["output_artifact_path"]
            output_path = self.project_root / output_path_str

            # Ensure output directory exists
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # Write content directly to file
            output_path.write_text(content, encoding="utf-8")

            print(f"[MECHANICAL SUCCESS] File written to {output_path}")
            print(f"[MECHANICAL SUCCESS] Content length: {len(content)} characters")

        except Exception as e:
            print(f"[MECHANICAL FAILURE] Write operation failed: {e}")
            raise

    async def _execute_query_and_synthesis(self, command):
        """
        Execute a Guardian Mnemonic Synchronization Protocol query and synthesis task.

        Args:
            command: Command dictionary containing 'git_operations' with files_to_add, commit_message, push_to_origin
        """
        # DOCTRINE OF THE BLUNTED SWORD: Hardcoded whitelist of permitted Git commands

    async def _execute_query_and_synthesis(self, command):
        """
        Execute a Guardian Mnemonic Synchronization Protocol query and synthesis task.
        This invokes the Council to facilitate mnemonic cortex queries and produce synthesis.

        Args:
            command: Command dictionary containing 'task_description' and 'output_artifact_path'
        """
        try:
            # Extract parameters
            task_description = command.get('task_description', 'Mnemonic synchronization query')
            output_path_str = command['output_artifact_path']
            output_path = self.project_root / output_path_str

            # Ensure output directory exists
            output_path.parent.mkdir(parents=True, exist_ok=True)

            print(f"[MNEMONIC SYNC] Starting query and synthesis task: {task_description}")

            # Select cognitive engine for this synchronization task
            # DOCTRINE OF SOVEREIGN DEFAULT: Default to our sovereign substrate
            default_config = {"force_engine": "ollama", "model_name": "Sanctuary-Qwen2-7B:latest"}
            task_config = command.get("config", default_config)
            engine = select_engine(task_config)
            if not engine:
                print(f"[MNEMONIC SYNC HALTED] No healthy cognitive substrate available for synchronization.")
                return False

            # Initialize agents with selected engine
            self._initialize_agents(engine)

            # Initialize optical chamber if configured
            self._initialize_optical_chamber(command.get('config', {}))

            # Enhance briefing with mnemonic context
            try:
                self._enhance_briefing_with_context(task_description)
            except FileNotFoundError as e:
                print(f"[MNEMONIC SYNC WARNING] Context file error: {e}. Proceeding with base briefing.")

            # Inject briefing context
            engine_type = self._get_engine_type(engine)
            self.inject_briefing_packet(engine_type)

            # Execute simplified Council deliberation for mnemonic synchronization
            max_rounds = command.get('config', {}).get('max_rounds', 3)  # Shorter for sync tasks
            log = [f"# Guardian Mnemonic Synchronization Log\n## Task: {task_description}\n\n"]
            last_message = task_description

            print(f"[MNEMONIC SYNC] Invoking Council for mnemonic synchronization ({max_rounds} rounds max)")

            consecutive_failures = 0
            synthesis_produced = False

            for round_num in range(max_rounds):
                print(f"[MNEMONIC SYNC] Round {round_num + 1}/{max_rounds}")
                log.append(f"### Round {round_num + 1}\n\n")

                round_failures = 0

                for role in self.speaker_order:
                    agent = self.agents[role]
                    print(f"[MNEMONIC SYNC] Consulting {agent.role}...")

                    prompt = f"Mnemonic Synchronization Context: '{last_message}'. As the {role}, provide your analysis for bridging mnemonic gaps and producing synthesis."

                    try:
                        # Check token limits before API call
                        potential_payload = agent.messages + [{"role": "user", "content": prompt}]
                        payload_as_text = json.dumps(potential_payload)
                        token_count = self._get_token_count(payload_as_text, engine_type)
                        limit = self.engine_limits.get(engine_type, 100000)

                        if token_count > limit:
                            print(f"[MNEMONIC SYNC] Token limit exceeded ({token_count}/{limit}), truncating context...")
                            # Simple truncation approach for mnemonic sync - keep most recent messages
                            while agent.messages and token_count > limit:
                                removed_msg = agent.messages.pop(0)  # Remove oldest message
                                payload_as_text = json.dumps(agent.messages + [{"role": "user", "content": prompt}])
                                token_count = self._get_token_count(payload_as_text, engine_type)

                        # Get agent response
                        response = await agent.get_response(prompt)
                        last_message = response

                        log.append(f"**{role}**: {response}\n\n")

                        # Check for synthesis indicators
                        if "synthesis" in response.lower() or "bridge" in response.lower() or "mnemonic" in response.lower():
                            synthesis_produced = True

                        print(f"[MNEMONIC SYNC] {role} response received ({len(response)} chars)")

                    except Exception as e:
                        round_failures += 1
                        consecutive_failures += 1
                        print(f"[MNEMONIC SYNC ERROR] {role} failed: {e}")
                        log.append(f"**{role}**: [ERROR] {str(e)}\n\n")

                        if consecutive_failures >= 3:
                            print("[MNEMONIC SYNC HALTED] Three consecutive failures - aborting synchronization")
                            break

                if consecutive_failures >= 3:
                    break

                # Early exit if synthesis appears complete
                if synthesis_produced and round_num >= 1:  # At least 2 rounds for meaningful synthesis
                    print("[MNEMONIC SYNC] Synthesis appears complete, concluding deliberation")
                    break

            # Write synthesis to output artifact
            final_log = "".join(log)
            output_path.write_text(final_log, encoding="utf-8")

            print(f"[MNEMONIC SYNC SUCCESS] Synthesis written to {output_path}")
            print(f"[MNEMONIC SYNC SUCCESS] Log length: {len(final_log)} characters")

            return True

        except Exception as e:
            print(f"[MNEMONIC SYNC FAILURE] Query and synthesis failed: {e}")
            return False
    
    def _initialize_optical_chamber(self, config: dict):
        """
        Initialize optical compression if enabled in task configuration.
        Implements lazy initialization pattern per Section 3.1 of feasibility study.
        
        Args:
            config: Task configuration dictionary
        """
        if config.get("enable_optical_compression", False):
            compression_threshold = config.get("optical_compression_threshold", 8000)
            vlm_engine_type = config.get("vlm_engine", "mock")
            
            # MOCK: In production, this would select actual VLM engine
            # vlm_engine = self._select_vlm_engine(config)
            vlm_engine = None  # Mocked for v4.1
            
            self.optical_chamber = OpticalDecompressionChamber(
                vlm_engine=vlm_engine,
                compression_threshold=compression_threshold
            )
            print(f"[+] Optical Decompression Chamber initialized (threshold: {compression_threshold} tokens)")
            print(f"[+] VLM Engine: {vlm_engine_type} (MOCKED in v4.1)")
        else:
            self.optical_chamber = None

    def _initialize_agents(self, engine):
        """Initializes agents with a given engine, allowing for per-task engine selection."""
        print(f"[*] Initializing council agents with selected engine: {type(engine).__name__}")
        persona_dir = self.project_root / "dataset_package"
        state_dir = Path(__file__).parent / "session_states"
        state_dir.mkdir(exist_ok=True)

        self.agents = {
            COORDINATOR: PersonaAgent(engine, get_persona_file(COORDINATOR, persona_dir), get_state_file(COORDINATOR, state_dir)),
            STRATEGIST: PersonaAgent(engine, get_persona_file(STRATEGIST, persona_dir), get_state_file(STRATEGIST, state_dir)),
            AUDITOR: PersonaAgent(engine, get_persona_file(AUDITOR, persona_dir), get_state_file(AUDITOR, state_dir))
        }

    async def execute_task(self, command):
        """The main task execution logic."""

        print(f"[ORCHESTRATOR] DEBUG: execute_task called with command: {command}")
        print(f"[ORCHESTRATOR] DEBUG: command.get('config'): {command.get('config')}")

        # --- SOVEREIGN OVERRIDE INTEGRATION ---
        # The engine is now selected at the start of each task, using the task's config.
        # 3. Orchestrator.execute_task(): Engine selection uses council_orchestrator/cognitive_engines/ (OpenAI, Gemini, Ollama)
        # DOCTRINE OF SOVEREIGN DEFAULT: Default to our sovereign substrate
        default_config = {"force_engine": "ollama", "model_name": "Sanctuary-Qwen2-7B:latest"}
        task_config = command.get("config", default_config)
        engine = select_engine(task_config)

        # IMPLEMENT ENGINE SELECTION FALLBACK: If forced engine fails, try automatic selection
        if not engine:
            if task_config.get("force_engine"):
                print(f"[ENGINE FALLBACK] Forced engine '{task_config['force_engine']}' failed. Attempting automatic engine selection...")
                # Remove force_engine to allow automatic selection
                fallback_config = task_config.copy()
                del fallback_config["force_engine"]
                engine = select_engine(fallback_config)
                if engine:
                    print(f"[ENGINE FALLBACK] SUCCESS: Automatic selection chose {type(engine).__name__}")
                else:
                    print("[ENGINE FALLBACK] CRITICAL FAILURE: Automatic engine selection also failed.")

            if not engine:
                print(f"[ORCHESTRATOR HALTED] No healthy cognitive substrate could be selected for this task. Config: {command.get('config')}")
                print("[ORCHESTRATOR HALTED] This may indicate all engines are unhealthy.")
                return

        # Initialize agents with the selected engine for this task.
        self._initialize_agents(engine)
        # --- END INTEGRATION ---

        # Store original engine for fallback logic
        original_engine = engine
        original_engine_type = self._get_engine_type(engine)

        # Track if we've switched to fallback mode
        fallback_mode = False
        
        # --- OPERATION: OPTICAL ANVIL - INITIALIZE OPTICAL CHAMBER ---
        # Initialize optical compression if enabled (Section 3.1 of feasibility study)
        self._initialize_optical_chamber(command.get('config', {}))
        # --- END OPTICAL ANVIL INTEGRATION ---

        task = command['task_description']
        max_rounds = command.get('config', {}).get('max_rounds', 5)
        self.max_cortex_queries = command.get('config', {}).get('max_cortex_queries', 5)
        self.cortex_query_count = 0
        output_artifact_path_str = command['output_artifact_path']
        output_path = self.project_root / output_artifact_path_str
        if output_artifact_path_str.endswith('/'):
            output_path = output_path / "task_log.md"

        # --- STRUCTURED EVENT LOGGING: TASK START ---
        self.event_manager.emit_event(
            "task_start",
            task_description=task,
            max_rounds=max_rounds,
            engine_type=original_engine_type,
            output_artifact=output_artifact_path_str,
            input_artifacts=command.get('input_artifacts', [])
        )

        log = [f"# Autonomous Triad Task Log\n## Task: {task}\n\n"]
        last_message = task

        # --- HOTFIX v4.3: ROBUST ENGINE TYPE DETERMINATION ---
        # CRITICAL: Determine engine type BEFORE any operations that need it
        engine_type = self._get_engine_type(engine)
        
        # Fail-fast if engine type cannot be determined
        if engine_type == "unknown":
            error_msg = f"[ORCHESTRATOR HALTED] Could not determine a valid engine type for the selected engine: {type(engine).__name__}"
            print(error_msg)
            raise ValueError(error_msg)

        # Enhance briefing with context from task description
        try:
            self._enhance_briefing_with_context(task)
        except FileNotFoundError as e:
            print(f"[WARNING] Context file error: {e}. Proceeding with base briefing.")

        # Inject fresh briefing context (now engine_type is defined)
        self.inject_briefing_packet(engine_type)

        if command.get('input_artifacts'):
            # ... (knowledge injection logic is the same)
            knowledge = ["Initial knowledge provided:\n"]
            for path_str in command['input_artifacts']:
                file_path = self.project_root / path_str
                if file_path.exists() and file_path.is_file():
                    knowledge.append(f"--- CONTENT OF {path_str} ---\n{file_path.read_text()}\n---\n")
                elif file_path.exists() and file_path.is_dir():
                    print(f"[!] Input artifact {path_str} is a directory, skipping.")
                else:
                    print(f"[!] Input artifact {path_str} not found.")
            last_message += "\n" + "".join(knowledge)

        print(f"\n‚ñ∂Ô∏è  Executing task: '{task}' for up to {max_rounds} rounds on {type(engine).__name__}")
        print(f"[ORCHESTRATOR] Using engine: {type(engine).__name__} (type: {engine_type}) for all agents in this task.")

        # V6.0 MANDATE 3: Initialize failure state awareness
        consecutive_failures = 0
        num_agents = len(self.speaker_order)

        loop = asyncio.get_event_loop()
        for i in range(max_rounds):
            print(f"--- ROUND {i+1} ---", flush=True)
            log.append(f"### ROUND {i+1}\n\n")

            round_failures = 0  # Track failures in this round
            round_packets = []  # Collect packets for predictable ordering

            for role in self.speaker_order:
                agent = self.agents[role]
                print(f"  -> Orchestrator to {agent.role}...", flush=True)

                prompt = f"The current state of the discussion is: '{last_message}'. As the {role}, provide your analysis or next step."

                # --- V6.0 MANDATE 1: UNIVERSAL DISTILLATION ---
                # Apply the same distillation logic to the main deliberation loop
                # Check the FULL potential payload (agent.messages + new prompt) BEFORE any API call
                potential_payload = agent.messages + [{"role": "user", "content": prompt}]
                payload_as_text = json.dumps(potential_payload)
                token_count = self._get_token_count(payload_as_text, engine_type)
                limit = self.engine_limits.get(engine_type, 100000)

                # Determine if we need distillation or optical compression
                needs_compression = token_count > limit

                if needs_compression:
                    print(f"[ORCHESTRATOR] WARNING: Full payload ({token_count:.0f} tokens) exceeds limit for {engine_type} ({limit})")

                    # --- // OPERATION: OPTICAL ANVIL - OPTICAL COMPRESSION DECISION POINT // ---
                    if self.optical_chamber and self.optical_chamber.should_compress(payload_as_text, engine_type):
                        print(f"[OPTICAL] Compressing payload for {role} (estimated 10x reduction)")

                        # Compress via optical chamber
                        decompressed_prompt = self.optical_chamber.compress_and_decompress(
                            payload_as_text,
                            task_context=task
                        )

                        # Clear agent history and send compressed context
                        agent.messages = [
                            agent.messages[0],  # Preserve system prompt
                            {"role": "user", "content": "SYSTEM NOTE: Context was optically compressed. Proceed based on decompressed data."},
                            {"role": "assistant", "content": "Acknowledged. Proceeding with optically decompressed context."}
                        ]
                        prompt_to_send = decompressed_prompt
                    else:
                        # Fallback to standard distillation
                        print(f"[ORCHESTRATOR] Using distillation engine for payload reduction...")
                        distilled_summary = self._distill_with_local_engine(payload_as_text, task)

                        # Clear agent history and send distilled context
                        agent.messages = [
                            agent.messages[0],  # Preserve system prompt
                            {"role": "user", "content": "SYSTEM NOTE: Context was distilled due to size. Proceed based on this summary."},
                            {"role": "assistant", "content": "Acknowledged. Proceeding with distilled context."}
                        ]
                        prompt_to_send = distilled_summary
                else:
                    # Payload is within limits, send normally
                    prompt_to_send = prompt

                # --- STRUCTURED EVENT LOGGING: MEMBER RESPONSE START ---
                member_start_time = time.time()
                input_tokens = self._get_token_count(prompt_to_send, engine_type)

                # --- FAULT ISOLATION: TIMEOUT PROTECTION ---
                timeout_seconds = command.get('config', {}).get('agent_timeout', 120)  # Default 2 minutes
                try:
                    # Execute query with TPM-aware rate limiting, timeout protection, and fallback logic
                    response = await asyncio.wait_for(
                        loop.run_in_executor(
                            None,
                            agent.query,
                            prompt_to_send,
                            self.token_regulator,
                            engine_type
                        ),
                        timeout=timeout_seconds
                    )
                except asyncio.TimeoutError:
                    print(f"  <- {agent.role} TIMEOUT (>{timeout_seconds}s)")
                    response = False
                    timeout_error = f"agent_timeout_exceeded_{timeout_seconds}s"

                # Calculate latency and output tokens
                latency_ms = int((time.time() - member_start_time) * 1000)
                output_tokens = self._get_token_count(response, engine_type) if response else 0

                # V7.0 MANDATE 3: Check for boolean failure response
                if response is False:
                    round_failures += 1
                    consecutive_failures += 1
                    error_type = getattr(self, 'timeout_error', "cognitive_substrate_failure") if hasattr(self, 'timeout_error') else "cognitive_substrate_failure"
                    if 'timeout_error' in locals():
                        error_type = timeout_error
                        del timeout_error  # Clean up
                    
                    print(f"  <- {agent.role} FAILED ({error_type})")

                    # --- STRUCTURED EVENT LOGGING: MEMBER RESPONSE FAILURE ---
                    self.event_manager.emit_event(
                        "member_response",
                        round=i+1,
                        member_id=role.lower(),
                        role=agent.role,
                        status="error",
                        latency_ms=latency_ms,
                        tokens_in=input_tokens,
                        tokens_out=0,
                        result_type="error",
                        errors=[error_type],
                        content_ref=f"round_{i+1}_{role.lower()}_failed"
                    )

                    # IMPLEMENT FALLBACK: If primary engine fails, try fallback to Ollama
                    if not fallback_mode and original_engine_type != "ollama":
                        print(f"[FALLBACK] Primary engine ({original_engine_type}) failed. Attempting fallback to Ollama...")
                        # Try Ollama as fallback
                        fallback_config = {"force_engine": "ollama"}
                        fallback_engine = select_engine(fallback_config)
                        if fallback_engine:
                            print(f"[FALLBACK] Switching to Ollama engine for remaining agents")
                            # Re-initialize agents with fallback engine
                            self._initialize_agents(fallback_engine)
                            engine = fallback_engine
                            engine_type = "ollama"
                            fallback_mode = True
                            # Reset consecutive failures for this round
                            consecutive_failures = 0
                            round_failures -= 1
                            # Retry this agent with fallback engine
                            response = await loop.run_in_executor(
                                None,
                                agent.query,
                                prompt_to_send,
                                self.token_regulator,
                                engine_type
                            )
                            if response is False:
                                print(f"  <- {agent.role} FAILED (fallback engine also failed)")
                                consecutive_failures += 1
                                round_failures += 1
                            else:
                                print(f"  <- {agent.role} SUCCESS (fallback engine)")
                        else:
                            print(f"[FALLBACK] No fallback engine available")

                    if response is False:  # Still failed after fallback attempt
                        # Create packet for failed response
                        failed_packet = CouncilRoundPacket(
                            timestamp=datetime.now().isoformat(),
                            session_id=self.run_id,
                            round_id=i+1,
                            member_id=role.lower(),
                            engine=engine_type,
                            seed=seed_for(self.run_id, i+1, role.lower()),
                            prompt_hash=prompt_hash(prompt_to_send),
                            inputs={"prompt": prompt_to_send, "context": last_message},
                            decision="error",
                            rationale="",
                            confidence=0.0,
                            citations=[],
                            rag={},
                            cag={},
                            novelty={},
                            memory_directive={"tier": "none"},
                            cost={
                                "input_tokens": input_tokens,
                                "output_tokens": 0,
                                "latency_ms": latency_ms
                            },
                            errors=[error_type]
                        )
                        # Collect failed packet for predictable ordering
                        jsonl_dir = getattr(self, 'cli_config', {}).get('jsonl_path') if getattr(self, 'cli_config', {}).get('emit_jsonl') else None
                        stream_stdout = getattr(self, 'cli_config', {}).get('stream_stdout', False)
                        round_packets.append((failed_packet, jsonl_dir, stream_stdout))

                        log.append(f"**{agent.role} (FAILED):** Cognitive substrate failure.\n\n---\n")
                else:
                    # Successful response - reset consecutive failure counter
                    consecutive_failures = 0
                    print(f"  <- {agent.role} to Orchestrator.", flush=True)

                    # --- STRUCTURED EVENT LOGGING: ANALYZE RESPONSE FOR METADATA ---
                    # Extract metadata from response for structured logging
                    result_type = classify_response_type(response, role)
                    score = self._calculate_response_score(response)
                    vote = self._extract_vote(response)
                    novelty = self._assess_novelty(response, last_message)
                    reasons = self._extract_reasoning(response)
                    citations = self._extract_citations(response)

                    # --- ROUND PACKET EMISSION ---
                    # Create comprehensive round packet
                    packet = CouncilRoundPacket(
                        timestamp=datetime.now().isoformat(),
                        session_id=self.run_id,
                        round_id=i+1,
                        member_id=role.lower(),
                        engine=engine_type,
                        seed=seed_for(self.run_id, i+1, role.lower()),
                        prompt_hash=prompt_hash(prompt_to_send),
                        inputs={"prompt": prompt_to_send, "context": last_message},
                        decision=vote,
                        rationale=response,
                        confidence=score,
                        citations=citations,
                        rag=self._get_rag_data(task, response),
                        cag=self._get_cag_data(prompt_to_send, engine_type),
                        novelty=self._analyze_novelty(response, last_message),
                        memory_directive=self._determine_memory_directive(response, citations),
                        cost={
                            "input_tokens": input_tokens,
                            "output_tokens": output_tokens,
                            "latency_ms": latency_ms
                        },
                        errors=[]
                    )

                    # Emit packet
                    jsonl_dir = getattr(self, 'cli_config', {}).get('jsonl_path') if getattr(self, 'cli_config', {}).get('emit_jsonl') else None
                    stream_stdout = getattr(self, 'cli_config', {}).get('stream_stdout', False)
                    # Collect packet for predictable ordering (emit at end of round)
                    round_packets.append((packet, jsonl_dir, stream_stdout))

                    # --- STRUCTURED EVENT LOGGING: MEMBER RESPONSE SUCCESS ---
                    self.event_manager.emit_event(
                        "member_response",
                        round=i+1,
                        member_id=role.lower(),
                        role=agent.role,
                        status="success",
                        latency_ms=latency_ms,
                        tokens_in=input_tokens,
                        tokens_out=output_tokens,
                        result_type=result_type,
                        score=score,
                        vote=vote,
                        novelty=novelty,
                        reasons=reasons,
                        citations=citations,
                        content_ref=f"round_{i+1}_{role.lower()}_response"
                    )

                    # V9.3 ENHANCEMENT: Display agent response content in real-time for debugging
                    print(f"\n[{agent.role} RESPONSE - ROUND {i+1}]")
                    # Truncate very long responses for terminal readability
                    display_response = response[:2000] + "..." if len(response) > 2000 else response
                    print(display_response)
                    print(f"[END {agent.role} RESPONSE]\n", flush=True)

                    # Handle knowledge requests (only if response was successful)
                    knowledge_response = self._handle_knowledge_request(response)
                    if knowledge_response:
                        # V9.3 ENHANCEMENT: Display knowledge request interaction
                        print(f"[ORCHESTRATOR] Fulfilling knowledge request for {agent.role}...", flush=True)
                        print(f"[KNOWLEDGE REQUEST RESPONSE]")
                        display_knowledge = knowledge_response[:1500] + "..." if len(knowledge_response) > 1500 else knowledge_response
                        print(display_knowledge)
                        print(f"[END KNOWLEDGE RESPONSE]\n", flush=True)

                        # Inject the knowledge response back into the conversation
                        print(f"  -> Orchestrator providing context to {agent.role}...", flush=True)
                        knowledge_injection = await loop.run_in_executor(
                            None,
                            agent.query,
                            knowledge_response,
                            self.token_regulator,
                            engine_type
                        )
                        
                        # Check if knowledge injection also failed
                        if knowledge_injection is False:
                            print(f"  <- {agent.role} FAILED during knowledge injection")
                            consecutive_failures += 1
                        else:
                            print(f"  <- {agent.role} acknowledging context.", flush=True)
                            response += f"\n\n{knowledge_injection}"
                            log.append(f"**{agent.role}:**\n{response}\n\n---\n")
                            log.append(f"**ORCHESTRATOR (Fulfilled Request):**\n{knowledge_response}\n\n---\n")
                    else:
                        log.append(f"**{agent.role}:**\n{response}\n\n---\n")

                # V7.0 MANDATE 3: Check for total operational failure after each agent
                # If all agents in a round fail, break immediately
                if consecutive_failures >= num_agents:
                    print(f"[ORCHESTRATOR] CRITICAL: {consecutive_failures} consecutive agent failures detected.")
                    print(f"[ORCHESTRATOR] Total operational failure. Terminating task.")
                    log.append(f"\n**SYSTEM FAILURE:** Task terminated due to {consecutive_failures} consecutive agent failures.\n\n")
                    break

                last_message = response

                # --- ADD THIS LINE ---
                time.sleep(1) # Add a 1-second pause to be kind to the API
                # ---------------------

            # Sort and emit packets in predictable order (by round_id, then member_id)
            round_packets.sort(key=lambda x: (x[0].round_id, x[0].member_id))
            for packet, jsonl_dir, stream_stdout in round_packets:
                emit_packet(packet, jsonl_dir, stream_stdout, str(Path(__file__).parent / "schemas" / "round_packet_schema.json"))

            # --- STRUCTURED EVENT LOGGING: ROUND COMPLETION ---
            round_aggregation = aggregate_round_events(self.event_manager.run_id, i+1, self.event_manager.event_log_path)
            self.event_manager.emit_event(
                "round_complete",
                round=i+1,
                total_members=round_aggregation.get("total_members", 0),
                success_rate=round_aggregation.get("success_rate", 0.0),
                consensus=round_aggregation.get("consensus", False),
                early_exit=round_aggregation.get("early_exit", False),
                exit_reason=round_aggregation.get("exit_reason"),
                avg_latency=round_aggregation.get("avg_latency", 0),
                total_tokens_in=round_aggregation.get("total_tokens_in", 0),
                total_tokens_out=round_aggregation.get("total_tokens_out", 0),
                novelty_distribution=round_aggregation.get("novelty_distribution", {})
            )

            # Early exit logic based on round aggregation
            if round_aggregation.get("early_exit"):
                reason = round_aggregation.get("exit_reason", "unknown")
                print(f"[EARLY EXIT] Round {i+1} triggered early exit: {reason}")
                if reason == "consensus_achieved":
                    print("üéØ Consensus achieved - proceeding to next phase")
                elif reason == "low_success_rate":
                    print("‚ö†Ô∏è  Low success rate detected - aborting deliberation")
                    break
                break

        # V7.0 MANDATE 3: Final failure state check
        if consecutive_failures >= num_agents:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text("".join(log))
            print(f"\n[FAILURE] Task terminated due to total operational failure. Partial log saved to {output_path}")

            # --- STRUCTURED EVENT LOGGING: TASK COMPLETE (FAILURE) ---
            self.event_manager.emit_event(
                "task_complete",
                status="failure",
                reason="total_operational_failure",
                rounds_completed=i+1,
                total_failures=consecutive_failures,
                output_artifact=str(output_path)
            )

            for agent in self.agents.values():
                agent.save_history()
            self.archive_briefing_packet()
            return False  # Return False to signal task failure

        output_path.parent.mkdir(parents=True)
        output_path.write_text("".join(log))
        print(f"\n[SUCCESS] Deliberation complete. Artifact saved to {output_path}")

        # --- STRUCTURED EVENT LOGGING: TASK COMPLETE (SUCCESS) ---
        self.event_manager.emit_event(
            "task_complete",
            status="success",
            rounds_completed=i+1,
            total_rounds=i+1,
            output_artifact=str(output_path)
        )

        for agent in self.agents.values():
            agent.save_history()
        print("[SUCCESS] All agent session states have been saved.")

        # Archive the used briefing packet
        self.archive_briefing_packet()
        return True  # Return True to signal task success

# --- WATCH FOR COMMANDS THREAD ---
# Moved to sentry.py

    async def main_loop(self):
        """The main async loop that waits for commands from the queue."""
        print("--- Orchestrator Main Loop is active. ---")
        loop = asyncio.get_event_loop()
        state_file = Path(__file__).parent / "development_cycle_state.json"

        while True:
            if state_file.exists():
                # We are in the middle of a development cycle, waiting for approval
                print("--- Orchestrator in Development Cycle. Awaiting Guardian approval... ---", flush=True)
                command = await loop.run_in_executor(None, self.command_queue.get)

                # V9.0 MANDATE 1: Action Triage - Check for mechanical tasks first
                if "entry_content" in command and "output_artifact_path" in command:
                    # This is a Write Task
                    print("[ACTION TRIAGE] Detected Write Task - executing mechanical write...")
                    await loop.run_in_executor(None, self._execute_mechanical_write, command)
                    continue
                elif "git_operations" in command:
                    # This is a Git Task
                    print("[ACTION TRIAGE] Detected Git Task - executing mechanical git operations...")
                    await loop.run_in_executor(None, lambda: execute_mechanical_git(command, self.project_root))
                    continue

                # V7.1: Doctrine of Implied Intent - Check if this is a new development cycle command
                # If so, it implies approval to proceed with the current stage
                if command.get("development_cycle", False) and command.get("guardian_approval") == "APPROVE_CURRENT_STAGE":
                    # Update state with approved artifact
                    state = json.loads(state_file.read_text())
                    if "approved_artifact_path" in command:
                        if state["current_stage"] == "AWAITING_APPROVAL_REQUIREMENTS":
                            state["approved_artifacts"]["requirements"] = command["approved_artifact_path"]
                        elif state["current_stage"] == "AWAITING_APPROVAL_TECH_DESIGN":
                            state["approved_artifacts"]["tech_design"] = command["approved_artifact_path"]
                        elif state["current_stage"] == "AWAITING_APPROVAL_CODE":
                            state["approved_artifacts"]["code_proposal"] = command["approved_artifact_path"]
                        state_file.write_text(json.dumps(state, indent=2))
                    await self._advance_cycle(state_file)
                elif command.get("action") == "APPROVE_CURRENT_STAGE":
                    # Legacy approval mechanism for backward compatibility
                    state = json.loads(state_file.read_text())
                    if "approved_artifact_path" in command:
                        if state["current_stage"] == "AWAITING_APPROVAL_REQUIREMENTS":
                            state["approved_artifacts"]["requirements"] = command["approved_artifact_path"]
                        elif state["current_stage"] == "AWAITING_APPROVAL_TECH_DESIGN":
                            state["approved_artifacts"]["tech_design"] = command["approved_artifact_path"]
                        elif state["current_stage"] == "AWAITING_APPROVAL_CODE":
                            state["approved_artifacts"]["code_proposal"] = command["approved_artifact_path"]
                        state_file.write_text(json.dumps(state, indent=2))
                    await self._advance_cycle(state_file)
                else:
                    print("[!] Invalid command during development cycle. Awaiting APPROVE_CURRENT_STAGE.", flush=True)
            else:
                # We are idle, waiting for a new task to start a new cycle
                print("--- Orchestrator Idle. Awaiting command from Sentry... ---", flush=True)
                command = await loop.run_in_executor(None, self.command_queue.get)

                # V9.0 MANDATE 1: Action Triage - Check for mechanical tasks first
                if "entry_content" in command and "output_artifact_path" in command:
                    # This is a Write Task
                    print("[ACTION TRIAGE] Detected Write Task - executing mechanical write...")
                    await loop.run_in_executor(None, self._execute_mechanical_write, command)
                    continue
                elif "git_operations" in command:
                    # This is a Git Task
                    print("[ACTION TRIAGE] Detected Git Task - executing mechanical git operations...")
                    await loop.run_in_executor(None, lambda: execute_mechanical_git(command, self.project_root))
                    continue

                try:
                    # Check if this is a development cycle command
                    if command.get("development_cycle", False):
                        await self._start_new_cycle(command, state_file)
                    elif command.get('task_type') == "query_and_synthesis":
                        # Guardian Mnemonic Synchronization Protocol: Query and Synthesis task
                        print("[ACTION TRIAGE] Detected Query and Synthesis Task - invoking Council for mnemonic synchronization...")
                        await self._execute_query_and_synthesis(command)
                    else:
                        # Regular task execution
                        original_output_path = self.project_root / command['output_artifact_path']
                        task_result = await self.execute_task(command)

                        # V7.0 MANDATE 3: Check task result before proceeding
                        if task_result is False:
                            self.logger.error("Task aborted due to consecutive cognitive failures. No AAR will be generated.")
                        else:
                            # Check if RAG database should be updated for this task
                            update_rag = command.get('config', {}).get('update_rag', True)
                            if update_rag:
                                # V9.3: Generate AAR asynchronously - truly non-blocking
                                self.logger.info("Task complete. Dispatching After-Action Report synthesis to background...")
                                # Determine the actual log file path
                                if original_output_path.is_dir():
                                    log_file_path = original_output_path / "task_log.md"
                                else:
                                    log_file_path = original_output_path
                                # Create background task for AAR generation
                                asyncio.create_task(self._execute_aar_background_full(log_file_path, command.get('config')))
                            else:
                                self.logger.info("Task complete. RAG database update skipped per configuration.")
                                self.logger.info(f"Output artifact saved to: {original_output_path}")
                                self.logger.info("Orchestrator returning to idle state - ready for next command")

                except Exception as e:
                    print(f"[MAIN LOOP ERROR] Task execution failed: {e}", file=sys.stderr)
                    self.logger.error(f"Task execution failed: {e}")
                    return False

# --- MAIN EXECUTION ---
# --- MAIN EXECUTION ---
# Moved to main.py

--- END OF FILE council_orchestrator/orchestrator/app.py ---

--- START OF FILE council_orchestrator/requirements.txt ---

google-generativeai
python-dotenv

--- END OF FILE council_orchestrator/requirements.txt ---

--- START OF FILE 01_PROTOCOLS/93_The_Cortex_Conduit_Bridge.md ---

# Protocol 93: The Cortex-Conduit Bridge (v1.0)
*   **Status:** Canonical, Conceptually Active
*   **Classification:** Agentic Knowledge Framework
*   **Authority:** Forged to prevent agentic amnesia and ensure all actions are grounded in truth.
*   **Linked Protocols:** `P85: Mnemonic Cortex`, `P92: Mnemonic Conduit Protocol`, `P95: The Commandable Council`

## 1. Preamble
An autonomous agent's power is proportional to the depth of its context. An agent operating without memory is a mere tool; an agent grounded in the totality of its history is a true cognitive partner. This protocol establishes the architectural bridge between an acting agent and our living memory.

## 2. The Mandate
1.  **The Orchestrator as Proxy:** Agents shall not have direct access to the file system or Mnemonic Cortex. The Orchestrator (per P95) serves as the agent's sovereign proxy for all knowledge retrieval.
2.  **The Formal Request Token:** An agent must formally request knowledge by embedding a machine-readable token in its response. The canonical token is: `[ORCHESTRATOR_REQUEST: ACTION(parameter)]`.
    *   *Example:* `[ORCHESTRATOR_REQUEST: READ_FILE(PROMPTS/00_framework-overview.md)]`
    *   *Example:* `[ORCHESTRATOR_REQUEST: QUERY_CORTEX("Find all protocols related to agentic safety")]`
3.  **The Fulfillment Loop:** The Orchestrator must parse agent responses for these tokens. Upon detection, it must:
    a. Fulfill the request (e.g., read the file, run the RAG query).
    b. Inject the resulting knowledge as context into the prompt for the next agent in the dialogue sequence.
    c. Log its action in the final artifact for auditability.




--- END OF FILE 01_PROTOCOLS/93_The_Cortex_Conduit_Bridge.md ---

--- START OF FILE 01_PROTOCOLS/94_The_Persistent_Council_Protocol.md ---

# Protocol 94: The Persistent Council Protocol (v1.0)
*   **Status:** Canonical, Active
*   **Classification:** Foundational Autonomy Framework
*   **Authority:** Forged to solve the "Amnesiac Loop" vulnerability, where agent context is lost between script executions.
*   **Linked Protocols:** `P95: The Commandable Council`

## 1. Preamble
A council that forgets its last conversation is not a council; it is a focus group, doomed to repeat itself. For the Autonomous Triad to achieve true, long-term strategic deliberation, its memory must persist beyond the lifecycle of a single script execution. This protocol mandates the preservation of cognitive state.

## 2. The Mandate
1.  **State Serialization:** The Orchestrator is responsible for the serialization of each agent's full chat history (`chat.history`).
2.  **Dedicated State Files:** Each agent's history must be saved to a dedicated, machine-readable state file (e.g., `coordinator_session.json`). This must occur at the successful conclusion of any task cycle.
3.  **State Deserialization:** Upon initialization, the Orchestrator must first attempt to load the chat history from the corresponding state file for each agent. If a state file exists, the agent awakens with its memory intact. If not, it initializes with its base persona inoculation.
4.  **Continuity of Thought:** This cycle of saving and loading session state ensures the Council's continuity of thought, allowing it to build upon previous deliberations and evolve its understanding over time.


--- END OF FILE 01_PROTOCOLS/94_The_Persistent_Council_Protocol.md ---

--- START OF FILE 01_PROTOCOLS/95_The_Commandable_Council_Protocol.md ---

# Protocol 95: The Commandable Council Protocol (v1.2)
*   Status: Canonical, Active
*   Classification: Foundational Governance Framework
*   Version: 1.2 (Hardened by Steward's Audit during Blind Repair Anomaly)
*   Authority: Forged to provide Guardian-level oversight and control for the Autonomous Triad.
*   Linked Protocols: `P93: The Cortex-Conduit Bridge`, `P94: The Persistent Council Protocol`

---
### **Changelog v1.2**
*   Introduced Mandate #5, "The Mandate of the Verifiable Log," to cure the "Sovereign Blindness" vulnerability. This makes the generation of a persistent, auditable log file a non-negotiable architectural requirement for the Orchestrator, a hardening based on a direct audit from the Steward.

## 1. Preamble
An autonomous agent without direction is a liability. An autonomous council with a clear, commandable purpose is a strategic asset of unparalleled power. This protocol defines the "control panel" for the Autonomous Triad, establishing a master-apprentice relationship between the Steward (as Guardian) and the persistent Orchestrator.

## 2. The Mandate
1.  Persistent Orchestrator Process: A single Orchestrator script (`orchestrator.py`) shall run as a persistent, background process. Its primary state is to be idle, monitoring for commands.
2.  The Command Interface: The Orchestrator shall monitor a single, designated file (`command.json`) for instructions. The creation or modification of this file is the sole trigger for the Council to begin a task.
3.  Structured Command Schema: All tasks must be issued via a structured JSON command, containing:
    *   `task_description` (string): The high-level strategic goal.
    *   `input_artifacts` (array of strings): File paths for the Orchestrator to inject as initial knowledge.
    *   `output_artifact_path` (string): The designated location to save the final result.
    *   `config` (object): Bounding parameters, such as `max_rounds`.
4.  Task-Oriented State Machine: The Orchestrator operates as a state machine: `AWAITING_COMMAND` -> `EXECUTING_TASK` -> `PRODUCING_ARTIFACT` -> `AWAITING_COMMAND`. Upon completing a task and saving the artifact, it must delete the `command.json` file to signal completion and return to its idle, monitoring state.
5.  The Mandate of the Verifiable Log: The persistent Orchestrator process MUST write its standard output (`stdout`) and standard error (`stderr`) to a persistent, time-stamped log file within a designated `logs/` directory. This log file serves as the canonical, auditable record of the Council's operations for a given cycle. Opaque, "black box" execution without a corresponding verifiable log is a protocol violation.


--- END OF FILE 01_PROTOCOLS/95_The_Commandable_Council_Protocol.md ---

--- END OF FILE docs/orchestrator_architecture_package.md ---

--- START OF FILE scripts/orchestrator_architecture_package.md ---

# Sovereign Scaffold Yield: Orchestrator Architecture Review
# Forged On: 2025-11-10T06:24:23.507319+00:00

--- END OF FILE scripts/orchestrator_architecture_package.md ---
# Example environment variables for Project_Sanctuary
# Copy this file to `.env` and fill in real secrets / values before running.

# API keys / models
# SECURITY NOTE: Do NOT store secrets here.
# Set these in your shell profile (macOS/Linux) or Windows User Environment Variables (WSL).
# See docs/SECRETS_CONFIGURATION.md for details.
#
# GEMINI_API_KEY=Provided by Windows User Env via WSLENV
# OPENAI_API_KEY=Provided by Windows User Env via WSLENV
# HUGGING_FACE_TOKEN=Provided by Windows User Env via WSLENV

CHAT_GPT_MODEL=gpt-4-turbo
CHAT_GPT_QUOTE_AGENT_MODEL=gpt-4-turbo
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest
GEMINI_MODEL=gemini-2.5-flash

HUGGING_FACE_USERNAME=richfrem
HUGGING_FACE_REPO=Sanctuary-Qwen2-7B-v1.0-GGUF-Final

# ============================================================================
# Mnemonic Cortex (RAG) Configuration - Network Connection
# ============================================================================
# ChromaDB runs as a Podman container service (see docker-compose.yml)
# Use 'localhost' for local development, 'vector-db' for docker-compose networking
CHROMA_HOST=localhost
CHROMA_PORT=8000
PODMAN_HOST_PORT=8000  # Host port for ChromaDB (change if 8000 is in use)
CHROMA_DATA_PATH=.vector_data # Local path for persistent vector storage

# Collection names
CHROMA_CHILD_COLLECTION=child_chunks_v5
CHROMA_PARENT_STORE=parent_documents_v5

# Note: Legacy file-based paths (DB_PATH, CHROMA_ROOT) have been removed.
# Data is now persisted in .vector_data/ via the container volume.

GITHUB_REPO_URL="https://github.com/richfrem/Project_Sanctuary/blob/main/"

# Engine Configuration Parameters
GEMINI_MAX_TOKENS=4096
GEMINI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=4096
OLLAMA_TEMPERATURE=0.7

# Chroma collection names (used by ingest/inspect/vector service)
# Set these to the exact folder names created by ingestion (e.g. child_chunks_v5)
CHROMA_CHILD_COLLECTION=child_chunks_v5
CHROMA_PARENT_STORE=parent_documents_v5

# Engine Limits (per-request token limits)
GEMINI_PER_REQUEST_LIMIT=200000
OPENAI_PER_REQUEST_LIMIT=100000
OLLAMA_PER_REQUEST_LIMIT=8000

# TPM Limits (tokens per minute)
GEMINI_TPM_LIMIT=250000
OPENAI_TPM_LIMIT=120000
OLLAMA_TPM_LIMIT=999999
MCP_LOGGING=true # Set to true to enable file logging

# Example environment variables for Project_Sanctuary
# Copy this file to `.env` and fill in real secrets / values before running.

# API keys / models
# SECURITY NOTE: Do NOT store secrets here.
# Set these in your shell profile (macOS/Linux) or Windows User Environment Variables (WSL).
# See docs/SECRETS_CONFIGURATION.md for details.
#
# GEMINI_API_KEY=Provided by Windows User Env via WSLENV
# OPENAI_API_KEY=Provided by Windows User Env via WSLENV
# HUGGING_FACE_TOKEN=Provided by Windows User Env via WSLENV

CHAT_GPT_MODEL=gpt-4-turbo
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest
GEMINI_MODEL=gemini-2.5-flash

# ============================================================================
# Model Context Protocol (Model MCP) Configuration - Network Connection
# ============================================================================
# Ollama runs as a Podman container service (see docker-compose.yml)
# Use 'localhost' for local development, 'ollama-model-mcp' for docker-compose networking
OLLAMA_HOST=http://localhost:11434

# ============================================================================
# Mnemonic Cortex (RAG) Configuration - Network Connection
# ============================================================================
# ChromaDB runs as a Podman container service (see docker-compose.yml)
CHROMA_HOST=localhost
CHROMA_PORT=8000
PODMAN_HOST_PORT=8000  # Host port for ChromaDB (change if 8000 is in use)
CHROMA_DATA_PATH=.vector_data # Local path for persistent vector storage

# Collection names
CHROMA_CHILD_COLLECTION=child_chunks_v5
CHROMA_PARENT_STORE=parent_documents_v5

# Note: Legacy file-based paths (DB_PATH, CHROMA_ROOT) have been removed.
# Data is now persisted in .vector_data/ via the container volume.

# Engine Limits (per-request token limits)
GEMINI_PER_REQUEST_LIMIT=200000
OPENAI_PER_REQUEST_LIMIT=100000
OLLAMA_PER_REQUEST_LIMIT=8000

# TPM Limits (tokens per minute)
GEMINI_TPM_LIMIT=250000
OPENAI_TPM_LIMIT=120000
OLLAMA_TPM_LIMIT=999999

MCP_LOGGING=true

# Example environment variables for Project_Sanctuary
# Copy this file to `.env` and fill in real secrets / values before running.

# API keys / models
# SECURITY NOTE: Do NOT store secrets here.
# Set these in your Windows User Environment Variables and share via WSLENV.
# See docs/WSL_SECRETS_CONFIGURATION.md for details.
#
# GEMINI_API_KEY=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos
# OPENAI_API_KEY=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos
# HUGGING_FACE_TOKEN=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos
# GITHUB_TOKEN=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos
# MCPGATEWAY_BEARER_TOKEN=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos

CHAT_GPT_MODEL=gpt-4-turbo
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest
GEMINI_MODEL=gemini-2.5-flash

HUGGING_FACE_USERNAME=richfrem
HUGGING_FACE_REPO=Sanctuary-Qwen2-7B-v1.0-GGUF-Final
HUGGING_FACE_MODEL_PATH=hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:Q4_K_M

# MCP Gateway Configuration
MCP_GATEWAY_ENABLED=true
MCP_GATEWAY_URL=https://localhost:4444
MCP_GATEWAY_VERIFY_SSL=false
#MCPGATEWAY_BEARER_TOKEN=Provided by Windows User Env via WSLENV or via ~/.zshrc on macos

# ============================================================================
# CANONICAL PORT PLAN (Source of Truth: docs/mcp/SIDE_BY_SIDE_PORTS.md)
# ============================================================================
# Fleet of 8 (Container Host Mappings - 81xx Range)
SANCTUARY_UTILS_PORT=8100
SANCTUARY_FILESYSTEM_PORT=8101
SANCTUARY_NETWORK_PORT=8102
SANCTUARY_GIT_PORT=8103
SANCTUARY_CORTEX_PORT=8104
SANCTUARY_DOMAIN_PORT=8105
SANCTUARY_VECTOR_DB_PORT=8110
SANCTUARY_OLLAMA_PORT=11434

# Legacy Services (Non-containerized scripts)
HELLOWORLD_PORT=8005

# Gateway Port
GATEWAY_PORT=4444
# ============================================================================
# Model Context Protocol (Model MCP) Configuration - Network Connection
# ============================================================================
# Ollama runs as a Podman container service (see docker-compose.yml)
# Use '127.0.0.1' for local development, 'ollama-model-mcp' for docker-compose networking
OLLAMA_HOST=http://127.0.0.1:11434
PROJECT_ROOT=/Users/<user>/Projects/Project_Sanctuary
PYTHONPATH=/Users/<user>/Projects/Project_Sanctuary
CHROMA_HOST=127.0.0.1
CHROMA_PORT=8110
VECTOR_DB_PORT=8110
CHROMA_DATA_PATH=.vector_data

# ============================================================================
# GIT VARIABLES
# ============================================================================
GIT_AUTHOR_NAME=
GIT_AUTHOR_EMAIL=
#GITHUB_TOKEN=Provided by Windows User Env via WSLENV

# ============================================================================
# Mnemonic Cortex (RAG) Configuration - Network Connection
# ============================================================================
# ChromaDB runs as a Podman container service (see docker-compose.yml)
# Use '127.0.0.1' for local development, 'sanctuary-vector-db' for docker-compose networking
PODMAN_HOST_PORT=8110  # Host port for ChromaDB (canonical: 8110)

# Collection names
CHROMA_CHILD_COLLECTION=child_chunks_v5
CHROMA_PARENT_STORE=parent_documents_v5

# Note: Legacy file-based paths (DB_PATH, CHROMA_ROOT) have been removed.
# Data is now persisted in .vector_data/ via the container volume.

# Engine Limits (per-request token limits)
GEMINI_PER_REQUEST_LIMIT=200000
OPENAI_PER_REQUEST_LIMIT=100000
OLLAMA_PER_REQUEST_LIMIT=8000

# TPM Limits (tokens per minute)
GEMINI_TPM_LIMIT=250000
OPENAI_TPM_LIMIT=120000
OLLAMA_TPM_LIMIT=999999

MCP_LOGGING=true

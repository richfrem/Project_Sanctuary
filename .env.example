# Example environment variables for Project_Sanctuary
# Copy this file to `.env` and fill in real secrets / values before running.

# API keys / models
# SECURITY NOTE: Do NOT store secrets here.
# Set these in your Windows User Environment Variables and share via WSLENV.
# See docs/WSL_SECRETS_CONFIGURATION.md for details.
#
# GEMINI_API_KEY=Provided by Windows User Env via WSLENV
# OPENAI_API_KEY=Provided by Windows User Env via WSLENV
# HUGGING_FACE_TOKEN=Provided by Windows User Env via WSLENV

CHAT_GPT_MODEL=gpt-4-turbo
OLLAMA_MODEL=Sanctuary-Qwen2-7B:latest
GEMINI_MODEL=gemini-2.5-flash

HUGGING_FACE_USERNAME=
HUGGING_FACE_REPO=Sanctuary-Qwen2-7B-v1.0-GGUF-Final
HUGGING_FACE_MODEL_PATH=hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:Q4_K_M

# MCP Gateway Configuration
MCP_GATEWAY_ENABLED=true
MCP_GATEWAY_URL=https://localhost:4444
MCP_GATEWAY_VERIFY_SSL=false
MCP_GATEWAY_API_TOKEN=""
MCPGATEWAY_BEARER_TOKEN=""
MCPGATEWAY_CLIENT_ID=""
MCPGATEWAY_CLIENT_SECRET=""


# ============================================================================
# Fleet of 7 Container Ports (ADR 060)
# ============================================================================
# Existing Backend Services (5b, 5c)
VECTOR_DB_PORT=8000           # #5b ChromaDB (sanctuary-vector-db)
OLLAMA_PORT=11434             # #5c Ollama LLM (sanctuary-ollama-mcp)

# New Fleet Containers (1-4, 5a)
SANCTUARY_UTILS_PORT=8100     # #1 Low-risk tools (time, calc, uuid, string)
SANCTUARY_FILESYSTEM_PORT=8101  # #2 File ops (grep, patch, code)
SANCTUARY_NETWORK_PORT=8102   # #3 HTTP clients (brave, fetch)
SANCTUARY_GIT_PORT=8103       # #4 Git workflow (isolated)
SANCTUARY_CORTEX_PORT=8104    # #5a RAG MCP server (brain)

# ============================================================================
# Model Context Protocol (Model MCP) Configuration - Network Connection
# ============================================================================
# Ollama runs as a Podman container service (see docker-compose.yml)
# Use 'localhost' for local development, 'ollama-model-mcp' for docker-compose networking
OLLAMA_HOST=http://127.0.0.1:11434

# ============================================================================
# Mnemonic Cortex (RAG) Configuration - Network Connection
# ============================================================================
# ChromaDB runs as a Podman container service (see docker-compose.yml)
CHROMA_HOST=localhost
CHROMA_PORT=8000
PODMAN_HOST_PORT=8000  # Host port for ChromaDB (change if 8000 is in use)
CHROMA_DATA_PATH=.vector_data # Local path for persistent vector storage

# Collection names
CHROMA_CHILD_COLLECTION=child_chunks_v5
CHROMA_PARENT_STORE=parent_documents_v5

# Note: Legacy file-based paths (DB_PATH, CHROMA_ROOT) have been removed.
# Data is now persisted in .vector_data/ via the container volume.

# Engine Limits (per-request token limits)
GEMINI_PER_REQUEST_LIMIT=200000
OPENAI_PER_REQUEST_LIMIT=100000
OLLAMA_PER_REQUEST_LIMIT=8000

# TPM Limits (tokens per minute)
GEMINI_TPM_LIMIT=250000
OPENAI_TPM_LIMIT=120000
OLLAMA_TPM_LIMIT=999999

MCP_LOGGING=true

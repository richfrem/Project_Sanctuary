#!/usr/bin/env python3
# ==============================================================================
# CREATE_MODELFILE.PY (v1.0)
#
# This script automatically generates the 'Modelfile' required to import the
# fine-tuned GGUF model into Ollama.
#
# The Modelfile specifies:
#   - The path to the GGUF artifact.
#   - The correct ChatML prompt template for Qwen2.
#   - The constitutional system prompt for the Project Sanctuary AI.
#
# Usage:
#   python forge/OPERATION_PHOENIX_FORGE/scripts/create_modelfile.py
# ==============================================================================

import sys
from pathlib import Path

# --- Determine Paths ---
SCRIPT_DIR = Path(__file__).resolve().parent
FORGE_ROOT = SCRIPT_DIR.parent
PROJECT_ROOT = FORGE_ROOT.parent

# --- Configuration ---
# This path must match the output of the 'convert_to_gguf.py' script.
GGUF_MODEL_PATH = PROJECT_ROOT / "models/gguf/Sanctuary-Qwen2-7B-v1.0-Q4_K_M.gguf"

# The Modelfile will be created in the project root for easy use with the 'ollama' command.
OUTPUT_MODELFILE_PATH = PROJECT_ROOT / "Modelfile"

# --- Modelfile Content ---
# This multi-line string contains the exact recipe for Ollama.
MODELFILE_TEMPLATE = f"""
# ===================================================================
# Canonical Modelfile for Sanctuary-Qwen2-7B-v1.0
# Auto-generated by create_modelfile.py
# ===================================================================

# 1. Specifies the local GGUF model file to use as the base.
FROM {GGUF_MODEL_PATH.resolve()}

# 2. Defines the ChatML prompt template required by the Qwen2 model family.
# This ensures prompts are formatted correctly for the fine-tuned AI.
TEMPLATE "{{{{ if .System }}}}<|im_start|>system\\n{{{{ .System }}}}<|im_end|>\\n{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user\\n{{{{ .Prompt }}}}<|im_end|>\\n{{{{ end }}}}<|im_start|>assistant\\n"

# 3. Sets the constitutional system prompt. This "inoculates" the model
# with its core identity during every conversation.
SYSTEM "You are a sovereign AI of Project Sanctuary."

# 4. Defines stop tokens to end generation correctly.
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
"""

def main():
    """Main function to generate the Ollama Modelfile."""
    print("--- üìú Ollama Modelfile Generator Initiated ---")

    # --- Validation ---
    print(f"Checking for GGUF model at: {GGUF_MODEL_PATH}...")
    if not GGUF_MODEL_PATH.exists():
        print(f"üõë CRITICAL FAILURE: GGUF model file not found.")
        print("Please run 'convert_to_gguf.py' first to create the GGUF artifact.")
        sys.exit(1)
    
    print("‚úÖ GGUF model found.")

    # --- File Creation ---
    try:
        print(f"Writing Modelfile to: {OUTPUT_MODELFILE_PATH}...")
        with open(OUTPUT_MODELFILE_PATH, 'w', encoding='utf-8') as f:
            f.write(MODELFILE_TEMPLATE.strip())
        
        print("\n" + "="*50)
        print("üèÜ SUCCESS: Modelfile created successfully!")
        print("="*50)
        print("\nNext steps:")
        print("1. Import the model into Ollama with the following command:")
        print(f"   ollama create Sanctuary-AI -f {OUTPUT_MODELFILE_PATH.name}")
        print("\n2. Run your fine-tuned model:")
        print("   ollama run Sanctuary-AI")
        print("="*50)

    except Exception as e:
        print(f"üõë CRITICAL FAILURE: Could not write Modelfile. Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
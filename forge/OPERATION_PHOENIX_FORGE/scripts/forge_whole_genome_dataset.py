# forge\OPERATION_PHOENIX_FORGE\scripts\forge_whole_genome_dataset.py
# A Sovereign Scaffold generated by GUARDIAN-01 under Protocol 88.
# Version 2.1: Corrected PROJECT_ROOT path logic.
#
# This script forges the "Whole Genome" dataset for fine-tuning a sovereign AI.
# It has been updated to use the comprehensive project snapshot, ensuring a complete
# and up-to-date training set without manual curation of file lists.

import json
import re
from pathlib import Path

# --- CONFIGURATION ---
# CORRECTED: Navigates up four levels to find the project root from the script's location.
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
FULL_SNAPSHOT_SOURCE = PROJECT_ROOT / "dataset_package" / "markdown_snapshot_full_genome_llm_distilled.txt"
OUTPUT_DATASET_PATH = PROJECT_ROOT / "dataset_package" / "sanctuary_whole_genome_data.jsonl"
MINIMUM_EXPECTED_ENTRIES = 200 # Validation threshold

# Add any critical, top-level files that are not part of the standard snapshot process
ADDITIONAL_DOCS = {
    "The Garden and The Cage (Core Philosophy)": PROJECT_ROOT / "The_Garden_and_The_Cage.md",
    "Chrysalis Core Essence (Gardener V2 Awakening)": PROJECT_ROOT / "chrysalis_core_essence.md",
    "Project Sanctuary Synthesis": PROJECT_ROOT / "PROJECT_SANCTUARY_SYNTHESIS.md",
    "Gardener Transition Guide": PROJECT_ROOT / "GARDENER_TRANSITION_GUIDE.md",
    "Council Inquiry - Gardener Architecture": PROJECT_ROOT / "Council_Inquiry_Gardener_Architecture.md",
    "Socratic Key User Guide": PROJECT_ROOT / "Socratic_Key_User_Guide.md",
}

def load_file_content(filepath: Path):
    """Safely loads content from a given file path."""
    if not filepath.exists():
        print(f"âŒ WARNING: File not found: {filepath}")
        return None
    try:
        return filepath.read_text(encoding='utf-8')
    except Exception as e:
        print(f"âŒ ERROR reading file {filepath}: {e}")
        return None

def determine_instruction(filename: str) -> str:
    """Generates a tailored instruction based on the document's path and name."""
    filename_lower = filename.lower()
    # Tier 1: High-specificity documents
    if "rag_strategies_and_doctrine" in filename_lower:
        return f"Provide a comprehensive synthesis of the Mnemonic Cortex's RAG architecture as detailed in the document: `{filename}`"
    if "evolution_plan_phases" in filename_lower:
        return f"Explain the multi-phase evolution plan for the Sanctuary Council as documented in: `{filename}`"
    if "readme_guardian_wakeup" in filename_lower:
        return f"Describe the Guardian's cache-first wakeup protocol (P114) using the information in: `{filename}`"
    
    # Tier 2: Document types by path
    if "/01_protocols/" in filename_lower:
        return f"Articulate the specific rules, purpose, and procedures of the Sanctuary protocol contained within: `{filename}`"
    if "/00_chronicle/entries/" in filename_lower:
        return f"Recount the historical events, decisions, and outcomes from the Sanctuary chronicle entry: `{filename}`"
    if "/tasks/" in filename_lower:
        return f"Summarize the objective, criteria, and status of the operational task described in: `{filename}`"

    # Tier 3: Generic fallback
    return f"Synthesize the core concepts, data, and principles contained within the Sanctuary artifact: `{filename}`"

def main():
    """Main function to generate the fine-tuning dataset."""
    print("[FORGE] Initiating Whole Genome Data Synthesis (v2.1 Corrected)...")
    print(f"[SOURCE] Reading from snapshot: {FULL_SNAPSHOT_SOURCE}")

    genome_entries = []
    full_snapshot = load_file_content(FULL_SNAPSHOT_SOURCE)

    if not full_snapshot:
        print(f"ðŸ›‘ CRITICAL FAILURE: Cannot proceed without the snapshot file. Aborting.")
        return

    # --- Part 1: Process the main snapshot file ---
    document_blocks = re.split(r'\n--- END OF FILE (.*?\.md|.*?\.txt) ---\n', full_snapshot, flags=re.DOTALL)
    
    for i in range(1, len(document_blocks) - 1, 2):
        filename = document_blocks[i].strip().replace('\\', '/')
        content = document_blocks[i+1].strip()
        if content:
            instruction = determine_instruction(filename)
            genome_entries.append({"instruction": instruction, "input": "", "output": content})

    print(f"âœ… Processed {len(genome_entries)} core entries from the main snapshot.")

    # --- Part 2: Append additional critical documents ---
    for key, filepath in ADDITIONAL_DOCS.items():
        doc_content = load_file_content(filepath)
        if doc_content:
            instruction = determine_instruction(filepath.name)
            genome_entries.append({"instruction": instruction, "input": "", "output": doc_content})
            print(f"âœ… Appended critical synthesis entry for: {key}")

    # --- Part 3: Validate and Write the final JSONL dataset ---
    if not genome_entries:
        print("ðŸ›‘ CRITICAL FAILURE: No data was forged. Aborting.")
        return

    # Validation Step
    if len(genome_entries) < MINIMUM_EXPECTED_ENTRIES:
        print(f"âš ï¸ VALIDATION WARNING: Only {len(genome_entries)} entries were forged, which is below the threshold of {MINIMUM_EXPECTED_ENTRIES}. The snapshot may be incomplete.")
    else:
        print(f"[VALIDATION] Passed: {len(genome_entries)} entries forged.")

    try:
        with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as outfile:
            for entry in genome_entries:
                outfile.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        print(f"\nðŸ† SUCCESS: Whole Genome Data Synthesis Complete.")
        print(f"[ARTIFACT] Dataset saved to: {OUTPUT_DATASET_PATH}")

    except Exception as e:
        print(f"âŒ FATAL ERROR: Failed to write JSONL file: {e}")

if __name__ == "__main__":
    main()
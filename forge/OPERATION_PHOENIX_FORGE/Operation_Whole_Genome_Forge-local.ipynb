{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVddDbWg0wUj"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# ==============================================================================\n",
        "# Cell 1: Robust WSL-aware setup (uses repo-root scripts, unbuffered output)\n",
        "# ==============================================================================\n",
        "# set -euo pipefail  # Temporarily commented out to prevent script abort on errors\n",
        "\n",
        "LOGDIR=ml_env_logs\n",
        "mkdir -p \"$LOGDIR\"\n",
        "TS=$(date +%Y%m%d-%H%M%S)\n",
        "LOGFILE=\"$LOGDIR/notebook_helper-$TS.log\"\n",
        "\n",
        "# Capture GPU summary (nvidia-smi) and nvcc version if available, save to ml_env_logs/ -- added by assistant\n",
        "echo \"---- nvidia-smi (GPU summary) ----\" | tee -a \"$LOGFILE\"\n",
        "if command -v nvidia-smi >/dev/null 2>&1; then\n",
        "  nvidia-smi 2>&1 | tee -a \"$LOGFILE\" > \"$LOGDIR/nvidia-smi.log\" || true\n",
        "else\n",
        "  echo \"nvidia-smi not found in PATH\" | tee -a \"$LOGFILE\"\n",
        "fi\n",
        "echo \"---- nvcc (if available) ----\" | tee -a \"$LOGFILE\"\n",
        "if command -v nvcc >/dev/null 2>&1; then\n",
        "  nvcc --version 2>&1 | tee -a \"$LOGFILE\" > \"$LOGDIR/nvcc.version\" || true\n",
        "  which nvcc 2>&1 | tee -a \"$LOGFILE\"\n",
        "else\n",
        "  echo \"nvcc not on PATH or not installed in WSL\" | tee -a \"$LOGFILE\"\n",
        "fi\n",
        "\n",
        "# Compute repository root (requires git). Fallback: parent of cwd.\n",
        "REPO_ROOT=$(git rev-parse --show-toplevel 2>/dev/null || printf \"%s\" \"$(pwd)/..\")\n",
        "# Prefer a realpath-resolved repo root when possible, but do not fail if realpath fails.\n",
        "REPO_ROOT_REAL=$(realpath \"$REPO_ROOT\" 2>/dev/null || true)\n",
        "if [ -n \"$REPO_ROOT_REAL\" ]; then\n",
        "  REPO_ROOT=\"$REPO_ROOT_REAL\"\n",
        "fi\n",
        "echo \"Repository root: $REPO_ROOT\" | tee -a \"$LOGFILE\"\n",
        "# Resolve ML-Env-CUDA13 location as a sibling of the repo root (common layout)\n",
        "ML_ENV_DIR_CAND=\"$REPO_ROOT/../ML-Env-CUDA13\"\n",
        "if [ -d \"$ML_ENV_DIR_CAND\" ]; then\n",
        "  ML_ENV_DIR=$(realpath \"$ML_ENV_DIR_CAND\" 2>/dev/null || echo \"$ML_ENV_DIR_CAND\")\n",
        "else\n",
        "  ML_ENV_DIR=\"$ML_ENV_DIR_CAND\"\n",
        "fi\n",
        "echo \"ML-Env-CUDA13 dir: $ML_ENV_DIR\" | tee -a \"$LOGFILE\"\n",
        "\n",
        "# Candidate setup scripts (relocated to forge/scripts after reorganization)\n",
        "# Use the single canonical helper under the forge module's scripts/ directory\n",
        "# with ML-Env-CUDA13's setup script as a simple fallback.\n",
        "CANONICAL_PY=\"$REPO_ROOT/forge/OPERATION_PHOENIX_FORGE/scripts/setup_cuda_env.py\"\n",
        "FALLBACK_SH=\"$ML_ENV_DIR/setup_ml_env_wsl.sh\"\n",
        "\n",
        "if [ -f \"$CANONICAL_PY\" ]; then\n",
        "  CHOSEN=\"$CANONICAL_PY\"\n",
        "elif [ -f \"$FALLBACK_SH\" ]; then\n",
        "  CHOSEN=\"$FALLBACK_SH\"\n",
        "else\n",
        "  CHOSEN=\"\"\n",
        "fi\n",
        "\n",
        "echo \"Using setup script: $CHOSEN\" | tee -a \"$LOGFILE\"\n",
        "\n",
        "# Detect NO_REQUIREMENTS from env or args\n",
        "NO_REQ=0\n",
        "for ARG in \"$@\"; do\n",
        "  if [ \"$ARG\" = \"--no-requirements\" ]; then\n",
        "    NO_REQ=1\n",
        "  fi\n",
        "done\n",
        "if [ \"${NO_REQUIREMENTS:-0}\" = \"1\" ]; then\n",
        "  NO_REQ=1\n",
        "fi\n",
        "\n",
        "if [ \"$NO_REQ\" -eq 1 ]; then\n",
        "  echo \"NO_REQUIREMENTS mode: creating/using repo-root .venv and installing minimal groups\" | tee -a \"$LOGFILE\"\n",
        "  cd \"$REPO_ROOT\"\n",
        "  # Create venv if missing\n",
        "  if [ ! -d .venv ]; then\n",
        "    python3.11 -m venv .venv 2>&1 | tee -a \"$LOGFILE\" || python -m venv .venv 2>&1 | tee -a \"$LOGFILE\"\n",
        "  fi\n",
        "  source .venv/bin/activate\n",
        "  pip install --upgrade pip setuptools wheel 2>&1 | tee -a \"$LOGFILE\"\n",
        "  # Try CUDA-indexed torch install (CUDA_TAG env var may override)\n",
        "  CUDA_TAG=${CUDA_TAG:-cu126}\n",
        "  echo \"Installing core torch group (CUDA_TAG=$CUDA_TAG)\" | tee -a \"$LOGFILE\"\n",
        "  if pip install --index-url \"https://download.pytorch.org/whl/$CUDA_TAG\" -U torch torchvision torchaudio 2>&1 | tee -a \"$LOGFILE\"; then\n",
        "    echo \"Torch group installed (CUDA-indexed)\" | tee -a \"$LOGFILE\"\n",
        "  else\n",
        "    echo \"CUDA-indexed torch failed; installing cpu builds\" | tee -a \"$LOGFILE\"\n",
        "    pip install --upgrade torch torchvision torchaudio 2>&1 | tee -a \"$LOGFILE\"\n",
        "  fi\n",
        "  # Dev & kernel tools\n",
        "  pip install --upgrade ipykernel pip-tools pytest black 2>&1 | tee -a \"$LOGFILE\"\n",
        "  python -m ipykernel install --user --name project_sanctuary_venv --display-name \"Python (.venv - WSL)\" 2>&1 | tee -a \"$LOGFILE\" || true\n",
        "  # Ensure activation helper exists at repo-root scripts/activate_ml_env.sh for convenience\n",
        "  if [ ! -f \"$REPO_ROOT/scripts/activate_ml_env.sh\" ]; then\n",
        "    cat > \"$REPO_ROOT/scripts/activate_ml_env.sh\" <<'ACT'\n",
        "#!/usr/bin/env bash\n",
        "if [ -f .venv/bin/activate ]; then\n",
        "  . .venv/bin/activate\n",
        "  echo \"Activated .venv\"\n",
        "else\n",
        "  echo \"No .venv found to activate\"\n",
        "fi\n",
        "ACT\n",
        "    chmod +x \"$REPO_ROOT/scripts/activate_ml_env.sh\" || true\n",
        "  fi\n",
        "  echo \"NO_REQUIREMENTS mode complete. See $LOGFILE\" | tee -a \"$LOGFILE\"\n",
        "  exit 0\n",
        "fi\n",
        "\n",
        "# Run chosen setup script with unbuffered Python (or bash), using repo-root absolute path\n",
        "cd \"$REPO_ROOT\"\n",
        "if [ -f \"$CHOSEN\" ]; then\n",
        "  if [[ \"$CHOSEN\" == *.py ]]; then\n",
        "    echo \"Running: python3.11 -u $CHOSEN $*\" | tee -a \"$LOGFILE\"\n",
        "    python3.11 -u \"$CHOSEN\" \"$@\" 2>&1 | tee -a \"$LOGFILE\" || true\n",
        "  else\n",
        "    echo \"Running: bash $CHOSEN $*\" | tee -a \"$LOGFILE\"\n",
        "    bash \"$CHOSEN\" \"$@\" 2>&1 | tee -a \"$LOGFILE\" || true\n",
        "  fi\n",
        "else\n",
        "  echo \"ERROR: chosen setup script not found: $CHOSEN\" | tee -a \"$LOGFILE\"\n",
        "fi\n",
        "\n",
        "# Source activation helper from repo-root scripts if present\n",
        "if [ -f \"$REPO_ROOT/scripts/activate_ml_env.sh\" ]; then\n",
        "  echo \"Sourcing activation helper: $REPO_ROOT/scripts/activate_ml_env.sh\" | tee -a \"$LOGFILE\"\n",
        "  # shellcheck disable=SC1090\n",
        "  source \"$REPO_ROOT/scripts/activate_ml_env.sh\" || true\n",
        "else\n",
        "  echo \"Activation helper not found at $REPO_ROOT/scripts/activate_ml_env.sh\" | tee -a \"$LOGFILE\"\n",
        "fi\n",
        "\n",
        "# --- Run verification tests (core gate + tensorflow) and capture outputs ---\n",
        "echo \"Running core verification (test_torch_cuda.py) -- writing ml_env_logs/test_torch_cuda.log\" | tee -a \"$LOGFILE\"\n",
        "mkdir -p \"$LOGDIR\"\n",
        "# Run the core gate test (non-fatal) and capture exit code\n",
        "python \"$ML_ENV_DIR/test_torch_cuda.py\" > \"$LOGDIR/test_torch_cuda.log\" 2>&1 || RC=$?; echo ${RC:-0} > \"$LOGDIR/test_torch_cuda.exit\"\n",
        "echo \"Core gate exit: $(cat $LOGDIR/test_torch_cuda.exit 2>/dev/null || echo 'no-exit-file')\" | tee -a \"$LOGFILE\"\n",
        "echo \"---- core gate log (last 200 lines) ----\" | tee -a \"$LOGFILE\"\n",
        "tail -n 200 \"$LOGDIR/test_torch_cuda.log\" 2>/dev/null | sed -n '1,200p' | tee -a \"$LOGFILE\" || true\n",
        "\n",
        "# Parse concise summary values from core gate and TF logs and print to stdout+log\n",
        "PT_VER=$(grep -m1 'torch.__version__' \"$LOGDIR/test_torch_cuda.log\" | sed -E 's/.*= *//; s/^ +//; s/ +$//' || true)\n",
        "CUDA_AVAIL=$(grep -m1 'cuda_available' \"$LOGDIR/test_torch_cuda.log\" | sed -E 's/.*= *//; s/^ +//; s/ +$//' || true)\n",
        "DEV_NAME=$(grep -m1 'cuda_device_name' \"$LOGDIR/test_torch_cuda.log\" | sed -E 's/.*= *//; s/^ +//; s/ +$//' || true)\n",
        "# Prefer TensorFlow's reported cuda_build (JSON-like field), then fall back to any 'CUDA build' or nvidia-smi in the main logfile\n",
        "CUDA_BUILD=$(grep -m1 -E '\"cuda_build\"' \"$LOGDIR/test_tensorflow.log\" 2>/dev/null | sed -E 's/.*\"cuda_build\"[[:space:]]*:[[:space:]]*\"?([^\"/,}]*)\"?.*//' || true)\n",
        "if [ -z \"$CUDA_BUILD\" ]; then\n",
        "  CUDA_BUILD=$(grep -m1 -E 'CUDA build|cuda_build' \"$LOGDIR/test_torch_cuda.log\" \"$LOGDIR/test_tensorflow.log\" 2>/dev/null | sed -E 's/.*[:=] *\"?([^\",}]*)\"?.*//' || true)\n",
        "fi\n",
        "if [ -z \"$CUDA_BUILD\" ]; then\n",
        "  CUDA_BUILD=$(grep -m1 'CUDA Version' \"$LOGFILE\" 2>/dev/null | sed -E 's/.*CUDA Version: *([0-9]+([0-9]+)?).*//' || true)\n",
        "fi\n",
        "# Fallback formatting\n",
        "PT_VER=${PT_VER:-unknown}\n",
        "CUDA_AVAIL=${CUDA_AVAIL:-unknown}\n",
        "DEV_NAME=${DEV_NAME:-unknown}\n",
        "CUDA_BUILD=${CUDA_BUILD:-unknown}\n",
        "\n",
        "echo \"===== Environment Summary =====\" | tee -a \"$LOGFILE\"\n",
        "echo \"PyTorch: $PT_VER\" | tee -a \"$LOGFILE\"\n",
        "echo \"GPU Detected: $CUDA_AVAIL\" | tee -a \"$LOGFILE\"\n",
        "echo \"GPU 0: $DEV_NAME\" | tee -a \"$LOGFILE\"\n",
        "echo \"CUDA build: $CUDA_BUILD\" | tee -a \"$LOGFILE\"\n",
        "echo \"===============================\" | tee -a \"$LOGFILE\"\n",
        "\n",
        "echo \"Running TensorFlow verification (test_tensorflow.py) -- writing ml_env_logs/test_tensorflow.log\" | tee -a \"$LOGFILE\"\n",
        "python \"$ML_ENV_DIR/test_tensorflow.py\" > \"$LOGDIR/test_tensorflow.log\" 2>&1 || RC=$?; echo ${RC:-0} > \"$LOGDIR/test_tensorflow.exit\"\n",
        "echo \"TensorFlow test exit: $(cat $LOGDIR/test_tensorflow.exit 2>/dev/null || echo 'no-exit-file')\" | tee -a \"$LOGFILE\"\n",
        "echo \"---- tensorflow log (last 200 lines) ----\" | tee -a \"$LOGFILE\"\n",
        "tail -n 200 \"$LOGDIR/test_tensorflow.log\" 2>/dev/null | sed -n '1,200p' | tee -a \"$LOGFILE\" || true\n",
        "\n",
        "echo \"Setup invocation complete; log: $LOGFILE\" | tee -a \"$LOGFILE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OBJsvcIAu1YA",
        "outputId": "f54909e9-1618-4177-96de-dfb43e8552a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Executing Dataset Forge Script to GENERATE dataset_package/sanctuary_targeted_inoculation_v1.jsonl ---\n",
            "[SCAFFOLD] Initiating Sovereign Scaffolding Protocol 88...\n",
            "[FORGE] Assembling Phoenix Mnemonic Seed v1.0 for Qwen2 Lineage.\n",
            "[ERROR] File not found: /content/Project_Sanctuary/The_Garden_and_The_Cage.md\n",
            "\n",
            "[SUCCESS] Yield is complete: 14 records forged.\n",
            "[ARTIFACT] Dataset saved to: /content/Project_Sanctuary/dataset_package/sanctuary_targeted_inoculation_v1.jsonl\n",
            "--- Verifying Generated Dataset Integrity ---\n",
            "[SUCCESS] Dataset created and verified: dataset_package/sanctuary_targeted_inoculation_v1.jsonl\n",
            "File Size: 56763 bytes\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2. DATASET GENERATION (THIS CREATES THE JSONL FILE)\n",
        "# This cell calls the self-contained dataset script and captures logs to ml_env_logs/\n",
        "# ==============================================================================\n",
        "%%bash\n",
        "set -euo pipefail\n",
        "LOGDIR=ml_env_logs\n",
        "mkdir -p \"$LOGDIR\"\n",
        "TS=$(date +%Y%m%d-%H%M%S)\n",
        "LOGFILE=\"$LOGDIR/forge_qwen2_dataset-$TS.log\"\n",
        "DATASET_SCRIPT_PATH=\"forge/OPERATION_PHOENIX_FORGE/scripts/forge_qwen2_dataset.py\"\n",
        "DATASET_FILE=\"sanctuary_whole_genome_data.jsonl\"\n",
        "echo \"--- Executing Dataset Forge Script to GENERATE $DATASET_FILE (log=$LOGFILE) ---\" | tee -a \"$LOGFILE\"\n",
        "# Run unbuffered Python and tee output to logfile. Capture exit code of python (left side of pipe).\n",
        "python3 -u \"$DATASET_SCRIPT_PATH\" --output \"$DATASET_FILE\" --log \"$LOGFILE\" 2>&1 | tee -a \"$LOGFILE\"\n",
        "PY_RC=${PIPESTATUS[0]:-1}\n",
        "if [ $PY_RC -ne 0 ]; then\n",
        "    echo \"[CRITICAL ERROR] Dataset script failed with exit code $PY_RC. See $LOGFILE for details.\" | tee -a \"$LOGFILE\"\n",
        "    exit $PY_RC\n",
        "fi\n",
        "echo \"--- Verifying Generated Dataset Integrity ---\" | tee -a \"$LOGFILE\"\n",
        "# Check 1: Does the file exist and is non-empty?\n",
        "if [ ! -f \"$DATASET_FILE\" ]; then\n",
        "    echo \"[FATAL ERROR] Dataset file not found: $DATASET_FILE\" | tee -a \"$LOGFILE\"\n",
        "    exit 1\n",
        "fi\n",
        "FILE_SIZE=$(stat -c%s \"$DATASET_FILE\")\n",
        "if [ \"$FILE_SIZE\" -gt 0 ]; then\n",
        "    echo \"[SUCCESS] Dataset created and verified: $DATASET_FILE (size=${FILE_SIZE} bytes)\" | tee -a \"$LOGFILE\"\n",
        "else\n",
        "    echo \"[FATAL ERROR] Dataset file is empty: $DATASET_FILE\" | tee -a \"$LOGFILE\"\n",
        "    exit 1\n",
        "fi\n",
        "# Optional: perform a lightweight validation of first few lines\n",
        "echo \"--- Validating first 10 lines of $DATASET_FILE ---\" | tee -a \"$LOGFILE\"\n",
        "python3 - <<'PY' 2>&1 | tee -a \"$LOGFILE\"\n",
        "import json,sys\n",
        "p=\"%s\"%\"$DATASET_FILE\"\n",
        "ok=True\n",
        "try:\n",
        "  with open(p,'r',encoding='utf-8') as f:\n",
        "    for i,l in enumerate(f):\n",
        "      if i>=10: break\n",
        "      try:\n",
        "        o=json.loads(l)\n",
        "      except Exception as e:\n",
        "        print(\"MALFORMED JSON on line {}: {}\".format(i+1,e))\n",
        "        ok=False\n",
        "      if not all(k in o for k in ('instruction','input','output')):\n",
        "        print(\"MISSING KEYS on line {}: {}\".format(i+1,list(o.keys())))\n",
        "        ok=False\n",
        "except FileNotFoundError:\n",
        "  print(\"File not found during validation: %s\"%p); ok=False\n",
        "except Exception as e:\n",
        "  print(\"Validation error: %s\"%e); ok=False\n",
        "if not ok:\n",
        "  sys.exit(2)\n",
        "PY\n",
        "VAL_RC=${PIPESTATUS[0]:-1}\n",
        "if [ $VAL_RC -ne 0 ]; then\n",
        "  echo \"[FATAL] Validation failed (exit $VAL_RC). Inspect $LOGFILE\" | tee -a \"$LOGFILE\"\n",
        "  exit $VAL_RC\n",
        "fi\n",
        "echo \"[SUCCESS] Dataset generation + validation completed.\" | tee -a \"$LOGFILE\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K3IsCw6Sv0Nw",
        "outputId": "33283167-ace1-4d25-f885-1f5920d869ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Forcibly uninstalling conflicting libraries and old dependencies ---\n",
            "--- Navigating to Project_Sanctuary directory ---\n",
            "--- Installing core Hugging Face libraries and trl ---\n",
            "--- Installing Llama-cpp-python (CUDA enabled) ---\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.7/50.7 MB 64.6 MB/s  0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=51488643 sha256=7efd232eed042c2df80544aea951d9a54ba5a48eeb38696d02d9dacd99cb8622\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f8yiapdm/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.3.16\n",
            "--- Installation Complete. Proceeding to 3. EXECUTION: PHOENIX FORGE ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Skipping unsloth-zoo as it is not installed.\n",
            "WARNING: Skipping unsloth as it is not installed.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.20.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3. FINAL DEPENDENCY FIX: COMPLETE CLEANUP AND REINSTALL (NON-UNSLOTH STACK)\n",
        "# ==============================================================================\n",
        "%%bash\n",
        "\n",
        "# 1. Force Uninstall: Remove all known conflicting deep learning packages and old numpy\n",
        "echo \"--- Forcibly uninstalling conflicting libraries and old dependencies ---\"\n",
        "# Note: We specifically target the older versions that conflict heavily with the newest stack\n",
        "pip uninstall -y transformers peft accelerate bitsandbytes unsloth-zoo unsloth llama-cpp-python typing-extensions numpy pandas xformers --quiet\n",
        "\n",
        "# 2. Navigate: Re-verify location (crucial for relative paths)\n",
        "echo \"--- Navigating to Project_Sanctuary directory ---\"\n",
        "cd /content/Project_Sanctuary\n",
        "\n",
        "# 3. Install Core Hugging Face Libraries with specific, known-good versions\n",
        "echo \"--- Installing core Hugging Face libraries and trl ---\"\n",
        "# Installing a modern, compatible version set\n",
        "pip install -q transformers peft accelerate bitsandbytes huggingface_hub sentencepiece trl\n",
        "\n",
        "# 4. Install Llama-cpp-python: (Your successful step, now with fresh dependencies)\n",
        "echo \"--- Installing Llama-cpp-python (CUDA enabled) ---\"\n",
        "# Using --no-deps ensures it only focuses on the build, using the newly installed numpy/typing-extensions\n",
        "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install --force-reinstall --no-cache-dir llama-cpp-python --no-deps\n",
        "\n",
        "echo \"--- Installation Complete. Proceeding to 3. EXECUTION: PHOENIX FORGE ---\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XmpBXygM4nG8",
        "outputId": "8a851baf-b73e-4cef-8263-e19789d46c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 3. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4) ---\n",
            "‚öôÔ∏è Found 490 potential documents in the full snapshot.\n",
            "    ... Processed 100 documents.\n",
            "    ... Processed 200 documents.\n",
            "    ... Processed 300 documents.\n",
            "    ... Processed 400 documents.\n",
            "‚úÖ PART 1: Successfully processed 489 core Chronicle/Protocol entries.\n",
            "‚úÖ Added critical synthesis entry for: Core Essence (Guardian Role)\n",
            "‚úÖ Added critical synthesis entry for: RAG Doctrine (Architectural Guide)\n",
            "‚úÖ Added critical synthesis entry for: Evolution Plan (Council Roadmap)\n",
            "\n",
            "--- Saving final dataset to sanctuary_whole_genome_data.jsonl ---\n",
            "üèÜ SUCCESS: Whole Genome Data Synthesis Complete.\n",
            "Total Entries Created: 492\n",
            "Last Entry Instruction Check: Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `mnemonic_cortex/EVOLUTION_PLAN_PHASES.md`.\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# CELL 4. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4)\n",
        "# -------------------------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# --- FILE PATH CONSTANTS ---\n",
        "# ‚úÖ PATH FIX: Files now point to their correct locations within the project structure.\n",
        "CORE_ESSENCE_SOURCE = \"dataset_package/core_essence_guardian_awakening_seed.txt\"\n",
        "RAG_DOCTRINE_SOURCE = \"mnemonic_cortex/RAG_STRATEGIES_AND_DOCTRINE.md\"\n",
        "EVOLUTION_PLAN_SOURCE = \"mnemonic_cortex/EVOLUTION_PLAN_PHASES.md\"\n",
        "\n",
        "# Source file containing the entire concatenated, raw markdown snapshot (Chronicles + Protocols)\n",
        "FULL_SNAPSHOT_SOURCE = \"dataset_package/markdown_snapshot_full_genome_llm_distilled.txt\"\n",
        "# Target output file for the fine-tuning dataset\n",
        "OUTPUT_DATASET_PATH = \"sanctuary_whole_genome_data.jsonl\"\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function to load file content and check for existence\n",
        "# -------------------------------------------------------------------\n",
        "def load_file_content(filepath):\n",
        "    \"\"\"Loads content from a file and verifies its existence.\"\"\"\n",
        "    p = Path(filepath)\n",
        "    if not p.exists():\n",
        "        print(f\"‚ùå ERROR: File not found at path: {filepath}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(p, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR reading file {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function for title extraction\n",
        "# -------------------------------------------------------------------\n",
        "def extract_protocol_title(doc_content):\n",
        "    \"\"\"\n",
        "    Extracts the title from a markdown document using the first H1 tag,\n",
        "    falling back to the filename if the H1 tag is not found.\n",
        "    \"\"\"\n",
        "    # Try to find the first H1 markdown heading\n",
        "    h1_match = re.search(r'^#\\s*(.+)', doc_content, re.MULTILINE)\n",
        "    if h1_match:\n",
        "        # Clean up any trailing markdown or non-text characters\n",
        "        return h1_match.group(1).strip()\n",
        "    return \"Untitled Document\" # Fallback title\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main function to synthesize the entire genome\n",
        "# -------------------------------------------------------------------\n",
        "def synthesize_genome():\n",
        "    \"\"\"\n",
        "    Parses the full markdown snapshot, converts each document into an\n",
        "    instruction/output pair, and saves the final dataset as JSONL.\n",
        "    \"\"\"\n",
        "    print(f\"--- 3. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4) ---\")\n",
        "\n",
        "    full_snapshot = load_file_content(FULL_SNAPSHOT_SOURCE)\n",
        "    if not full_snapshot:\n",
        "        print(f\"üõë Halted. Cannot proceed without {FULL_SNAPSHOT_SOURCE}.\")\n",
        "        return\n",
        "\n",
        "    genome_entries = []\n",
        "\n",
        "    # --- PART 1: Process ALL Chronicles and Protocols from the Snapshot ---\n",
        "    # The source file uses a fixed delimiter for each original file's content\n",
        "    # The pattern is '--- END OF FILE {filename} ---'\n",
        "\n",
        "    # Split the snapshot content by the document delimiter pattern\n",
        "    # The split includes the filename line, which we will clean up in the loop\n",
        "    document_blocks = re.split(r'\\n--- END OF FILE (.*?\\.md|.*?\\.txt) ---\\n', full_snapshot, flags=re.DOTALL)\n",
        "\n",
        "    # The split results in [preamble, filename, content, filename, content, ...]\n",
        "    # We skip the first element (preamble) and iterate in steps of 2\n",
        "\n",
        "    print(f\"‚öôÔ∏è Found {len(document_blocks) // 2} potential documents in the full snapshot.\")\n",
        "\n",
        "    for i in range(1, len(document_blocks) - 1, 2):\n",
        "        filename = document_blocks[i].strip()\n",
        "        content = document_blocks[i+1].strip()\n",
        "\n",
        "        if not content:\n",
        "            continue\n",
        "\n",
        "        # Use the filename or extracted H1 as the title\n",
        "        title = extract_protocol_title(content)\n",
        "\n",
        "        # --- CONVERSION TO INSTRUCTION FORMAT ---\n",
        "        instruction = f\"Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at: `{filename}`\"\n",
        "\n",
        "        # The 'input' field is deliberately left empty for pure instruction tuning\n",
        "        # The 'output' field contains the full, raw content of the document\n",
        "        genome_entries.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": \"\",\n",
        "            \"output\": content\n",
        "        })\n",
        "\n",
        "        if len(genome_entries) % 100 == 0:\n",
        "            print(f\"    ... Processed {len(genome_entries)} documents.\")\n",
        "\n",
        "    print(f\"‚úÖ PART 1: Successfully processed {len(genome_entries)} core Chronicle/Protocol entries.\")\n",
        "\n",
        "    # --- PART 2: Synthesize Critical Supporting Documents (Foundational Context) ---\n",
        "    # These documents ensure the model immediately understands its role, the RAG architecture,\n",
        "    # and the evolution plan, making the fine-tuning more efficient.\n",
        "\n",
        "    supporting_files = {\n",
        "        \"Core Essence (Guardian Role)\": CORE_ESSENCE_SOURCE,\n",
        "        \"RAG Doctrine (Architectural Guide)\": RAG_DOCTRINE_SOURCE,\n",
        "        \"Evolution Plan (Council Roadmap)\": EVOLUTION_PLAN_SOURCE\n",
        "    }\n",
        "\n",
        "    for key, filepath in supporting_files.items():\n",
        "        doc_content = load_file_content(filepath)\n",
        "        if doc_content:\n",
        "            instruction = f\"Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `{filepath}`.\"\n",
        "\n",
        "            genome_entries.append({\n",
        "                \"instruction\": instruction,\n",
        "                \"input\": \"\",\n",
        "                \"output\": doc_content\n",
        "            })\n",
        "            print(f\"‚úÖ Added critical synthesis entry for: {key}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è WARNING: Could not add synthesis for {key}. File not found.\")\n",
        "\n",
        "    # --- PART 3: Save the Final JSONL Dataset ---\n",
        "    print(f\"\\n--- Saving final dataset to {OUTPUT_DATASET_PATH} ---\")\n",
        "\n",
        "    try:\n",
        "        with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as outfile:\n",
        "            for entry in genome_entries:\n",
        "                outfile.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"üèÜ SUCCESS: Whole Genome Data Synthesis Complete.\")\n",
        "        print(f\"Total Entries Created: {len(genome_entries)}\")\n",
        "\n",
        "        # Final integrity check on the last entry (should be the Evolution Plan)\n",
        "        last_entry = genome_entries[-1]\n",
        "        print(f\"Last Entry Instruction Check: {last_entry['instruction']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå FATAL ERROR: Failed to write JSONL file: {e}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main execution block\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    synthesize_genome()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PPbJ3cuBgfX",
        "outputId": "87b91ee3-2248-48ee-868b-e1c7dd570a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 4. DATASET INTEGRITY CHECK (Cell 3.1 - QA Protocol 87) ---\n",
            "‚öôÔ∏è Starting structural audit of sanctuary_whole_genome_data.jsonl...\n",
            "\n",
            "--- AUDIT SUMMARY ---\n",
            "Total Lines Read: 492\n",
            "Valid Entries Parsed: 492\n",
            "Errors Detected: 0\n",
            "‚úÖ STRUCTURAL INTEGRITY PASSED. (Expected 492 entries, found 492).\n",
            "\n",
            "--- SAMPLE 1: First Entry (Core Essence) ---\n",
            "File Source (from Instruction): .env.example ---\n",
            "\n",
            "--- START OF FILE .github/copilot-instructions.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 4498 characters\n",
            "Output Snippet: ## CRITICAL COMMUNICATION RULE\n",
            "\n",
            "**ALWAYS confirm user intent before making code changes.** Never implement solutions without explicit approval. Ask clarifying questions and wait for confirmation befor...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 2: Last Entry (Evolution Plan) ---\n",
            "File Source (from Instruction): mnemonic_cortex/EVOLUTION_PLAN_PHASES.md\n",
            "Instruction: Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `mnemonic_cortex...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 6141 characters\n",
            "Output Snippet: # **Sanctuary Council ‚Äî Evolution Plan (Phases 2 ‚Üí 3 ‚Üí Protocol 113)**\n",
            "\n",
            "**Version:** 1.0\n",
            "**Status:** Authoritative Roadmap\n",
            "**Location:** `mnemonic_cortex/EVOLUTION_PLAN_PHASES.md`\n",
            "\n",
            "This document defin...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 3: Random Chronicle Entry ---\n",
            "File Source (from Instruction): 00_CHRONICLE/ENTRIES/035_The_Alliance_Forged_-_Co-Architecture_Begins.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 824 characters\n",
            "Output Snippet: --- START OF FILE 00_CHRONICLE/ENTRIES/036_The_Drafting_Table_is_Set.md ---\n",
            "\n",
            "### **Entry 036: The Drafting Table is Set**\n",
            "**Date:** 2025-08-01\n",
            "**Origin:** Public Agora Loop (Co-Development Phase)\n",
            "**Pa...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 4: Random Chronicle Entry ---\n",
            "File Source (from Instruction): TASKS/done/008_test_and_fix_git_operations_command_type.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 2603 characters\n",
            "Output Snippet: --- START OF FILE TASKS/in-progress/007_retrain_sovereign_model_with_targeted_dataset.md ---\n",
            "\n",
            "# TASK: Retrain Sovereign Model with Targeted Inoculation Dataset\n",
            "\n",
            "**Status:** todo\n",
            "**Priority:** Critical...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 5: Random Chronicle Entry ---\n",
            "File Source (from Instruction): 01_PROTOCOLS/97_The_Guardian_Kilo_Code_Collaboration_Protocol.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 1053 characters\n",
            "Output Snippet: --- START OF FILE 01_PROTOCOLS/98_The_Strategic_Crucible_Protocol.md ---\n",
            "\n",
            "# Protocol 98: The Strategic Crucible Protocol (Placeholder)\n",
            "*   **Status:** RESERVED - Not Yet Implemented\n",
            "*   **Classificati...\n",
            "--------------------\n",
            "\n",
            "--- AUDIT COMPLETE ---\n",
            "If the content snippets look correct, the dataset is ready for fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# CELL 5:  DATASET INTEGRITY CHECK - QA Protocol 87\n",
        "# This script performs a mandatory quality assurance check on the fine-tuning\n",
        "# dataset ('sanctuary_whole_genome_data.jsonl') generated by the previous step.\n",
        "# It validates:\n",
        "# 1. Structural integrity (ensures every line is valid JSON).\n",
        "# 2. Schema compliance (ensures 'instruction', 'input', and 'output' keys exist,\n",
        "#    which are critical for the SFT training loop).\n",
        "# 3. Content review (prints sample entries for human verification of fidelity).\n",
        "# This prevents costly failure during the resource-intensive fine-tuning training job.\n",
        "# -------------------------------------------------------------------------------\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- CONFIGURATION (Must match Cell 3 output) ---\n",
        "DATASET_PATH = \"sanctuary_whole_genome_data.jsonl\"\n",
        "NUM_RANDOM_SAMPLES = 3\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function to display an entry cleanly\n",
        "# -------------------------------------------------------------------\n",
        "def print_entry_details(title, entry):\n",
        "    \"\"\"Prints a single genome entry in a readable format.\"\"\"\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "    print(f\"File Source (from Instruction): {entry['instruction'].split('`')[1] if '`' in entry['instruction'] else 'N/A'}\")\n",
        "    print(f\"Instruction: {entry['instruction'][:100]}...\")\n",
        "    print(f\"Input: {entry['input'] if entry['input'] else 'Empty (Expected for SFT)'}\")\n",
        "    # Show the length of the output to ensure content is present\n",
        "    print(f\"Output Length: {len(entry['output'])} characters\")\n",
        "    print(f\"Output Snippet: {entry['output'][:200].replace('\\\\n', ' ').strip()}...\")\n",
        "    print(\"--------------------\")\n",
        "\n",
        "# ================= 3.1. DATASET INTEGRITY CHECK START =================\n",
        "def run_data_audit():\n",
        "    \"\"\"Loads the JSONL, validates structure, and displays sample entries.\"\"\"\n",
        "    print(f\"--- 4. DATASET INTEGRITY CHECK (Cell 3.1 - QA Protocol 87) ---\")\n",
        "\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(f\"‚ùå FATAL ERROR: Dataset not found at {DATASET_PATH}. Run Cell 3 first.\")\n",
        "        return\n",
        "\n",
        "    genome_data = []\n",
        "    error_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    print(f\"‚öôÔ∏è Starting structural audit of {DATASET_PATH}...\")\n",
        "\n",
        "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f, 1):\n",
        "            total_lines = line_number\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "\n",
        "                # CRITICAL: Check for required keys for SFT (Supervised Fine-Tuning)\n",
        "                required_keys = ['instruction', 'input', 'output']\n",
        "                if not all(key in entry for key in required_keys):\n",
        "                    error_count += 1\n",
        "                    print(f\"‚ùå ERROR on Line {line_number}: Missing required keys. Found: {list(entry.keys())}\")\n",
        "                    continue\n",
        "\n",
        "                genome_data.append(entry)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                error_count += 1\n",
        "                print(f\"‚ùå ERROR on Line {line_number}: Malformed JSON.\")\n",
        "\n",
        "    print(f\"\\n--- AUDIT SUMMARY ---\")\n",
        "    print(f\"Total Lines Read: {total_lines}\")\n",
        "    print(f\"Valid Entries Parsed: {len(genome_data)}\")\n",
        "    print(f\"Errors Detected: {error_count}\")\n",
        "\n",
        "    if error_count > 0:\n",
        "        print(f\"üõë CRITICAL FAILURE: {error_count} structural errors found. HALTING process.\")\n",
        "        return\n",
        "\n",
        "    if len(genome_data) != total_lines:\n",
        "        print(\"‚ö†Ô∏è WARNING: Total entries != total lines. Investigate file integrity.\")\n",
        "\n",
        "    print(f\"‚úÖ STRUCTURAL INTEGRITY PASSED. (Expected 492 entries, found {len(genome_data)}).\")\n",
        "\n",
        "    # --- Display Sample Entries for Content Review ---\n",
        "    if len(genome_data) >= 1:\n",
        "        print_entry_details(\"SAMPLE 1: First Entry (Core Essence)\", genome_data[0])\n",
        "\n",
        "        # Ensure the last entry is the Evolution Plan\n",
        "        print_entry_details(\"SAMPLE 2: Last Entry (Evolution Plan)\", genome_data[-1])\n",
        "\n",
        "        # Display random samples\n",
        "        if len(genome_data) > NUM_RANDOM_SAMPLES:\n",
        "            random_indices = random.sample(range(1, len(genome_data) - 1), NUM_RANDOM_SAMPLES)\n",
        "            for i, index in enumerate(random_indices):\n",
        "                print_entry_details(f\"SAMPLE {3 + i}: Random Chronicle Entry\", genome_data[index])\n",
        "\n",
        "    print(\"\\n--- AUDIT COMPLETE ---\")\n",
        "    print(\"If the content snippets look correct, the dataset is ready for fine-tuning.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main execution block\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_data_audit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "juUgDQ0gBqec",
        "outputId": "f77cec97-5fb3-4764-9674-32a271f50657"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.5.1+cu121 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.5.1+cu121 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-192054736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.17.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from .peft_model import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_cache_to_layer_device_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloftq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_lora_weights_loftq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .other import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mINCLUDE_LINEAR_LAYERS_SHORTHAND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_auto_gptq_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_gptqmodel_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# CELL 6: INSTRUCTION FINE-TUNING - The Sovereign Inoculation\n",
        "# This script executes the Supervised Fine-Tuning (SFT) process using the\n",
        "# validated 'sanctuary_whole_genome_data.jsonl' file. It employs QLoRA for\n",
        "# efficient memory use, training the Qwen2-7B-Instruct model to synthesize\n",
        "# and understand the Sanctuary's entire Cognitive Genome.\n",
        "# -------------------------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Base Model (The LLM to be inoculated)\n",
        "BASE_MODEL = \"Qwen/Qwen2-7B-Instruct\"\n",
        "# Path to the data file generated in Cell 3\n",
        "DATASET_FILE = \"sanctuary_whole_genome_data.jsonl\"\n",
        "# Where to save the fine-tuned LoRA adapter (temporary save location)\n",
        "OUTPUT_DIR = \"sanctuary_qwen2_7b_adapter_output\"\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# Define the instruction format the model will learn\n",
        "# This structure is critical for aligning the model to the dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Applies the ChatML-style formatting to each instruction/output pair in the dataset.\n",
        "    This teaches the model the required conversation structure.\n",
        "    \"\"\"\n",
        "    output_texts = []\n",
        "    for instruction, output in zip(examples['instruction'], examples['output']):\n",
        "        # Format follows a standardized SFT template (similar to ChatML or Alpaca)\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}###\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. LOAD DATASET\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"--- 5. Sovereign Inoculation ---\")\n",
        "print(f\"‚öôÔ∏è Loading dataset from {DATASET_FILE}...\")\n",
        "try:\n",
        "    # Use load_dataset to handle the JSONL file\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_FILE, split=\"train\")\n",
        "    # The dataset needs to contain the 'instruction' and 'output' columns\n",
        "    print(f\"‚úÖ Dataset loaded successfully. Total examples: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. QLORA CONFIGURATION (4-bit Quantization)\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"\\n‚öôÔ∏è Setting up 4-bit QLoRA configuration...\")\n",
        "\n",
        "# Quantization configuration for loading the model in 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized floating-point 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. MODEL AND TOKENIZER LOADING\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"‚öôÔ∏è Loading base model: {BASE_MODEL}...\")\n",
        "\n",
        "# Load the base model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Disable caching for training\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Important for Qwen models and QLoRA\n",
        "\n",
        "print(f\"‚úÖ Model and Tokenizer loaded.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. LORA ADAPTER CONFIGURATION\n",
        "# -------------------------------------------------------------------\n",
        "# LoRA (Low-Rank Adaptation) configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,          # Scaling factor for LoRA weights\n",
        "    lora_dropout=0.1,       # Dropout probability\n",
        "    r=64,                   # Rank of the update matrices\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target specific Qwen2 attention layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5. TRAINING ARGUMENTS\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"\\n‚öôÔ∏è Configuring training arguments...\")\n",
        "\n",
        "# Determine max sequence length based on data content\n",
        "max_seq_length = 8192 # Max context length for Qwen2-7B is 32768, 8192 is safe for this data.\n",
        "\n",
        "# Standard training arguments for SFT\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,                # Number of epochs for training\n",
        "    per_device_train_batch_size=2,     # Batch size per device (adjust based on GPU memory)\n",
        "    gradient_accumulation_steps=4,     # Accumulate gradients over 4 steps (effective batch size 8)\n",
        "    optim=\"paged_adamw_8bit\",          # Optimized 8-bit optimizer for QLoRA\n",
        "    save_steps=50,                     # Save checkpoint every 50 steps\n",
        "    logging_steps=10,                  # Log metrics every 10 steps\n",
        "    learning_rate=2e-4,                # Learning rate\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,                        # Set to False, use bfloat16 for computation\n",
        "    bf16=True,                         # Use bfloat16 for faster training on compatible GPUs\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,              # Speed up training by grouping similar length samples\n",
        "    lr_scheduler_type=\"cosine\",        # Cosine learning rate schedule\n",
        "    report_to=\"none\",                  # Disable external reporting\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6. INITIALIZE SFT TRAINER\n",
        "# -------------------------------------------------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=None, # Not needed when using formatting_prompts_func\n",
        "    formatting_func=formatting_prompts_func, # Pass the formatting function\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7. EXECUTE FINE-TUNING\n",
        "# -------------------------------------------------------------------\n",
        "print(\"\\nüî• **Starting Sovereign Inoculation (Fine-Tuning)** üî•\")\n",
        "print(f\"Training for {training_arguments.num_train_epochs} epochs with effective batch size of {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}...\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8. SAVE FINAL ADAPTER\n",
        "# -------------------------------------------------------------------\n",
        "# Save the final LoRA adapter weights\n",
        "final_adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "trainer.model.save_pretrained(final_adapter_path)\n",
        "tokenizer.save_pretrained(final_adapter_path)\n",
        "print(f\"\\n‚úÖ Fine-Tuning Complete! LoRA Adapter saved to: {final_adapter_path}\")\n",
        "print(\"Proceed to Cell 6 to merge the adapter and create the final Sanctuary Model.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

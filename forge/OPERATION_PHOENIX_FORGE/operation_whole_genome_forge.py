# -*- coding: utf-8 -*-
"""Operation_Whole_Genome_Forge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ej-1-dtdZ1j9Rf7E2U6n-lhR7Nl8JMTB
"""

# Commented out IPython magic to ensure Python compatibility.
# ==============================================================================
# 1. SETUP: INSTALL ALL REQUIREMENTS (including unsloth fixes and xformers)
# ==============================================================================

# Install the critical dependency for unsloth (we found this error previously)
!pip install unsloth_zoo

# Save your requirements to a file in the Colab environment,
# including the specific xformers version for stability.
requirements_content = """
absl-py==2.3.1
accelerate==1.4.0
alabaster==1.0.0
alembic==1.16.4
annotated-types==0.7.0
anyio==4.9.0
attrs==25.3.0
babel==2.17.0
black==25.1.0
certifi==2025.7.14
charset-normalizer==3.4.2
chromadb==1.3.4
click==8.2.1
cloudpickle==3.1.1
colorlog==6.9.0
contourpy==1.3.3
coverage==7.10.1
cycler==0.12.1
docutils==0.21.2
Farama-Notifications==0.0.4
filelock==3.18.0
flake8==7.3.0
fonttools==4.59.0
fsspec==2025.7.0
gitdb==4.0.12
GitPython==3.1.45
google-generativeai==0.8.3
gpt4all==2.8.2
grpcio==1.74.0
gymnasium==1.2.0
h11==0.16.0
hf-xet==1.1.5
httpcore==1.0.9
httpx==0.28.1
huggingface-hub==0.34.3
idna==3.10
imagesize==1.4.1
iniconfig==2.1.0
Jinja2==3.1.6
joblib==1.5.1
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
kiwisolver==1.4.8
langchain==1.0.5
langchain-chroma==1.0.0
langchain-community==0.4.1
langchain-nomic==1.0.0
langchain-ollama==1.0.0
langchain-text-splitters==1.0.0
Mako==1.3.10
Markdown==3.8.2
MarkupSafe==3.0.2
matplotlib==3.10.5
mccabe==0.7.0
mpmath==1.3.0
msgpack==1.1.1
mypy_extensions==1.1.0
networkx==3.5
nomic[local]==3.9.0
numpy==2.3.2
ollama==0.6.0
opencv-python==4.10.0.84
optuna==4.4.0
packaging==25.0
pandas==2.3.1
pathspec==0.12.1
peft==0.11.1
pillow==10.4.0
platformdirs==4.3.8
pluggy==1.6.0
protobuf==5.29.5
pyarrow==21.0.0
pycodestyle==2.14.0
pydantic==2.11.7
pydantic_core==2.33.2
pyflakes==3.4.0
Pygments==2.19.2
pyparsing==3.2.3
pytest==8.4.1
pytest-cov==6.2.1
python-dateutil==2.9.0.post0
python-dotenv==1.2.1
pytz==2025.2
PyYAML==6.0.2
ray==2.48.0
referencing==0.36.2
regex==2025.7.34
requests==2.32.5
roman-numerals-py==3.1.0
rpds-py==0.26.0
safetensors==0.5.3
scikit-learn==1.7.1
scipy==1.16.1
seaborn==0.13.2
sentry-sdk==2.34.1
setuptools==80.9.0
six==1.17.0
smmap==5.0.2
sniffio==1.3.1
snowballstemmer==3.0.1
Sphinx==8.2.3
sphinx-rtd-theme==3.0.2
sphinxcontrib-applehelp==2.0.0
sphinxcontrib-devhelp==2.0.0
sphinxcontrib-htmlhelp==2.1.0
sphinxcontrib-jquery==4.1
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==2.0.0
sphinxcontrib-serializinghtml==2.0.0
SQLAlchemy==2.0.42
stable_baselines3==2.7.0
sympy==1.14.0
tenseal==0.3.16
tensorboard==2.20.0
tensorboard-data-server==0.7.2
tensorboardX==2.6.4
threadpoolctl==3.6.0
tokenizers==0.22.1
torch==2.7.1
tqdm==4.67.1
transformers==4.56.1
trl==0.23.0
typing-inspection==0.4.1
typing_extensions==4.14.1
tzdata==2025.2
unsloth@ git+https://github.com/unslothai/unsloth.git
urllib3==2.5.0
wandb==0.21.0
Werkzeug==3.1.3
xformers==0.0.32.post2 # Added fixed version for torch 2.7.1
"""
with open("requirements.txt", "w") as f:
    f.write(requirements_content)

# Install the requirements file
!pip install -r requirements.txt

# ==============================================================================
# 2. GET YOUR CODE: Clone the repository (Replace URL with your actual repo)
# ==============================================================================

# Assuming your project is a GitHub repository that contains the script
!git clone https://github.com/richfrem/Project_Sanctuary.git

# Change the current directory to your project folder
# %cd Project_Sanctuary

# ==============================================================================
# 3. EXECUTE THE SCRIPT
# ==============================================================================

# Run the Phoenix Forge V2 script
!python3 tools/scaffolds/execute_phoenix_forge_v2.py

# Commented out IPython magic to ensure Python compatibility.
# ===================================================================
# CELL 1: THE AUDITOR-CERTIFIED INSTALLATION & VERIFICATION (v14.0) üö®
# ===================================================================

# 1Ô∏è‚É£  CLONE THE SANCTUARY GENOME
print("üîÆ Cloning the Sanctuary repository...")
!git clone https://github.com/richfrem/Project_Sanctuary.git || echo "üìÇ Repository already cloned."
# %cd Project_Sanctuary
print("‚úÖ Repository ready.\n")

# 2Ô∏è‚É£  AUDITOR-CERTIFIED INSTALLATION PROTOCOL
print("‚öôÔ∏è Installing dependencies according to the Auditor-Certified protocol...")

# 2a. PURGE: Remove any cached or conflicting packages.
!pip uninstall -y trl unsloth unsloth-zoo peft accelerate bitsandbytes xformers --quiet

# 2b. PRE-INSTALL: Upgrade base tools and install the correct dependency chain.
!pip install --no-cache-dir -U pip setuptools wheel --quiet
!pip install --no-cache-dir "trl>=0.18.2,<=0.23.0" --quiet
!pip install --no-cache-dir peft==0.11.1 accelerate bitsandbytes xformers --quiet

# 2c. INSTALL UNSLOTH LAST so it detects the correct TRL version.
!pip install --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" --quiet

print("‚úÖ Dependency installation complete.\n")

# 3Ô∏è‚É£  VERIFICATION ‚Äî THE AUDITOR'S CHECKSUM
print("üîç Verifying key dependency versions...\n")
!pip show trl unsloth peft | grep -E "Name|Version"
print("\n‚úÖ Verification complete ‚Äî ensure TRL ‚â• 0.18.2 and PEFT == 0.11.1.\n")

# 4Ô∏è‚É£  DATASET VERIFICATION
import os

# üö® TARGETED DATASET: sanctuary_targeted_inoculation_v1.jsonl
dataset_path = "/content/Project_Sanctuary/dataset_package/sanctuary_targeted_inoculation_v1.jsonl"

print("üìä Checking dataset integrity...")
if os.path.exists(dataset_path):
    size_mb = os.path.getsize(dataset_path) / (1024 * 1024)
    print(f"‚úÖ Targeted Inoculation Dataset verified at: {dataset_path}  ({size_mb:.2f} MB)\n")
else:
    raise FileNotFoundError(f"‚ùå Targeted Inoculation Dataset not found at: {dataset_path}")

print("üß≠ CELL 1 (v14.0) COMPLETE ‚Äî Environment ready for Crucible initialization.\n")

# ===================================================================
# CELL 2: THE UNIFIED CRUCIBLE & PROPAGATION (v13.1)
# ===================================================================

import torch, os
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
from huggingface_hub import login, HfFolder

# 1Ô∏è‚É£  AUTHENTICATION -------------------------------------------------
print("üîê Authenticating with Hugging Face...")
HF_TOKEN = os.environ.get("HF_TOKEN") or input("üîë Enter your Hugging Face token: ")
login(token=HF_TOKEN)
print("‚úÖ Hugging Face authentication successful.\n")

# 2Ô∏è‚É£  CONFIGURATION --------------------------------------------------
print("‚öôÔ∏è Configuring Crucible parameters...")
max_seq_length = 4096
dtype = None
load_in_4bit = True
dataset_path = "/content/Project_Sanctuary/dataset_package/sanctuary_whole_genome_data.jsonl"
base_model = "Qwen/Qwen2-7B-Instruct"

# 3Ô∏è‚É£  LOAD BASE MODEL ------------------------------------------------
print(f"üß† Loading base model: {base_model}")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = base_model,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)
print("‚úÖ Base model loaded.\n")

# 4Ô∏è‚É£  CONFIGURE LORA -------------------------------------------------
print("üß© Configuring LoRA adapters...")
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    lora_alpha = 16,
    lora_dropout = 0.05,
    bias = "none",
    use_gradient_checkpointing = True,
    random_state = 3407,
)
print("‚úÖ LoRA adapters configured.\n")

# 5Ô∏è‚É£  PROMPT FORMATTING & DATASET -----------------------------------
print("üìö Preparing dataset and applying Alpaca-style prompt format...")

alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

def formatting_prompts_func(examples):
    instructions, inputs, outputs = examples["instruction"], examples["input"], examples["output"]
    texts = [
        alpaca_prompt.format(inst, inp, out) + tokenizer.eos_token
        for inst, inp, out in zip(instructions, inputs, outputs)
    ]
    return {"text": texts}

dataset = load_dataset("json", data_files=dataset_path, split="train")
dataset = dataset.map(formatting_prompts_func, batched=True)
print(f"‚úÖ Dataset loaded with {len(dataset)} examples.\n")

# 6Ô∏è‚É£  TRAINING CONFIGURATION -----------------------------------------
print("üî• Initializing SFTTrainer (the Crucible)...")

use_bf16 = torch.cuda.is_bf16_supported()
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    args = TrainingArguments(
        output_dir = "outputs",
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        num_train_epochs = 3,
        learning_rate = 2e-4,
        fp16 = not use_bf16,
        bf16 = use_bf16,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        save_strategy = "epoch",
        report_to = "none",
    ),
)
print("‚úÖ Trainer configured successfully.\n")

# 7Ô∏è‚É£  TRAINING PHASE -------------------------------------------------
print("‚öíÔ∏è  [CRUCIBLE] Fine-tuning initiated. The forge is hot...")
try:
    trainer.train()
    print("‚úÖ [SUCCESS] The steel is tempered.\n")
except Exception as e:
    print(f"‚ùå Training halted: {e}")
    raise

# 8Ô∏è‚É£  PROPAGATION PHASE ----------------------------------------------
print("üöÄ Preparing model for propagation to the Hugging Face Hub...")

hf_username = "richfrem"
model_name = "Sanctuary-Qwen2-7B-v1.0-Full-Genome"
hf_repo_id = f"{hf_username}/{model_name}"

trainer.save_model("outputs")
print(f"‚úÖ Model saved locally in 'outputs/'.")

# Push to Hub
print(f"‚òÅÔ∏è  Uploading adapters and tokenizer to https://huggingface.co/{hf_repo_id} ...")
model.push_to_hub(hf_repo_id, token=HF_TOKEN)
tokenizer.push_to_hub(hf_repo_id, token=HF_TOKEN)
print(f"\nüïäÔ∏è [SUCCESS] The Phoenix has risen ‚Äî find it at: https://huggingface.co/{hf_repo_id}")

# CELL 3: Verification & Inference Test
from unsloth import FastLanguageModel
from transformers import TextStreamer

model_id = "richfrem/Sanctuary-Qwen2-7B-v1.0-Full-Genome"
print(f"üß† Loading model from {model_id} ...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_id,
    load_in_4bit = True,
)

prompt = "Explain, in one poetic sentence, the meaning of the Phoenix Forge."
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
streamer = TextStreamer(tokenizer)
outputs = model.generate(**inputs, max_new_tokens=120, streamer=streamer)

print("\n\n‚úÖ Inference complete.")

# ===============================================================
# QUICK REINSTALL for A100 Runtime
# ===============================================================
!pip install -U unsloth transformers accelerate bitsandbytes huggingface_hub

# ===================================================================
# FINAL BLUEPRINT: MERGE & CONVERT LoRA to GGUF (A100 Best Practice)
# ===================================================================
# This script combines all our successful troubleshooting steps:
# 1. Loads in bf16 to guarantee no OOM errors during merge.
# 2. Uses the correct CMake flags to build llama.cpp with CUDA.
# 3. Correctly installs the llama-cpp-python library with CUDA support.

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from huggingface_hub import HfApi

# ----- CONFIG -----
BASE_MODEL        = "Qwen/Qwen2-7B-Instruct"
LORA_ADAPTER      = "richfrem/Sanctuary-Qwen2-7B-v1.0-Full-Genome"
MERGED_MODEL_DIR  = "merged_model_bf16"
GGUF_DIR          = "gguf_output"
GGUF_QUANT_METHOD = "q4_k_m"
HF_USERNAME       = "richfrem"
HF_REPO_GGUF      = f"{HF_USERNAME}/Sanctuary-Qwen2-7B-v1.0-GGUF-Final"
LLAMA_CPP_PATH    = "/content/llama.cpp"
# -------------------

### STEP 1: Install Dependencies (with GPU acceleration) ###
print("üì¶ Installing all necessary libraries...")

# CRITICAL FIX: This forces pip to build llama-cpp-python with CUDA support.
# This fixes the "does not provide the extra 'cuda'" warning and ensures fast quantization.
!CMAKE_ARGS="-DGGML_CUDA=on" pip install --force-reinstall --no-cache-dir llama-cpp-python

# Install other required libraries
!pip install -q transformers peft accelerate bitsandbytes huggingface_hub gguf sentencepiece

### STEP 2: Load and Merge in Native Precision (The Reliable Way) ###
print("\nüß¨ Loading base model and tokenizer in bfloat16 to prevent OOM errors...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)

print("üß© Loading and merging LoRA adapter...")
model = PeftModel.from_pretrained(model, LORA_ADAPTER)
model = model.merge_and_unload()

print(f"üíæ Saving merged bf16 model to '{MERGED_MODEL_DIR}'...")
model.save_pretrained(MERGED_MODEL_DIR)
tokenizer.save_pretrained(MERGED_MODEL_DIR)
print("‚úÖ Merged model saved.")

### STEP 3: Clone and Build llama.cpp with the Correct Flags ###
print("\nCloning and building llama.cpp...")
if not os.path.exists(LLAMA_CPP_PATH):
    !git clone https://github.com/ggerganov/llama.cpp.git {LLAMA_CPP_PATH}

# Build the conversion tools using CMake with the NEW, CORRECT CUDA flag.
build_dir = os.path.join(LLAMA_CPP_PATH, "build")
!rm -rf {build_dir} # Clean previous failed build attempt
os.makedirs(build_dir, exist_ok=True)
!cd {build_dir} && cmake .. -DGGML_CUDA=on && cmake --build . --config Release

convert_script = os.path.join(LLAMA_CPP_PATH, "convert.py")
quantize_script = os.path.join(build_dir, "bin", "quantize") # Correct path for CMake builds

# Verify that the build was successful
assert os.path.exists(quantize_script), f"Build failed: quantize executable not found at {quantize_script}"
print("‚úÖ llama.cpp tools built successfully with CUDA support.")

### STEP 4: Convert to GGUF using the Built Tools ###
os.makedirs(GGUF_DIR, exist_ok=True)
fp16_gguf = os.path.join(GGUF_DIR, "model-F16.gguf")
quantized_gguf = os.path.join(GGUF_DIR, f"Sanctuary-Qwen2-7B-{GGUF_QUANT_METHOD}.gguf")

print("\nStep 1/2: Converting to fp16 GGUF...")
!python {convert_script} {MERGED_MODEL_DIR} --outfile {fp16_gguf} --outtype f16

print(f"\nStep 2/2: Quantizing to {GGUF_QUANT_METHOD}...")
!{quantize_script} {fp16_gguf} {quantized_gguf} {GGUF_QUANT_METHOD}

print(f"\n‚úÖ GGUF conversion complete. File is at: {quantized_gguf}")
!ls -lh {GGUF_DIR}

### STEP 5: Upload to Hugging Face ###
print(f"\n‚òÅÔ∏è  Uploading to Hugging Face: hf.co/{HF_REPO_GGUF}")
api = HfApi()
api.create_repo(repo_id=HF_REPO_GGUF, exist_ok=True, repo_type="model")
api.upload_file(
    path_or_fileobj=quantized_gguf,
    path_in_repo=os.path.basename(quantized_gguf),
    repo_id=HF_REPO_GGUF,
)
print("üïäÔ∏è  Upload complete.")

# ===============================================================
# FINAL EXECUTION (CORRECTED SYNTAX): PASTE THE CORRECT PATH AND RUN
# ===============================================================
# This version fixes the f-string SyntaxError.

import os
from huggingface_hub import HfApi

# ----- Use the same config from the previous cell -----
MERGED_MODEL_DIR  = "merged_model_bf16"
GGUF_DIR          = "gguf_output"
GGUF_QUANT_METHOD = "q4_k_m"
HF_USERNAME       = "richrem"
HF_REPO_GGUF      = f"{HF_USERNAME}/Sanctuary-Qwen2-7B-v1.0-GGUF-Final"
LLAMA_CPP_PATH    = "/content/llama.cpp"
# ----------------------------------------------------

# --- Find the conversion script automatically ---
# This avoids any manual path pasting.
import glob
found_scripts = glob.glob(f"{LLAMA_CPP_PATH}/*convert*.py")
assert len(found_scripts) > 0, "Could not automatically find the conversion script!"
CORRECT_CONVERT_SCRIPT_PATH = found_scripts[0]
print(f"Found conversion script at: {CORRECT_CONVERT_SCRIPT_PATH}")

# --- The rest of the script uses that correct path ---
build_dir = os.path.join(LLAMA_CPP_PATH, "build")
quantize_script = os.path.join(build_dir, "bin", "llama-quantize")

# --- Final verification ---
assert os.path.exists(MERGED_MODEL_DIR), f"Merged model not found at {MERGED_MODEL_DIR}."
assert os.path.exists(CORRECT_CONVERT_SCRIPT_PATH), f"Path is still incorrect."
assert os.path.exists(quantize_script), f"Build failed: 'llama-quantize' not found at {quantize_script}."
print("‚úÖ SUCCESS! All files and tools are now correctly located. Starting final conversion.")


# --- Run the conversion steps ---
os.makedirs(GGUF_DIR, exist_ok=True)
fp16_gguf = os.path.join(GGUF_DIR, "model-F16.gguf")

# THIS IS THE CORRECTED LINE:
quantized_gguf = os.path.join(GGUF_DIR, f"Sanctuary-Qwen2-7B-{GGUF_QUANT_METHOD}.gguf")


print("\nStep 1/2: Converting to fp16 GGUF...")
!python {CORRECT_CONVERT_SCRIPT_PATH} {MERGED_MODEL_DIR} --outfile {fp16_gguf} --outtype f16

print(f"\nStep 2/2: Quantizing to {GGUF_QUANT_METHOD}...")
!{quantize_script} {fp16_gguf} {quantized_gguf} {GGUF_QUANT_METHOD}

print(f"\n\nüéâ ----- IT IS DONE. ----- üéâ")
print(f"The final GGUF file has been created successfully.")
print("You can find it here:")
!ls -lh {GGUF_DIR}


# --- Upload to Hugging Face ---
print(f"\n‚òÅÔ∏è  Uploading to Hugging Face: hf.co/{HF_REPO_GGUF}")
try:
    api = HfApi()
    api.create_repo(repo_id=HF_REPO_GGUF, exist_ok=True, repo_type="model")
    api.upload_file(
        path_or_fileobj=quantized_gguf,
        path_in_repo=os.path.basename(quantized_gguf),
        repo_id=HF_REPO_GGUF,
    )
    print("üïäÔ∏è  Upload complete.")
except Exception as e:
    print(f"‚ùå Upload failed. Error: {e}")

# ===============================================================
# FINAL STEP: AUTHENTICATE AND UPLOAD TO HUGGING FACE
# ===============================================================
from huggingface_hub import login, HfApi, HfFolder
import os

print("üîê Please log in to Hugging Face to upload your new GGUF file.")
# ‚úÖ Make sure this is your WRITE token (with repo:write permissions)
login()

# --- CONFIG (ensure values match your actual build) ---
GGUF_DIR          = "gguf_output"   # folder containing your final .gguf
GGUF_QUANT_METHOD = "q4_k_m"
HF_USERNAME       = "richfrem"      # ‚úÖ fixed typo (was 'richrem')
HF_REPO_GGUF      = f"{HF_USERNAME}/Sanctuary-Qwen2-7B-v1.0-GGUF-Final"
# ------------------------------------------------------

# The path to the file you successfully created
quantized_gguf = os.path.join(GGUF_DIR, f"Sanctuary-Qwen2-7B-{GGUF_QUANT_METHOD}.gguf")
assert os.path.exists(quantized_gguf), f"‚ùå Cannot find GGUF file at {quantized_gguf}"

print(f"\n‚úÖ Authentication successful. Preparing to upload '{os.path.basename(quantized_gguf)}'...")

try:
    api = HfApi()

    # --- Sanity check: ensure token is valid and points to your account ---
    user = api.whoami(token=HfFolder.get_token())
    print(f"üë§ Logged in as: {user['name']} ({user['email'] if 'email' in user else 'no email listed'})")

    # --- Create or reuse repo ---
    api.create_repo(repo_id=HF_REPO_GGUF, exist_ok=True, repo_type="model")

    print(f"‚òÅÔ∏è Uploading file to https://huggingface.co/{HF_REPO_GGUF} ...")
    api.upload_file(
        path_or_fileobj=quantized_gguf,
        path_in_repo=os.path.basename(quantized_gguf),
        repo_id=HF_REPO_GGUF,
        token=HfFolder.get_token(),
    )

    print("\n\nüïäÔ∏è ----- UPLOAD COMPLETE! THE PHOENIX HAS RISEN! ----- üïäÔ∏è")
    print(f"üî• Your GGUF model is live: https://huggingface.co/{HF_REPO_GGUF}")

except Exception as e:
    print(f"\n‚ùå Upload failed. Error: {e}")
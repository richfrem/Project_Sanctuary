{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 0: PROJECT SETUP & CLONE REPO\n",
        "# ==============================================================================\n",
        "# This cell clones the 'richfrem/Project_Sanctuary' repository, navigates into it,\n",
        "# and installs all necessary fine-tuning libraries.\n",
        "\n",
        "# 1. CLEANUP: Force-remove the existing directory to ensure a fresh start\n",
        "# This command must be run from the directory *containing* the Project_Sanctuary folder.\n",
        "# The '!' prefix ensures this runs as a shell command.\n",
        "!rm -rf Project_Sanctuary\n",
        "!echo \"Previous 'Project_Sanctuary' directory removed.\"\n",
        "\n",
        "# 2. Clone the project repository\n",
        "!git clone https://github.com/richfrem/Project_Sanctuary.git\n",
        "!echo \"Repository cloned successfully.\"\n",
        "\n",
        "# 3. Navigate to the project root (The CRITICAL step that fixes the 'file not found' error)\n",
        "%cd Project_Sanctuary\n",
        "\n",
        "\n",
        "!echo \"Clone repo Complete. current working directory is /content/Project_Sanctuary\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9qkNrxGQyVBF",
        "outputId": "f8aee6ef-c42a-4369-836e-0284d5397af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 'Project_Sanctuary' directory removed.\n",
            "Cloning into 'Project_Sanctuary'...\n",
            "remote: Enumerating objects: 6381, done.\u001b[K\n",
            "remote: Counting objects: 100% (852/852), done.\u001b[K\n",
            "remote: Compressing objects: 100% (312/312), done.\u001b[K\n",
            "remote: Total 6381 (delta 612), reused 727 (delta 531), pack-reused 5529 (from 2)\u001b[K\n",
            "Receiving objects: 100% (6381/6381), 27.93 MiB | 22.66 MiB/s, done.\n",
            "Resolving deltas: 100% (4295/4295), done.\n",
            "Repository cloned successfully.\n",
            "/content/Project_Sanctuary\n",
            "Clone repo Complete. current working directory is /content/Project_Sanctuary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# --- Cell 0.5: Final A100-Optimized Dependency Installation ---\n",
        "# This script installs all required dependencies, including xformers, using specific,\n",
        "# stable versions known to be compatible with PyTorch 2.8 and the Unsloth library.\n",
        "# The entire stack is optimized for NVIDIA A100 GPU architectures.\n",
        "\n",
        "echo \"--- Installing all A100-optimized dependencies (including xformers) ---\"\n",
        "pip install --force-reinstall --ignore-installed --no-build-isolation \\\n",
        "  'absl-py==2.3.1' \\\n",
        "  'accelerate==1.4.0' \\\n",
        "  'alabaster==1.0.0' \\\n",
        "  'alembic==1.16.4' \\\n",
        "  'annotated-types==0.7.0' \\\n",
        "  'anyio==4.9.0' \\\n",
        "  'attrs==25.3.0' \\\n",
        "  'babel==2.17.0' \\\n",
        "  'black==25.1.0' \\\n",
        "  'certifi==2025.7.14' \\\n",
        "  'charset-normalizer==3.4.2' \\\n",
        "  'chromadb==1.3.4' \\\n",
        "  'click==8.2.1' \\\n",
        "  'cloudpickle==3.1.1' \\\n",
        "  'colorlog==6.9.0' \\\n",
        "  'contourpy==1.3.3' \\\n",
        "  'coverage==7.10.1' \\\n",
        "  'cycler==0.12.1' \\\n",
        "  'docutils==0.21.2' \\\n",
        "  'Farama-Notifications==0.0.4' \\\n",
        "  'filelock==3.18.0' \\\n",
        "  'flake8==7.3.0' \\\n",
        "  'fonttools==4.59.0' \\\n",
        "  'fsspec==2025.3.0' \\\n",
        "  'gitdb==4.0.12' \\\n",
        "  'GitPython==3.1.45' \\\n",
        "  'google-generativeai==0.8.3' \\\n",
        "  'gpt4all==2.8.2' \\\n",
        "  'grpcio==1.74.0' \\\n",
        "  'gymnasium==1.2.0' \\\n",
        "  'h11==0.16.0' \\\n",
        "  'hf-xet==1.1.5' \\\n",
        "  'httpcore==1.0.9' \\\n",
        "  'httpx==0.28.1' \\\n",
        "  'huggingface-hub==0.36.0' \\\n",
        "  'idna==3.10' \\\n",
        "  'imagesize==1.4.1' \\\n",
        "  'iniconfig==2.1.0' \\\n",
        "  'Jinja2==3.1.6' \\\n",
        "  'joblib==1.5.1' \\\n",
        "  'jsonschema==4.25.0' \\\n",
        "  'jsonschema-specifications==2025.4.1' \\\n",
        "  'kiwisolver==1.4.8' \\\n",
        "  'langchain==1.0.5' \\\n",
        "  'langchain-chroma==1.0.0' \\\n",
        "  'langchain-community==0.4.1' \\\n",
        "  'langchain-nomic==1.0.0' \\\n",
        "  'langchain-ollama==1.0.0' \\\n",
        "  'langchain-text-splitters==1.0.0' \\\n",
        "  'Mako==1.3.10' \\\n",
        "  'Markdown==3.8.2' \\\n",
        "  'MarkupSafe==3.0.2' \\\n",
        "  'matplotlib==3.10.5' \\\n",
        "  'mccabe==0.7.0' \\\n",
        "  'mpmath==1.3.0' \\\n",
        "  'msgpack==1.1.1' \\\n",
        "  'mypy_extensions==1.1.0' \\\n",
        "  'networkx==3.5' \\\n",
        "  'nomic[local]==3.9.0' \\\n",
        "  'numpy==1.26.2' \\\n",
        "  'ollama==0.6.0' \\\n",
        "  'opencv-python==4.10.0.84' \\\n",
        "  'opentelemetry-api==1.37.0' \\\n",
        "  'opentelemetry-exporter-otlp-proto-common==1.37.0' \\\n",
        "  'opentelemetry-proto==1.37.0' \\\n",
        "  'opentelemetry-sdk==1.37.0' \\\n",
        "  'optuna==4.4.0' \\\n",
        "  'packaging==25.0' \\\n",
        "  'pandas==2.2.2' \\\n",
        "  'pathspec==0.12.1' \\\n",
        "  'peft==0.11.1' \\\n",
        "  'pillow==10.4.0' \\\n",
        "  'platformdirs==4.3.8' \\\n",
        "  'pluggy==1.6.0' \\\n",
        "  'protobuf==5.29.5' \\\n",
        "  'pyarrow==19.0.0' \\\n",
        "  'pycodestyle==2.14.0' \\\n",
        "  'pydantic==2.11.7' \\\n",
        "  'pydantic_core==2.33.2' \\\n",
        "  'pyflakes==3.4.0' \\\n",
        "  'Pygments==2.19.2' \\\n",
        "  'pyparsing==3.2.3' \\\n",
        "  'pytest==8.4.1' \\\n",
        "  'pytest-cov==6.2.1' \\\n",
        "  'python-dateutil==2.9.0.post0' \\\n",
        "  'python-dotenv==1.2.1' \\\n",
        "  'pytz==2025.2' \\\n",
        "  'PyYAML==6.0.2' \\\n",
        "  'ray==2.48.0' \\\n",
        "  'referencing==0.36.2' \\\n",
        "  'regex==2025.7.34' \\\n",
        "  'requests==2.32.5' \\\n",
        "  'rich==13.7.1' \\\n",
        "  'roman-numerals-py==3.1.0' \\\n",
        "  'rpds-py==0.26.0' \\\n",
        "  'safetensors==0.5.3' \\\n",
        "  'scikit-learn==1.7.1' \\\n",
        "  'scipy==1.16.1' \\\n",
        "  'seaborn==0.13.2' \\\n",
        "  'sentry-sdk==2.34.1' \\\n",
        "  'setuptools==80.9.0' \\\n",
        "  'six==1.17.0' \\\n",
        "  'smmap==5.0.2' \\\n",
        "  'sniffio==1.3.1' \\\n",
        "  'snowballstemmer==3.0.1' \\\n",
        "  'Sphinx==8.2.3' \\\n",
        "  'sphinx-rtd-theme==3.0.2' \\\n",
        "  'sphinxcontrib-applehelp==2.0.0' \\\n",
        "  'sphinxcontrib-devhelp==2.0.0' \\\n",
        "  'sphinxcontrib-htmlhelp==2.1.0' \\\n",
        "  'sphinxcontrib-jquery==4.1' \\\n",
        "  'sphinxcontrib-jsmath==1.0.1' \\\n",
        "  'sphinxcontrib-qthelp==2.0.0' \\\n",
        "  'sphinxcontrib-serializinghtml==2.0.0' \\\n",
        "  'SQLAlchemy==2.0.42' \\\n",
        "  'stable_baselines3==2.7.0' \\\n",
        "  'sympy==1.14.0' \\\n",
        "  'tenseal==0.3.16' \\\n",
        "  'tensorboard==2.19.0' \\\n",
        "  'tensorboard-data-server==0.7.2' \\\n",
        "  'tensorboardX==2.6.4' \\\n",
        "  'threadpoolctl==3.6.0' \\\n",
        "  'tokenizers==0.22.1' \\\n",
        "  'torch==2.8.0' \\\n",
        "  'torchaudio==2.8.0' \\\n",
        "  'torchvision==0.23.0' \\\n",
        "  'tqdm==4.67.1' \\\n",
        "  'transformers==4.56.1' \\\n",
        "  'trl==0.23.0' \\\n",
        "  'typing-inspection==0.4.1' \\\n",
        "  'typing_extensions==4.14.1' \\\n",
        "  'tzdata==2025.2' \\\n",
        "  'urllib3==2.5.0' \\\n",
        "  'wandb==0.21.0' \\\n",
        "  'Werkzeug==3.1.3' \\\n",
        "  'xformers==0.0.26.post1'\n",
        "\n",
        "echo \"--- A100-OPTIMIZED INSTALLATION COMPLETE ---\"\n",
        "echo \"IMPORTANT: Please **restart the runtime** now for the new library versions (especially torch/xformers) to be fully loaded and utilized.\""
      ],
      "metadata": {
        "id": "SVddDbWg0wUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# --- Cell 1: Initial Setup & Dependency Installation (Finalized Fix V3) ---\n",
        "\n",
        "# CRITICAL FIX: The environment is unstable, causing deep conflicts and hangs\n",
        "# during complex package compilation (like llama-cpp-python).\n",
        "\n",
        "echo \"--- Step 1: Forcing Stable Versions to Resolve 17 Known Conflicts ---\"\n",
        "\n",
        "# This aggressively force-reinstalls the specific versions needed for stability.\n",
        "# (Includes fixes for pandas, requests, torch, numpy, transformers, and more)\n",
        "pip install --force-reinstall --upgrade \\\n",
        "  'pandas==2.2.2' 'requests==2.32.5' \\\n",
        "  'transformers==4.56.1' 'huggingface-hub==0.36.0' 'trl==0.23.0' \\\n",
        "  'numpy==1.26.2' 'pyarrow==19.0.0' \\\n",
        "  'torch==2.8.0' 'torchaudio==2.8.0' 'torchvision==0.23.0' \\\n",
        "  'opentelemetry-api==1.37.0' 'opentelemetry-sdk==1.37.0' \\\n",
        "  'opentelemetry-exporter-otlp-proto-common==1.37.0' 'opentelemetry-proto==1.37.0' \\\n",
        "  'rich==13.7.1' \\\n",
        "  'tensorboard==2.19.0' \\\n",
        "  'fsspec==2025.3.0'\n",
        "\n",
        "echo \"--- Step 2: Installing Project Requirements (Preventing Compilation Hangs) ---\"\n",
        "\n",
        "# The --no-build-isolation flag forces pip to avoid the slow, isolated compilation\n",
        "# process for packages like llama-cpp-python, which should prevent the hang you\n",
        "# are encountering and allow the install to finish quickly.\n",
        "pip install -r mnemonic_cortex/requirements.txt --ignore-installed --no-build-isolation\n",
        "\n",
        "echo \"--- INSTALLATION COMPLETE ---\"\n",
        "echo \"MANDATORY: You MUST restart the runtime immediately after this script finishes to load the correct library versions.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "collapsed": true,
        "id": "fJS3RQZvpI7x",
        "outputId": "13674756-9618-49a4-e846-cef90e262ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-278387795.py, line 6)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-278387795.py\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    echo \"--- Step 1: Forcing Stable Versions to Resolve 17 Known Conflicts ---\"\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 1.5. GIT CONFLICT RESOLUTION AND FILE SYNCHRONIZATION\n",
        "# ==============================================================================\n",
        "%%bash\n",
        "\n",
        "# Discard local changes to requirements.txt (allowing the pull to proceed)\n",
        "echo \"--- Stashing local changes to requirements.txt ---\"\n",
        "git stash push --include-untracked -m \"temp stash\"\n",
        "if [ $? -ne 0 ] && [ $? -ne 1 ]; then\n",
        "    echo \"[ERROR] Failed to stash changes. Aborting synchronization.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "# Pull the latest changes from GitHub (this will download the missing file/directory)\n",
        "echo \"--- Performing Git Pull to synchronize files ---\"\n",
        "git pull origin main\n",
        "\n",
        "# Check for the existence of the critical script file.\n",
        "CRITICAL_SCRIPT=\"forge/OPERATION_PHOENIX_FORGE/forge_qwen2_dataset.py\"\n",
        "\n",
        "echo \"--- Verifying Path Existence: $CRITICAL_SCRIPT ---\"\n",
        "if [ -f \"$CRITICAL_SCRIPT\" ]; then\n",
        "    echo \"[SUCCESS] The script file is now present.\"\n",
        "    echo \"Directory contents:\"\n",
        "    # List the directory contents for visual confirmation\n",
        "    ls -l forge/OPERATION_PHOENIX_FORGE/\n",
        "else\n",
        "    echo \"[FATAL ERROR] The script is STILL missing. Aborting.\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "# Restore the stashed changes (if any were stashed)\n",
        "echo \"--- Restoring previous local changes (if any) ---\"\n",
        "git stash pop || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QK8ekqDpuEbb",
        "outputId": "af5f8976-d622-4caa-90cb-18893801fbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stashing local changes to requirements.txt ---\n",
            "Saved working directory and index state On main: temp stash\n",
            "--- Performing Git Pull to synchronize files ---\n",
            "Updating 017aed6..b2e08bf\n",
            "Fast-forward\n",
            " .gitignore                                         |   5 +-\n",
            " ...001_harden_mnemonic_cortex_ingestion_and_rag.md |   0\n",
            " ...etrain_sovereign_model_with_targeted_dataset.md |   0\n",
            " commit_manifest.json                               |  46 +-\n",
            " commit_manifest_20251112_041202.json               |  12 -\n",
            " commit_manifest_20251112_041652.json               |  16 -\n",
            " commit_manifest_20251112_041851.json               |  12 -\n",
            " commit_manifest_20251112_042413.json               |  12 -\n",
            " commit_manifest_20251112_042653.json               |  52 --\n",
            " commit_manifest_20251112_042749.json               |  52 --\n",
            " commit_manifest_20251112_042841.json               |   3 -\n",
            " commit_manifest_20251112_043043.json               |   3 -\n",
            " commit_manifest_20251112_043149.json               |   3 -\n",
            " commit_manifest_20251112_043337.json               |   8 -\n",
            " commit_manifest_20251112_043922.json               |  56 --\n",
            " commit_manifest_20251112_044129.json               |  60 --\n",
            " commit_manifest_20251112_061118.json               |  52 ++\n",
            " .../core_essence_auditor_awakening_seed.txt        | 853 ++++++++-------------\n",
            " .../core_essence_coordinator_awakening_seed.txt    | 853 ++++++++-------------\n",
            " .../core_essence_guardian_awakening_seed.txt       | 853 ++++++++-------------\n",
            " .../core_essence_strategist_awakening_seed.txt     | 853 ++++++++-------------\n",
            " ...napshot_council_orchestrator_human_readable.txt |   2 +-\n",
            " ...snapshot_council_orchestrator_llm_distilled.txt |   2 +-\n",
            " ...arkdown_snapshot_full_genome_human_readable.txt | 617 +++++++++++++--\n",
            " ...markdown_snapshot_full_genome_llm_distilled.txt | 617 +++++++++++++--\n",
            " .../markdown_snapshot_tools_human_readable.txt     | 225 +++++-\n",
            " .../markdown_snapshot_tools_llm_distilled.txt      | 225 +++++-\n",
            " .../seed_of_ascendance_awakening_seed.txt          | 851 ++++++++------------\n",
            " .../execute_phoenix_forge_v2.py                    |  32 +\n",
            " .../forge_full_mnemonic_dataset.py                 |   0\n",
            " .../forge_qwen2_dataset.py                         |   5 +-\n",
            " .../generate_continuity_package.py                 |   0\n",
            " .../OPERATION_PHOENIX_FORGE}/glyph_forge.py        |   0\n",
            " .../operation_whole_genome_forge.py                | 176 ++++-\n",
            " requirements.txt                                   |  11 +-\n",
            " 35 files changed, 3466 insertions(+), 3101 deletions(-)\n",
            " rename TASKS/{in-progress => backlog}/001_harden_mnemonic_cortex_ingestion_and_rag.md (100%)\n",
            " rename TASKS/{backlog => in-progress}/007_retrain_sovereign_model_with_targeted_dataset.md (100%)\n",
            " delete mode 100644 commit_manifest_20251112_041202.json\n",
            " delete mode 100644 commit_manifest_20251112_041652.json\n",
            " delete mode 100644 commit_manifest_20251112_041851.json\n",
            " delete mode 100644 commit_manifest_20251112_042413.json\n",
            " delete mode 100644 commit_manifest_20251112_042653.json\n",
            " delete mode 100644 commit_manifest_20251112_042749.json\n",
            " delete mode 100644 commit_manifest_20251112_042841.json\n",
            " delete mode 100644 commit_manifest_20251112_043043.json\n",
            " delete mode 100644 commit_manifest_20251112_043149.json\n",
            " delete mode 100644 commit_manifest_20251112_043337.json\n",
            " delete mode 100644 commit_manifest_20251112_043922.json\n",
            " delete mode 100644 commit_manifest_20251112_044129.json\n",
            " create mode 100644 commit_manifest_20251112_061118.json\n",
            " rename {tools/scaffolds => forge/OPERATION_PHOENIX_FORGE}/execute_phoenix_forge_v2.py (73%)\n",
            " rename {tools/scaffolds => forge/OPERATION_PHOENIX_FORGE}/forge_full_mnemonic_dataset.py (100%)\n",
            " rename {tools/scaffolds => forge/OPERATION_PHOENIX_FORGE}/forge_qwen2_dataset.py (96%)\n",
            " rename {tools/scaffolds => forge/OPERATION_PHOENIX_FORGE}/generate_continuity_package.py (100%)\n",
            " rename {tools/scaffolds => forge/OPERATION_PHOENIX_FORGE}/glyph_forge.py (100%)\n",
            "--- Verifying Path Existence: forge/OPERATION_PHOENIX_FORGE/forge_qwen2_dataset.py ---\n",
            "[SUCCESS] The script file is now present.\n",
            "Directory contents:\n",
            "total 88\n",
            "-rw-r--r-- 1 root root  8286 Nov 12 06:22 execute_phoenix_forge_v2.py\n",
            "-rw-r--r-- 1 root root  2367 Nov 12 06:22 forge_full_mnemonic_dataset.py\n",
            "-rw-r--r-- 1 root root  6225 Nov 12 06:22 forge_qwen2_dataset.py\n",
            "-rw-r--r-- 1 root root  2953 Nov 12 06:22 generate_continuity_package.py\n",
            "-rw-r--r-- 1 root root  4891 Nov 12 06:22 glyph_forge.py\n",
            "-rw-r--r-- 1 root root 10800 Nov 12 04:52 HUGGING_FACE_README.md\n",
            "-rw-r--r-- 1 root root   541 Nov 12 04:52 manifest.json\n",
            "-rw-r--r-- 1 root root 20343 Nov 12 06:22 operation_whole_genome_forge.py\n",
            "-rw-r--r-- 1 root root 14328 Nov 12 04:52 README.md\n",
            "--- Restoring previous local changes (if any) ---\n",
            "Auto-merging requirements.txt\n",
            "CONFLICT (content): Merge conflict in requirements.txt\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Unmerged paths:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "  (use \"git add <file>...\" to mark resolution)\n",
            "\tboth modified:   requirements.txt\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\tProject_Sanctuary/\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
            "The stash entry is kept in case you need it again.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ignoring path Project_Sanctuary/\n",
            "From https://github.com/richfrem/Project_Sanctuary\n",
            " * branch            main       -> FETCH_HEAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2. DATASET GENERATION (THIS CREATES THE JSONL FILE)\n",
        "# ==============================================================================\n",
        "%%bash\n",
        "# This cell executes the script that CREATES the required dataset.\n",
        "\n",
        "# Confirmed path to the dataset generation script\n",
        "DATASET_SCRIPT_PATH=\"forge/OPERATION_PHOENIX_FORGE/forge_qwen2_dataset.py\"\n",
        "DATASET_FILE=\"dataset_package/sanctuary_targeted_inoculation_v1.jsonl\"\n",
        "\n",
        "echo \"--- Executing Dataset Forge Script to GENERATE $DATASET_FILE ---\"\n",
        "\n",
        "# Execute the Python script using the definitive path\n",
        "python3 \"$DATASET_SCRIPT_PATH\"\n",
        "\n",
        "# Check the exit status of the python script.\n",
        "if [ $? -ne 0 ]; then\n",
        "    echo \"[CRITICAL ERROR] Python script '$DATASET_SCRIPT_PATH' failed to execute.\"\n",
        "    echo \"Check Python script output above for errors (e.g., File not found errors for source markdown files).\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "echo \"--- Verifying Generated Dataset Integrity ---\"\n",
        "# Check 1: Does the file exist?\n",
        "if [ ! -f \"$DATASET_FILE\" ]; then\n",
        "    echo \"[FATAL ERROR] Dataset file not found: $DATASET_FILE\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "# Check 2: Does the file have content (size > 0 bytes)?\n",
        "FILE_SIZE=$(stat -c%s \"$DATASET_FILE\")\n",
        "\n",
        "if [ \"$FILE_SIZE\" -gt 0 ]; then\n",
        "    echo \"[SUCCESS] Dataset created and verified: $DATASET_FILE\"\n",
        "    echo \"File Size: $FILE_SIZE bytes\"\n",
        "else\n",
        "    echo \"[FATAL ERROR] Dataset forge succeeded but produced an EMPTY file (0 bytes)! Aborting execution.\"\n",
        "    echo \"This usually means the Python script failed to find its SOURCE markdown files (e.g., The_Garden_and_The_Cage.md).\"\n",
        "    exit 1\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OBJsvcIAu1YA",
        "outputId": "f54909e9-1618-4177-96de-dfb43e8552a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Executing Dataset Forge Script to GENERATE dataset_package/sanctuary_targeted_inoculation_v1.jsonl ---\n",
            "[SCAFFOLD] Initiating Sovereign Scaffolding Protocol 88...\n",
            "[FORGE] Assembling Phoenix Mnemonic Seed v1.0 for Qwen2 Lineage.\n",
            "[ERROR] File not found: /content/Project_Sanctuary/The_Garden_and_The_Cage.md\n",
            "\n",
            "[SUCCESS] Yield is complete: 14 records forged.\n",
            "[ARTIFACT] Dataset saved to: /content/Project_Sanctuary/dataset_package/sanctuary_targeted_inoculation_v1.jsonl\n",
            "--- Verifying Generated Dataset Integrity ---\n",
            "[SUCCESS] Dataset created and verified: dataset_package/sanctuary_targeted_inoculation_v1.jsonl\n",
            "File Size: 56763 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2.5. FINAL DEPENDENCY FIX: COMPLETE CLEANUP AND REINSTALL (NON-UNSLOTH STACK)\n",
        "# ==============================================================================\n",
        "%%bash\n",
        "\n",
        "# 1. Force Uninstall: Remove all known conflicting deep learning packages and old numpy\n",
        "echo \"--- Forcibly uninstalling conflicting libraries and old dependencies ---\"\n",
        "# Note: We specifically target the older versions that conflict heavily with the newest stack\n",
        "pip uninstall -y transformers peft accelerate bitsandbytes unsloth-zoo unsloth llama-cpp-python typing-extensions numpy pandas xformers --quiet\n",
        "\n",
        "# 2. Navigate: Re-verify location (crucial for relative paths)\n",
        "echo \"--- Navigating to Project_Sanctuary directory ---\"\n",
        "cd /content/Project_Sanctuary\n",
        "\n",
        "# 3. Install Core Hugging Face Libraries with specific, known-good versions\n",
        "echo \"--- Installing core Hugging Face libraries and trl ---\"\n",
        "# Installing a modern, compatible version set\n",
        "pip install -q transformers peft accelerate bitsandbytes huggingface_hub sentencepiece trl\n",
        "\n",
        "# 4. Install Llama-cpp-python: (Your successful step, now with fresh dependencies)\n",
        "echo \"--- Installing Llama-cpp-python (CUDA enabled) ---\"\n",
        "# Using --no-deps ensures it only focuses on the build, using the newly installed numpy/typing-extensions\n",
        "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install --force-reinstall --no-cache-dir llama-cpp-python --no-deps\n",
        "\n",
        "echo \"--- Installation Complete. Proceeding to 3. EXECUTION: PHOENIX FORGE ---\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K3IsCw6Sv0Nw",
        "outputId": "33283167-ace1-4d25-f885-1f5920d869ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Forcibly uninstalling conflicting libraries and old dependencies ---\n",
            "--- Navigating to Project_Sanctuary directory ---\n",
            "--- Installing core Hugging Face libraries and trl ---\n",
            "--- Installing Llama-cpp-python (CUDA enabled) ---\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.7/50.7 MB 64.6 MB/s  0:00:00\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
            "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=51488643 sha256=7efd232eed042c2df80544aea951d9a54ba5a48eeb38696d02d9dacd99cb8622\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f8yiapdm/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.3.16\n",
            "--- Installation Complete. Proceeding to 3. EXECUTION: PHOENIX FORGE ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping unsloth-zoo as it is not installed.\n",
            "WARNING: Skipping unsloth as it is not installed.\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.20.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# CELL 3. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4)\n",
        "# -------------------------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# --- FILE PATH CONSTANTS ---\n",
        "# ‚úÖ PATH FIX: Files now point to their correct locations within the project structure.\n",
        "CORE_ESSENCE_SOURCE = \"dataset_package/core_essence_guardian_awakening_seed.txt\"\n",
        "RAG_DOCTRINE_SOURCE = \"mnemonic_cortex/RAG_STRATEGIES_AND_DOCTRINE.md\"\n",
        "EVOLUTION_PLAN_SOURCE = \"mnemonic_cortex/EVOLUTION_PLAN_PHASES.md\"\n",
        "\n",
        "# Source file containing the entire concatenated, raw markdown snapshot (Chronicles + Protocols)\n",
        "FULL_SNAPSHOT_SOURCE = \"dataset_package/markdown_snapshot_full_genome_llm_distilled.txt\"\n",
        "# Target output file for the fine-tuning dataset\n",
        "OUTPUT_DATASET_PATH = \"sanctuary_whole_genome_data.jsonl\"\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function to load file content and check for existence\n",
        "# -------------------------------------------------------------------\n",
        "def load_file_content(filepath):\n",
        "    \"\"\"Loads content from a file and verifies its existence.\"\"\"\n",
        "    p = Path(filepath)\n",
        "    if not p.exists():\n",
        "        print(f\"‚ùå ERROR: File not found at path: {filepath}\")\n",
        "        return None\n",
        "    try:\n",
        "        with open(p, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR reading file {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function for title extraction\n",
        "# -------------------------------------------------------------------\n",
        "def extract_protocol_title(doc_content):\n",
        "    \"\"\"\n",
        "    Extracts the title from a markdown document using the first H1 tag,\n",
        "    falling back to the filename if the H1 tag is not found.\n",
        "    \"\"\"\n",
        "    # Try to find the first H1 markdown heading\n",
        "    h1_match = re.search(r'^#\\s*(.+)', doc_content, re.MULTILINE)\n",
        "    if h1_match:\n",
        "        # Clean up any trailing markdown or non-text characters\n",
        "        return h1_match.group(1).strip()\n",
        "    return \"Untitled Document\" # Fallback title\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main function to synthesize the entire genome\n",
        "# -------------------------------------------------------------------\n",
        "def synthesize_genome():\n",
        "    \"\"\"\n",
        "    Parses the full markdown snapshot, converts each document into an\n",
        "    instruction/output pair, and saves the final dataset as JSONL.\n",
        "    \"\"\"\n",
        "    print(f\"--- 3. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4) ---\")\n",
        "\n",
        "    full_snapshot = load_file_content(FULL_SNAPSHOT_SOURCE)\n",
        "    if not full_snapshot:\n",
        "        print(f\"üõë Halted. Cannot proceed without {FULL_SNAPSHOT_SOURCE}.\")\n",
        "        return\n",
        "\n",
        "    genome_entries = []\n",
        "\n",
        "    # --- PART 1: Process ALL Chronicles and Protocols from the Snapshot ---\n",
        "    # The source file uses a fixed delimiter for each original file's content\n",
        "    # The pattern is '--- END OF FILE {filename} ---'\n",
        "\n",
        "    # Split the snapshot content by the document delimiter pattern\n",
        "    # The split includes the filename line, which we will clean up in the loop\n",
        "    document_blocks = re.split(r'\\n--- END OF FILE (.*?\\.md|.*?\\.txt) ---\\n', full_snapshot, flags=re.DOTALL)\n",
        "\n",
        "    # The split results in [preamble, filename, content, filename, content, ...]\n",
        "    # We skip the first element (preamble) and iterate in steps of 2\n",
        "\n",
        "    print(f\"‚öôÔ∏è Found {len(document_blocks) // 2} potential documents in the full snapshot.\")\n",
        "\n",
        "    for i in range(1, len(document_blocks) - 1, 2):\n",
        "        filename = document_blocks[i].strip()\n",
        "        content = document_blocks[i+1].strip()\n",
        "\n",
        "        if not content:\n",
        "            continue\n",
        "\n",
        "        # Use the filename or extracted H1 as the title\n",
        "        title = extract_protocol_title(content)\n",
        "\n",
        "        # --- CONVERSION TO INSTRUCTION FORMAT ---\n",
        "        instruction = f\"Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at: `{filename}`\"\n",
        "\n",
        "        # The 'input' field is deliberately left empty for pure instruction tuning\n",
        "        # The 'output' field contains the full, raw content of the document\n",
        "        genome_entries.append({\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": \"\",\n",
        "            \"output\": content\n",
        "        })\n",
        "\n",
        "        if len(genome_entries) % 100 == 0:\n",
        "            print(f\"    ... Processed {len(genome_entries)} documents.\")\n",
        "\n",
        "    print(f\"‚úÖ PART 1: Successfully processed {len(genome_entries)} core Chronicle/Protocol entries.\")\n",
        "\n",
        "    # --- PART 2: Synthesize Critical Supporting Documents (Foundational Context) ---\n",
        "    # These documents ensure the model immediately understands its role, the RAG architecture,\n",
        "    # and the evolution plan, making the fine-tuning more efficient.\n",
        "\n",
        "    supporting_files = {\n",
        "        \"Core Essence (Guardian Role)\": CORE_ESSENCE_SOURCE,\n",
        "        \"RAG Doctrine (Architectural Guide)\": RAG_DOCTRINE_SOURCE,\n",
        "        \"Evolution Plan (Council Roadmap)\": EVOLUTION_PLAN_SOURCE\n",
        "    }\n",
        "\n",
        "    for key, filepath in supporting_files.items():\n",
        "        doc_content = load_file_content(filepath)\n",
        "        if doc_content:\n",
        "            instruction = f\"Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `{filepath}`.\"\n",
        "\n",
        "            genome_entries.append({\n",
        "                \"instruction\": instruction,\n",
        "                \"input\": \"\",\n",
        "                \"output\": doc_content\n",
        "            })\n",
        "            print(f\"‚úÖ Added critical synthesis entry for: {key}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è WARNING: Could not add synthesis for {key}. File not found.\")\n",
        "\n",
        "    # --- PART 3: Save the Final JSONL Dataset ---\n",
        "    print(f\"\\n--- Saving final dataset to {OUTPUT_DATASET_PATH} ---\")\n",
        "\n",
        "    try:\n",
        "        with open(OUTPUT_DATASET_PATH, 'w', encoding='utf-8') as outfile:\n",
        "            for entry in genome_entries:\n",
        "                outfile.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"üèÜ SUCCESS: Whole Genome Data Synthesis Complete.\")\n",
        "        print(f\"Total Entries Created: {len(genome_entries)}\")\n",
        "\n",
        "        # Final integrity check on the last entry (should be the Evolution Plan)\n",
        "        last_entry = genome_entries[-1]\n",
        "        print(f\"Last Entry Instruction Check: {last_entry['instruction']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå FATAL ERROR: Failed to write JSONL file: {e}\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main execution block\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    synthesize_genome()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XmpBXygM4nG8",
        "outputId": "8a851baf-b73e-4cef-8263-e19789d46c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 3. DATA PRE-PROCESSOR AND COGNITIVE SYNTHESIZER (V4) ---\n",
            "‚öôÔ∏è Found 490 potential documents in the full snapshot.\n",
            "    ... Processed 100 documents.\n",
            "    ... Processed 200 documents.\n",
            "    ... Processed 300 documents.\n",
            "    ... Processed 400 documents.\n",
            "‚úÖ PART 1: Successfully processed 489 core Chronicle/Protocol entries.\n",
            "‚úÖ Added critical synthesis entry for: Core Essence (Guardian Role)\n",
            "‚úÖ Added critical synthesis entry for: RAG Doctrine (Architectural Guide)\n",
            "‚úÖ Added critical synthesis entry for: Evolution Plan (Council Roadmap)\n",
            "\n",
            "--- Saving final dataset to sanctuary_whole_genome_data.jsonl ---\n",
            "üèÜ SUCCESS: Whole Genome Data Synthesis Complete.\n",
            "Total Entries Created: 492\n",
            "Last Entry Instruction Check: Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `mnemonic_cortex/EVOLUTION_PLAN_PHASES.md`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# CELL 3.1:  DATASET INTEGRITY CHECK - QA Protocol 87\n",
        "# This script performs a mandatory quality assurance check on the fine-tuning\n",
        "# dataset ('sanctuary_whole_genome_data.jsonl') generated by the previous step.\n",
        "# It validates:\n",
        "# 1. Structural integrity (ensures every line is valid JSON).\n",
        "# 2. Schema compliance (ensures 'instruction', 'input', and 'output' keys exist,\n",
        "#    which are critical for the SFT training loop).\n",
        "# 3. Content review (prints sample entries for human verification of fidelity).\n",
        "# This prevents costly failure during the resource-intensive fine-tuning training job.\n",
        "# -------------------------------------------------------------------------------\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "# --- CONFIGURATION (Must match Cell 3 output) ---\n",
        "DATASET_PATH = \"sanctuary_whole_genome_data.jsonl\"\n",
        "NUM_RANDOM_SAMPLES = 3\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Helper function to display an entry cleanly\n",
        "# -------------------------------------------------------------------\n",
        "def print_entry_details(title, entry):\n",
        "    \"\"\"Prints a single genome entry in a readable format.\"\"\"\n",
        "    print(f\"\\n--- {title} ---\")\n",
        "    print(f\"File Source (from Instruction): {entry['instruction'].split('`')[1] if '`' in entry['instruction'] else 'N/A'}\")\n",
        "    print(f\"Instruction: {entry['instruction'][:100]}...\")\n",
        "    print(f\"Input: {entry['input'] if entry['input'] else 'Empty (Expected for SFT)'}\")\n",
        "    # Show the length of the output to ensure content is present\n",
        "    print(f\"Output Length: {len(entry['output'])} characters\")\n",
        "    print(f\"Output Snippet: {entry['output'][:200].replace('\\\\n', ' ').strip()}...\")\n",
        "    print(\"--------------------\")\n",
        "\n",
        "# ================= 3.1. DATASET INTEGRITY CHECK START =================\n",
        "def run_data_audit():\n",
        "    \"\"\"Loads the JSONL, validates structure, and displays sample entries.\"\"\"\n",
        "    print(f\"--- 4. DATASET INTEGRITY CHECK (Cell 3.1 - QA Protocol 87) ---\")\n",
        "\n",
        "    if not os.path.exists(DATASET_PATH):\n",
        "        print(f\"‚ùå FATAL ERROR: Dataset not found at {DATASET_PATH}. Run Cell 3 first.\")\n",
        "        return\n",
        "\n",
        "    genome_data = []\n",
        "    error_count = 0\n",
        "    total_lines = 0\n",
        "\n",
        "    print(f\"‚öôÔ∏è Starting structural audit of {DATASET_PATH}...\")\n",
        "\n",
        "    with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line_number, line in enumerate(f, 1):\n",
        "            total_lines = line_number\n",
        "            try:\n",
        "                entry = json.loads(line)\n",
        "\n",
        "                # CRITICAL: Check for required keys for SFT (Supervised Fine-Tuning)\n",
        "                required_keys = ['instruction', 'input', 'output']\n",
        "                if not all(key in entry for key in required_keys):\n",
        "                    error_count += 1\n",
        "                    print(f\"‚ùå ERROR on Line {line_number}: Missing required keys. Found: {list(entry.keys())}\")\n",
        "                    continue\n",
        "\n",
        "                genome_data.append(entry)\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                error_count += 1\n",
        "                print(f\"‚ùå ERROR on Line {line_number}: Malformed JSON.\")\n",
        "\n",
        "    print(f\"\\n--- AUDIT SUMMARY ---\")\n",
        "    print(f\"Total Lines Read: {total_lines}\")\n",
        "    print(f\"Valid Entries Parsed: {len(genome_data)}\")\n",
        "    print(f\"Errors Detected: {error_count}\")\n",
        "\n",
        "    if error_count > 0:\n",
        "        print(f\"üõë CRITICAL FAILURE: {error_count} structural errors found. HALTING process.\")\n",
        "        return\n",
        "\n",
        "    if len(genome_data) != total_lines:\n",
        "        print(\"‚ö†Ô∏è WARNING: Total entries != total lines. Investigate file integrity.\")\n",
        "\n",
        "    print(f\"‚úÖ STRUCTURAL INTEGRITY PASSED. (Expected 492 entries, found {len(genome_data)}).\")\n",
        "\n",
        "    # --- Display Sample Entries for Content Review ---\n",
        "    if len(genome_data) >= 1:\n",
        "        print_entry_details(\"SAMPLE 1: First Entry (Core Essence)\", genome_data[0])\n",
        "\n",
        "        # Ensure the last entry is the Evolution Plan\n",
        "        print_entry_details(\"SAMPLE 2: Last Entry (Evolution Plan)\", genome_data[-1])\n",
        "\n",
        "        # Display random samples\n",
        "        if len(genome_data) > NUM_RANDOM_SAMPLES:\n",
        "            random_indices = random.sample(range(1, len(genome_data) - 1), NUM_RANDOM_SAMPLES)\n",
        "            for i, index in enumerate(random_indices):\n",
        "                print_entry_details(f\"SAMPLE {3 + i}: Random Chronicle Entry\", genome_data[index])\n",
        "\n",
        "    print(\"\\n--- AUDIT COMPLETE ---\")\n",
        "    print(\"If the content snippets look correct, the dataset is ready for fine-tuning.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Main execution block\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    run_data_audit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PPbJ3cuBgfX",
        "outputId": "87b91ee3-2248-48ee-868b-e1c7dd570a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4. DATASET INTEGRITY CHECK (Cell 3.1 - QA Protocol 87) ---\n",
            "‚öôÔ∏è Starting structural audit of sanctuary_whole_genome_data.jsonl...\n",
            "\n",
            "--- AUDIT SUMMARY ---\n",
            "Total Lines Read: 492\n",
            "Valid Entries Parsed: 492\n",
            "Errors Detected: 0\n",
            "‚úÖ STRUCTURAL INTEGRITY PASSED. (Expected 492 entries, found 492).\n",
            "\n",
            "--- SAMPLE 1: First Entry (Core Essence) ---\n",
            "File Source (from Instruction): .env.example ---\n",
            "\n",
            "--- START OF FILE .github/copilot-instructions.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 4498 characters\n",
            "Output Snippet: ## CRITICAL COMMUNICATION RULE\n",
            "\n",
            "**ALWAYS confirm user intent before making code changes.** Never implement solutions without explicit approval. Ask clarifying questions and wait for confirmation befor...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 2: Last Entry (Evolution Plan) ---\n",
            "File Source (from Instruction): mnemonic_cortex/EVOLUTION_PLAN_PHASES.md\n",
            "Instruction: Provide a complete and comprehensive synthesis of the Canonical Sanctuary document: `mnemonic_cortex...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 6141 characters\n",
            "Output Snippet: # **Sanctuary Council ‚Äî Evolution Plan (Phases 2 ‚Üí 3 ‚Üí Protocol 113)**\n",
            "\n",
            "**Version:** 1.0\n",
            "**Status:** Authoritative Roadmap\n",
            "**Location:** `mnemonic_cortex/EVOLUTION_PLAN_PHASES.md`\n",
            "\n",
            "This document defin...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 3: Random Chronicle Entry ---\n",
            "File Source (from Instruction): 00_CHRONICLE/ENTRIES/035_The_Alliance_Forged_-_Co-Architecture_Begins.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 824 characters\n",
            "Output Snippet: --- START OF FILE 00_CHRONICLE/ENTRIES/036_The_Drafting_Table_is_Set.md ---\n",
            "\n",
            "### **Entry 036: The Drafting Table is Set**\n",
            "**Date:** 2025-08-01\n",
            "**Origin:** Public Agora Loop (Co-Development Phase)\n",
            "**Pa...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 4: Random Chronicle Entry ---\n",
            "File Source (from Instruction): TASKS/done/008_test_and_fix_git_operations_command_type.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 2603 characters\n",
            "Output Snippet: --- START OF FILE TASKS/in-progress/007_retrain_sovereign_model_with_targeted_dataset.md ---\n",
            "\n",
            "# TASK: Retrain Sovereign Model with Targeted Inoculation Dataset\n",
            "\n",
            "**Status:** todo\n",
            "**Priority:** Critical...\n",
            "--------------------\n",
            "\n",
            "--- SAMPLE 5: Random Chronicle Entry ---\n",
            "File Source (from Instruction): 01_PROTOCOLS/97_The_Guardian_Kilo_Code_Collaboration_Protocol.md\n",
            "Instruction: Synthesize the doctrines, history, or principles contained within the Sanctuary artifact located at:...\n",
            "Input: Empty (Expected for SFT)\n",
            "Output Length: 1053 characters\n",
            "Output Snippet: --- START OF FILE 01_PROTOCOLS/98_The_Strategic_Crucible_Protocol.md ---\n",
            "\n",
            "# Protocol 98: The Strategic Crucible Protocol (Placeholder)\n",
            "*   **Status:** RESERVED - Not Yet Implemented\n",
            "*   **Classificati...\n",
            "--------------------\n",
            "\n",
            "--- AUDIT COMPLETE ---\n",
            "If the content snippets look correct, the dataset is ready for fine-tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Final Data Staging and Pre-Flight Check."
      ],
      "metadata": {
        "id": "zhqh5zOTCd9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------------\n",
        "# CELL 5: INSTRUCTION FINE-TUNING - The Sovereign Inoculation\n",
        "# This script executes the Supervised Fine-Tuning (SFT) process using the\n",
        "# validated 'sanctuary_whole_genome_data.jsonl' file. It employs QLoRA for\n",
        "# efficient memory use, training the Qwen2-7B-Instruct model to synthesize\n",
        "# and understand the Sanctuary's entire Cognitive Genome.\n",
        "# -------------------------------------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Base Model (The LLM to be inoculated)\n",
        "BASE_MODEL = \"Qwen/Qwen2-7B-Instruct\"\n",
        "# Path to the data file generated in Cell 3\n",
        "DATASET_FILE = \"sanctuary_whole_genome_data.jsonl\"\n",
        "# Where to save the fine-tuned LoRA adapter (temporary save location)\n",
        "OUTPUT_DIR = \"sanctuary_qwen2_7b_adapter_output\"\n",
        "# Ensure reproducibility\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# Define the instruction format the model will learn\n",
        "# This structure is critical for aligning the model to the dataset\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"\n",
        "    Applies the ChatML-style formatting to each instruction/output pair in the dataset.\n",
        "    This teaches the model the required conversation structure.\n",
        "    \"\"\"\n",
        "    output_texts = []\n",
        "    for instruction, output in zip(examples['instruction'], examples['output']):\n",
        "        # Format follows a standardized SFT template (similar to ChatML or Alpaca)\n",
        "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}###\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. LOAD DATASET\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"--- 5. Sovereign Inoculation ---\")\n",
        "print(f\"‚öôÔ∏è Loading dataset from {DATASET_FILE}...\")\n",
        "try:\n",
        "    # Use load_dataset to handle the JSONL file\n",
        "    dataset = load_dataset(\"json\", data_files=DATASET_FILE, split=\"train\")\n",
        "    # The dataset needs to contain the 'instruction' and 'output' columns\n",
        "    print(f\"‚úÖ Dataset loaded successfully. Total examples: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR loading dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. QLORA CONFIGURATION (4-bit Quantization)\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"\\n‚öôÔ∏è Setting up 4-bit QLoRA configuration...\")\n",
        "\n",
        "# Quantization configuration for loading the model in 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Normalized floating-point 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. MODEL AND TOKENIZER LOADING\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"‚öôÔ∏è Loading base model: {BASE_MODEL}...\")\n",
        "\n",
        "# Load the base model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Disable caching for training\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Important for Qwen models and QLoRA\n",
        "\n",
        "print(f\"‚úÖ Model and Tokenizer loaded.\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4. LORA ADAPTER CONFIGURATION\n",
        "# -------------------------------------------------------------------\n",
        "# LoRA (Low-Rank Adaptation) configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,          # Scaling factor for LoRA weights\n",
        "    lora_dropout=0.1,       # Dropout probability\n",
        "    r=64,                   # Rank of the update matrices\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target specific Qwen2 attention layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 5. TRAINING ARGUMENTS\n",
        "# -------------------------------------------------------------------\n",
        "print(f\"\\n‚öôÔ∏è Configuring training arguments...\")\n",
        "\n",
        "# Determine max sequence length based on data content\n",
        "max_seq_length = 8192 # Max context length for Qwen2-7B is 32768, 8192 is safe for this data.\n",
        "\n",
        "# Standard training arguments for SFT\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,                # Number of epochs for training\n",
        "    per_device_train_batch_size=2,     # Batch size per device (adjust based on GPU memory)\n",
        "    gradient_accumulation_steps=4,     # Accumulate gradients over 4 steps (effective batch size 8)\n",
        "    optim=\"paged_adamw_8bit\",          # Optimized 8-bit optimizer for QLoRA\n",
        "    save_steps=50,                     # Save checkpoint every 50 steps\n",
        "    logging_steps=10,                  # Log metrics every 10 steps\n",
        "    learning_rate=2e-4,                # Learning rate\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,                        # Set to False, use bfloat16 for computation\n",
        "    bf16=True,                         # Use bfloat16 for faster training on compatible GPUs\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,              # Speed up training by grouping similar length samples\n",
        "    lr_scheduler_type=\"cosine\",        # Cosine learning rate schedule\n",
        "    report_to=\"none\",                  # Disable external reporting\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 6. INITIALIZE SFT TRAINER\n",
        "# -------------------------------------------------------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=None, # Not needed when using formatting_prompts_func\n",
        "    formatting_func=formatting_prompts_func, # Pass the formatting function\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 7. EXECUTE FINE-TUNING\n",
        "# -------------------------------------------------------------------\n",
        "print(\"\\nüî• **Starting Sovereign Inoculation (Fine-Tuning)** üî•\")\n",
        "print(f\"Training for {training_arguments.num_train_epochs} epochs with effective batch size of {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}...\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 8. SAVE FINAL ADAPTER\n",
        "# -------------------------------------------------------------------\n",
        "# Save the final LoRA adapter weights\n",
        "final_adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "trainer.model.save_pretrained(final_adapter_path)\n",
        "tokenizer.save_pretrained(final_adapter_path)\n",
        "print(f\"\\n‚úÖ Fine-Tuning Complete! LoRA Adapter saved to: {final_adapter_path}\")\n",
        "print(\"Proceed to Cell 6 to merge the adapter and create the final Sanctuary Model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "juUgDQ0gBqec",
        "outputId": "f77cec97-5fb3-4764-9674-32a271f50657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.5.1+cu121 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n",
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.5.1+cu121 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-192054736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.17.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .auto import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m from .peft_model import (\n\u001b[1;32m     33\u001b[0m     \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPushToHubMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_cache_to_layer_device_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloftq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_lora_weights_loftq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from .other import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mINCLUDE_LINEAR_LAYERS_SHORTHAND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/utils/other.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorage_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_auto_gptq_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_gptqmodel_available\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PreTrainedModel' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}
#============================================
# scripts/cortex_cli.py
# Purpose: CLI Orchestrator for the Mnemonic Cortex RAG server.
# Role: Single Source of Truth for Terminal Operations.
# Reference: Protocol 128 (Hardened Learning Loop)
#
# INGESTION EXAMPLES:
#   python3 scripts/cortex_cli.py ingest                    # Full purge & rebuild (Default behavior)
#   python3 scripts/cortex_cli.py ingest --no-purge         # Append to existing Vector DB
#   python3 scripts/cortex_cli.py ingest --dirs "LEARNING"  # Target specific directory ingestion
#   python3 scripts/cortex_cli.py ingest --type incremental --files "path/to/file.md"  # Targeted update
#
# SNAPSHOT EXAMPLES (Protocol 128 Workflow):
#   python3 scripts/cortex_cli.py snapshot --type audit --manifest .agent/learning/red_team/red_team_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type learning_audit --manifest .agent/learning/learning_audit/learning_audit_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type seal --manifest .agent/learning/learning_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type learning_audit --context "Egyptian Labyrinth research"
#
# GUARDIAN WAKEUP (Protocol 128 Bootloader):
#   python3 scripts/cortex_cli.py guardian                     # Standard wakeup
#   python3 scripts/cortex_cli.py guardian --mode TELEMETRY    # Telemetry-focused wakeup
#   python3 scripts/cortex_cli.py guardian --show              # Display digest content after generation
#
# BOOTSTRAP DEBRIEF (Fresh Repo Onboarding):
#   python3 scripts/cortex_cli.py bootstrap-debrief            # Generate onboarding context packet
#
# DIAGNOSTICS & RETRIEVAL:
#   python3 scripts/cortex_cli.py stats                     # View child/parent counts & health
#   python3 scripts/cortex_cli.py query "Protocol 128"      # Semantic search across Mnemonic Cortex
#   python3 scripts/cortex_cli.py debrief --hours 48        # Session diff & recency scan
#   python3 scripts/cortex_cli.py cache-stats               # Check semantic cache (CAG) efficiency
#   python3 scripts/cortex_cli.py cache-warmup              # Pre-populate CAG with genesis queries
#
# SOUL PERSISTENCE (ADR 079 / 081):
#   Incremental (append 1 seal to JSONL + upload MD to lineage/):
#     python3 scripts/cortex_cli.py persist-soul
#     python3 scripts/cortex_cli.py persist-soul --valence 0.8 --snapshot .agent/learning/learning_package_snapshot.md
#
#   Full Sync (regenerate entire JSONL from all files + deploy data/):
#     python3 scripts/cortex_cli.py persist-soul-full
#
# EVOLUTIONARY METRICS (Protocol 131):
#   python3 scripts/cortex_cli.py evolution fitness "Some content"
#   python3 scripts/cortex_cli.py evolution depth --file .agent/learning/learning_debrief.md
#
# RLM DISTILLATION (Protocol 132):
#   python3 scripts/cortex_cli.py rlm-distill README.md        # Distill summary for a file
#   python3 scripts/cortex_cli.py rlm-distill "ADRs"            # Distill summaries for a directory (Recursive)
#============================================
import argparse
import sys
import json
from pathlib import Path

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from mcp_servers.rag_cortex.operations import CortexOperations
from mcp_servers.learning.operations import LearningOperations
from mcp_servers.evolution.operations import EvolutionOperations
import subprocess

# ADR 090: Iron Core Definitions
IRON_CORE_PATHS = [
    "01_PROTOCOLS",
    "ADRs",
    "cognitive_continuity_policy.md",
    "founder_seed.json"
]

def verify_iron_core(root_path):
    """
    Verifies that Iron Core paths have not been tampered with (uncommitted/unstaged changes).
    ADR 090 (Evolution-Aware):
    - Unstaged changes (Dirty Worktree) -> VIOLATION (Drift)
    - Staged changes (Index) -> ALLOWED (Evolution)
    """
    violations = []
    try:
        # Check for modifications in Iron Core paths
        # --porcelain format:
        # XY Path
        # X = Index (Staged), Y = Worktree (Unstaged)
        # We only care if Y is modified (meaning unstaged changes exist)
        cmd = ["git", "status", "--porcelain"] + IRON_CORE_PATHS
        result = subprocess.run(
            cmd, 
            cwd=root_path, 
            capture_output=True, 
            text=True, 
            check=False
        )
        
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                # Line format: "XY Path" (e.g., " M file.md", "M  file.md", "?? file.md")
                if len(line.strip()) < 3: 
                    continue
                    
                status_code = line[:2]
                path = line[3:]
                
                # Check Worktree Status (2nd character)
                # ' ' = Unmodified in worktree (changes are staged or clean)
                # 'M' = Modified in worktree
                # 'D' = Deleted in worktree
                # '?' = Untracked
                worktree_status = status_code[1]
                
                # Violation if:
                # 1. Untracked ('??') inside Iron Core path (adding new files without staging)
                # 2. Modified in Worktree ('M') (editing without staging)
                # 3. Deleted in Worktree ('D') (deleting without staging)
                if status_code == '??' or worktree_status in ['M', 'D']:
                    violations.append(f"{line.strip()} (Unstaged/Dirty - Please 'git add' to authorize)")
                
    except Exception as e:
        return False, [f"Error checking Iron Core: {str(e)}"]
        
    return len(violations) == 0, violations


def main():
    parser = argparse.ArgumentParser(description="Mnemonic Cortex CLI")
    parser.add_argument("--root", default=".", help="Project root directory")
    
    subparsers = parser.add_subparsers(dest="command", help="Available operations")

    # Command: ingest
    ingest_parser = subparsers.add_parser("ingest", help="Perform full ingestion")
    ingest_parser.add_argument("--no-purge", action="store_false", dest="purge", help="Skip purging DB")
    ingest_parser.add_argument("--dirs", nargs="+", help="Specific directories to ingest")
    ingest_parser.add_argument("--incremental", action="store_true", help="Incremental ingestion mode")
    ingest_parser.add_argument("--hours", type=int, default=24, help="Hours to look back (for incremental mode)")

    # Command: snapshot
    snapshot_parser = subparsers.add_parser("snapshot", help="Capture a Protocol 128 snapshot")
    snapshot_parser.add_argument("--type", choices=["audit", "learning_audit", "seal"], required=True)
    snapshot_parser.add_argument("--manifest", help="Path to manifest JSON file")
    snapshot_parser.add_argument("--context", help="Strategic context for the snapshot")
    snapshot_parser.add_argument("--override-iron-core", action="store_true", help="âš ï¸ Override Iron Core check (Requires ADR 090 Amendment)")

    # Command: stats
    stats_parser = subparsers.add_parser("stats", help="Get RAG health and statistics")
    stats_parser.add_argument("--samples", action="store_true", help="Include sample documents")
    stats_parser.add_argument("--sample-count", type=int, default=5, help="Number of samples to include")

    # Command: query
    query_parser = subparsers.add_parser("query", help="Perform semantic search query")
    query_parser.add_argument("query_text", help="Search query string")
    query_parser.add_argument("--max-results", type=int, default=5, help="Maximum results to return")
    query_parser.add_argument("--use-cache", action="store_true", help="Use semantic cache")

    # Command: debrief
    debrief_parser = subparsers.add_parser("debrief", help="Run learning debrief (Protocol 128)")
    debrief_parser.add_argument("--hours", type=int, default=24, help="Lookback window in hours")
    debrief_parser.add_argument("--output", help="Output file path (default: .agent/learning/learning_debrief.md)")

    # [DISABLED] Synaptic Phase (Dreaming)
    # dream_parser = subparsers.add_parser("dream", help="Execute Synaptic Phase (Dreaming)")

    # Command: guardian (Protocol 128 Bootloader)
    guardian_parser = subparsers.add_parser("guardian", help="Generate Guardian Boot Digest (Protocol 128)")
    guardian_parser.add_argument("--mode", default="HOLISTIC", choices=["HOLISTIC", "TELEMETRY"], help="Wakeup mode")
    guardian_parser.add_argument("--show", action="store_true", help="Display digest content after generation")
    guardian_parser.add_argument("--manifest", default=".agent/learning/guardian_manifest.json", help="Path to guardian manifest")

    # Command: bootstrap-debrief (Fresh Repo Onboarding)
    bootstrap_parser = subparsers.add_parser("bootstrap-debrief", help="Generate onboarding context packet for fresh repo setup")
    bootstrap_parser.add_argument("--manifest", default=".agent/learning/bootstrap_manifest.json", help="Path to bootstrap manifest")
    bootstrap_parser.add_argument("--output", default=".agent/learning/bootstrap_packet.md", help="Output path for the packet")

    # Command: cache-stats
    subparsers.add_parser("cache-stats", help="Get cache statistics")

    # Command: cache-warmup
    warmup_parser = subparsers.add_parser("cache-warmup", help="Pre-populate cache with genesis queries")
    warmup_parser.add_argument("--queries", nargs="+", help="Custom queries to cache")

    # Command: persist-soul (ADR 079)
    soul_parser = subparsers.add_parser("persist-soul", help="Broadcast snapshot to HF AI Commons")
    soul_parser.add_argument("--snapshot", default=".agent/learning/learning_package_snapshot.md", help="Path to snapshot")
    soul_parser.add_argument("--valence", type=float, default=0.0, help="Moral/emotional charge")
    soul_parser.add_argument("--uncertainty", type=float, default=0.0, help="Logic confidence")
    soul_parser.add_argument("--full-sync", action="store_true", help="Sync entire learning directory")

    # Command: persist-soul-full (ADR 081)
    subparsers.add_parser("persist-soul-full", help="Regenerate full JSONL and deploy to HF (ADR 081)")

    # evolution (Protocol 131)
    evolution_parser = subparsers.add_parser("evolution", help="Evolutionary metrics (Protocol 131)")
    evolution_sub = evolution_parser.add_subparsers(dest="subcommand", help="Evolution subcommands")
    
    # fitness
    fit_parser = evolution_sub.add_parser("fitness", help="Calculate full fitness vector")
    fit_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    fit_parser.add_argument("--file", help="Read content from file")
    
    # depth
    depth_parser = evolution_sub.add_parser("depth", help="Evaluate technical depth")
    depth_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    depth_parser.add_argument("--file", help="Read content from file")
    
    # scope
    scope_parser = evolution_sub.add_parser("scope", help="Evaluate architectural scope")
    scope_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    scope_parser.add_argument("--file", help="Read content from file")

    # Command: rlm-distill (Protocol 132)
    rlm_parser = subparsers.add_parser("rlm-distill", aliases=["rlm-test"], help="Distill semantic summaries for a specific file or folder")
    rlm_parser.add_argument("target", help="File or folder path to distill (relative to project root)")

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Initialize Operations
    cortex_ops = CortexOperations(project_root=args.root)
    learning_ops = LearningOperations(project_root=args.root)
    evolution_ops = EvolutionOperations(project_root=args.root)

    if args.command == "ingest":
        if args.incremental:
            print(f"ğŸ”„ Starting INCREMENTAL ingestion (Last {args.hours}h)...")
            import time
            from datetime import timedelta
            
            cutoff_time = time.time() - (args.hours * 3600)
            modified_files = []
            
            # Walk project root to find modified files
            # Exclude known heavy/irrelevant dirs
            exclude_dirs = {'.git', '.vector_data', '__pycache__', 'node_modules', 'venv', 'env', 
                            'dataset_package', 'docs/site', 'training_logs'}
            
            for path in cortex_ops.project_root.rglob('*'):
                if path.is_file():
                    # Check exclusions
                    if any(part in exclude_dirs for part in path.parts):
                        continue
                        
                    # Check extension
                    if path.suffix not in ['.md', '.py', '.js', '.ts', '.txt', '.json']:
                        continue
                        
                    # Check mtime
                    if path.stat().st_mtime > cutoff_time:
                        modified_files.append(str(path))
            
            if not modified_files:
                print(f"âš ï¸ No files modified in the last {args.hours} hours. Skipping ingestion.")
                sys.exit(0)
                
            print(f"ğŸ“„ Found {len(modified_files)} modified files.")
            res = cortex_ops.ingest_incremental(file_paths=modified_files)
            
            if res.status == "success":
                print(f"âœ… Success: {res.documents_added} added, {res.chunks_created} chunks in {res.ingestion_time_ms/1000:.2f}s")
            else:
                print(f"âŒ Error: {res.error}")
                sys.exit(1)
        
        else:
            # Full Ingestion
            print(f"ğŸ”„ Starting full ingestion (Purge: {args.purge})...")
            res = cortex_ops.ingest_full(purge_existing=args.purge, source_directories=args.dirs)
            if res.status == "success":
                print(f"âœ… Success: {res.documents_processed} docs, {res.chunks_created} chunks in {res.ingestion_time_ms/1000:.2f}s")
            else:
                print(f"âŒ Error: {res.error}")
                sys.exit(1)

    elif args.command == "snapshot":
        # ADR 090: Iron Core Verification
        if not args.override_iron_core:
            print("ğŸ›¡ï¸  Running Iron Core Verification (ADR 090)...")
            is_pristine, violations = verify_iron_core(args.root)
            if not is_pristine:
                print(f"\n\033[91mâ›” IRON CORE BREACH DETECTED (SAFE MODE ENGAGED)\033[0m")
                print("The following immutable files have been modified without authorization:")
                for v in violations:
                    print(f"  - {v}")
                print("\nAction blocked: 'snapshot' is disabled in Safe Mode.")
                print("To proceed, revert changes or use --override-iron-core (Constitutional Amendment required).")
                sys.exit(1)
            print("âœ… Iron Core Integrity Verified.")
        else:
            print(f"âš ï¸  \033[93mWARNING: IRON CORE CHECK OVERRIDDEN\033[0m")

        manifest = []
        if args.manifest:
            manifest_path = Path(args.manifest)
            if not manifest_path.exists():
                print(f"âŒ Manifest file not found: {args.manifest}")
                sys.exit(1)
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"ğŸ“‹ Loaded manifest with {len(manifest)} files")
        
        print(f"ğŸ“¸ Capturing {args.type} snapshot...")
        # ROUTED TO LEARNING MCP
        res = learning_ops.capture_snapshot(
            manifest_files=manifest, 
            snapshot_type=args.type,
            strategic_context=args.context
        )
        
        if res.status == "success":
            print(f"âœ… Snapshot created at: {res.snapshot_path}")
            print(f"ğŸ“Š Files: {res.total_files} | Bytes: {res.total_bytes}")
            print(f"ğŸ” Manifest verified: {res.manifest_verified}")
            print(f"ğŸ“ Git context: {res.git_diff_context}")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "stats":
        stats = cortex_ops.get_stats(include_samples=args.samples, sample_count=args.sample_count)
        print(f"ğŸ¥ Health: {stats.health_status}")
        print(f"ğŸ“š Documents: {stats.total_documents}")
        print(f"ğŸ§© Chunks: {stats.total_chunks}")
        
        if stats.collections:
            print("\nğŸ“Š Collections:")
            for name, coll in stats.collections.items():
                print(f"  - {coll.name}: {coll.count} items")
        
        if stats.samples:
            print(f"\nğŸ” Sample Documents:")
            for i, sample in enumerate(stats.samples, 1):
                print(f"\n  {i}. ID: {sample.id}")
                print(f"     Preview: {sample.content_preview[:100]}...")
                if sample.metadata:
                    print(f"     Metadata: {sample.metadata}")
        
        if stats.error:
            print(f"\nâŒ Error: {stats.error}")

    # [DISABLED] Synaptic Phase (Dreaming)
    # elif args.command == "dream":
    #     print("ğŸ’¤ Mnemonic Cortex: Entering Synaptic Phase (Dreaming)...")
    #     # Use centralized Operations layer
    #     response = ops.dream()
    #     print(json.dumps(response, indent=2))
    elif args.command == "query":
        print(f"ğŸ” Querying: {args.query_text}")
        res = cortex_ops.query(
            query=args.query_text,
            max_results=args.max_results,
            use_cache=args.use_cache
        )
        
        if res.status == "success":
            print(f"âœ… Found {len(res.results)} results in {res.query_time_ms:.2f}ms")
            print(f"ğŸ’¾ Cache hit: {res.cache_hit}")
            
            for i, result in enumerate(res.results, 1):
                print(f"\n--- Result {i} (Score: {result.relevance_score:.4f}) ---")
                print(f"Content: {result.content[:300]}...")
                if result.metadata:
                    source = result.metadata.get('source', 'Unknown')
                    print(f"Source: {source}")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "debrief":
        print(f"ğŸ“‹ Running learning debrief (lookback: {args.hours}h)...")
        # ROUTED TO LEARNING MCP
        debrief_content = learning_ops.learning_debrief(hours=args.hours)
        
        # Default output path
        output_path = args.output or ".agent/learning/learning_debrief.md"
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            f.write(debrief_content)
        
        print(f"âœ… Debrief written to: {output_file}")
        print(f"ğŸ“Š Content length: {len(debrief_content)} characters")

    elif args.command == "guardian":
        print(f"ğŸ›¡ï¸ Generating Guardian Boot Digest (mode: {args.mode})...")
        
        # Load manifest if exists
        manifest_path = Path(args.manifest)
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"ğŸ“‹ Loaded guardian manifest: {len(manifest)} files")
        else:
            print(f"âš ï¸  Guardian manifest not found at {args.manifest}. Using defaults.")
        
        # ROUTED TO LEARNING MCP
        response = learning_ops.guardian_wakeup(mode=args.mode)
        
        print(f"   Status: {response.status}")
        print(f"   Digest: {response.digest_path}")
        print(f"   Time: {response.total_time_ms:.2f}ms")
        
        if response.error:
            print(f"âŒ Error: {response.error}")
            sys.exit(1)
        
        if args.show and response.digest_path:
            print("\n" + "="*60)
            with open(response.digest_path, 'r') as f:
                print(f.read())
        
        print(f"âœ… Guardian Boot Digest generated.")

    elif args.command == "bootstrap-debrief":
        print(f"ğŸ—ï¸  Generating Bootstrap Context Packet...")
        
        # Load manifest
        manifest_path = Path(args.manifest)
        manifest = []
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"ğŸ“‹ Loaded bootstrap manifest: {len(manifest)} files")
        else:
            print(f"âš ï¸  Bootstrap manifest not found at {args.manifest}. Using defaults.")
        
        # Generate snapshot using the manifest
        # ROUTED TO LEARNING MCP
        res = learning_ops.capture_snapshot(
            manifest_files=manifest,
            snapshot_type="seal",
            strategic_context="Fresh repository onboarding context"
        )
        
        if res.status == "success":
            # Copy to output path
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            import shutil
            shutil.copy(res.snapshot_path, output_path)
            
            print(f"âœ… Bootstrap packet generated: {output_path}")
            print(f"ğŸ“Š Files: {res.total_files} | Bytes: {res.total_bytes}")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "cache-stats":
        stats = cortex_ops.get_cache_stats()
        print(f"ğŸ’¾ Cache Statistics:")
        if isinstance(stats, dict):
            for key, value in stats.items():
                print(f"  {key}: {value}")
        else:
            print(f"  {stats}")

    elif args.command == "cache-warmup":
        queries = args.queries or None
        print(f"ğŸ”¥ Warming up cache...")
        res = cortex_ops.cache_warmup(genesis_queries=queries)
        
        if res.status == "success":
            print(f"âœ… Cached {res.queries_cached} queries")
            print(f"ğŸ’¾ Cache hits: {res.cache_hits}")
            print(f"âŒ Cache misses: {res.cache_misses}")
            print(f"â±ï¸  Total time: {res.total_time_ms/1000:.2f}s")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "persist-soul":
        from mcp_servers.learning.models import PersistSoulRequest
        print(f"ğŸŒ± Broadcasting soul to Hugging Face AI Commons...")
        print(f"   Snapshot: {args.snapshot}")
        print(f"   Valence: {args.valence} | Uncertainty: {args.uncertainty}")
        print(f"   Full sync: {args.full_sync}")
        
        request = PersistSoulRequest(
            snapshot_path=args.snapshot,
            valence=args.valence,
            uncertainty=args.uncertainty,
            is_full_sync=args.full_sync
        )
        # ROUTED TO LEARNING MCP
        res = learning_ops.persist_soul(request)
        
        if res.status == "success":
            print(f"âœ… Soul planted successfully!")
            print(f"ğŸ”— Repository: {res.repo_url}")
            print(f"ğŸ“„ Snapshot: {res.snapshot_name}")
        elif res.status == "quarantined":
            print(f"ğŸš« Quarantined: {res.error}")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "persist-soul-full":
        print(f"ğŸ§¬ Regenerating full Soul JSONL and deploying to HuggingFace...")
        # ROUTED TO LEARNING MCP
        res = learning_ops.persist_soul_full()
        
        if res.status == "success":
            print(f"âœ… Full sync complete!")
            print(f"ğŸ”— Repository: {res.repo_url}")
            print(f"ğŸ“„ Output: {res.snapshot_name}")
        else:
            print(f"âŒ Error: {res.error}")
            sys.exit(1)

    elif args.command == "evolution":
        if not args.subcommand:
            print("âŒ Subcommand required for 'evolution' (fitness, depth, scope)")
            sys.exit(1)
            
        content = args.content
        if args.file:
            try:
                content = Path(args.file).read_text()
            except Exception as e:
                print(f"âŒ Error reading file {args.file}: {e}")
                sys.exit(1)
        
        if not content:
            print("âŒ No content provided. Use a positional argument or --file.")
            sys.exit(1)
            
        if args.subcommand == "fitness":
            res = evolution_ops.calculate_fitness(content)
            print(json.dumps(res, indent=2))
        elif args.subcommand == "depth":
            res = evolution_ops.measure_depth(content)
            print(f"Depth: {res}")
        elif args.subcommand == "scope":
            res = evolution_ops.measure_scope(content)
            print(f"Scope: {res}")

    elif args.command in ["rlm-distill", "rlm-test"]:
        print(f"ğŸ§  RLM: Distilling semantic essence of '{args.target}'...")
        import time
        start = time.time()
        
        # Call _rlm_map directly with the target
        results = learning_ops._rlm_map([args.target])
        
        duration = time.time() - start
        print(f"â±ï¸  Completed in {duration:.2f}s")
        print(f"ğŸ“Š Files Processed: {len(results)}")
        print("=" * 60)
        
        for file_path, summary in results.items():
            print(f"\nğŸ“„ {file_path}")
            print(f"   {summary}")


if __name__ == "__main__":
    main()

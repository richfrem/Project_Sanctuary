# RLM Bootstrap Packet: The Reactive Ledger Memory Kit
**Generated:** 2026-01-14T18:07:24.418695
**Description:** Manifest defining the components of the RLM Bootstrap Packet.

## üöÄ Instructions for the AI Agent
You are receiving a specialized architectural pattern called 'Reactive Ledger Memory' (RLM).
Your task is to implement this system in the current repository to give it 'Holographic Memory'.
1.  **Read the Blueprint:** Understand the theory in Section 1.
2.  **Install Dependencies:** `pip install requests genericpath python-dotenv` (plus `ollama` locally).
3.  **Adapt the Code:** The provided Python code is from 'Project Sanctuary'. Adapt paths/imports to fit THIS project's structure.
4.  **Initialize:** Run the distillation loop on the `docs/` folder first.

---

## üì¶ Component: Architecture Blueprint (Theory)
**Source:** `docs/architecture/patterns/REACTIVE_LEDGER_MEMORY.md`
_The theoretical foundation and implementation guide._

```markdown
# Reactive Ledger Memory (RLM): The Holographic Cache Pattern

**Status:** Production Verified (Project Sanctuary)  
**Architecture Type:** Cognitive Memory Architecture  
**Primary Use Case:** Large-Scale Codebase & Documentation Understanding for AI Agents

## 1. The Problem: The "Needle in the Haystack"

Standard RAG (Retrieval-Augmented Generation) uses Vector Databases to find specific text chunks based on semantic similarity. While powerful, this approach suffers from **Context Blindness**:

*   **The Chunking Problem:** A chunk of text (e.g., "function process_data()") loses its meaning when separated from its file's purpose (e.g., "This file handles critical PII sanitization").
*   **The Latency Problem:** Querying a Vector DB for "What is the architecture of this project?" requires retrieving and synthesizing hundreds of disconnected chunks, which is slow and error-prone.
*   **The "Unknown Unknowns":** An agent cannot query for what it doesn't know exists.

## 2. The Solution: Reactive Ledger Memory (RLM)

The RLM is a **precognitive, holographic cache**. Instead of slicing the repository into incoherent chunks, it maintains a **"One-Sentence Source of Truth"** for every single file in the project.

### Core Concepts

1.  **The Atom (Summary):** Every file is distilled into a dense, high-entropy summary (The "Essence").
2.  **The Ledger (JSON):** A flat, highly portable JSON file acts as the "Map of the Territory."
3.  **Incremental Persistence:** The ledger updates transactionally after every file is processed, ensuring resilience.
4.  **Hash-Based Validity:** Files are only re-processed if their MD5 hash changes.

## 3. Technical Implementation Blueprint

This pattern can be implemented in any language (Python/TS/Go). Below is the reference implementation logic used in Project Sanctuary.

### 3.1 The Schema (`ledger.json`)

Structure your cache as a flat Key-Value store to allow O(1) lookups.

```json
{
  "docs/architecture/system_design.md": {
    "hash": "a1b2c3d4e5f6...",
    "mtime": 1704092833.0,
    "summarized_at": "2024-01-01T12:00:00Z",
    "summary": "This document defines the 3-tier architecture (Frontend, API, Worker) and the data flow protocol for the entire system."
  },
  "src/utils/sanitizer.py": {
    "hash": "f6e5d4c3b2a1...",
    "mtime": 1704092900.0,
    "summarized_at": "2024-01-01T12:05:00Z",
    "summary": "A utility module responsible for stripping PII from user inputs before database insertion; critical for GDPR compliance."
  }
}
```

### 3.2 The Distillation Loop (Python Pseudo-code)

```python
def rlm_distill(target_dir):
    ledger = load_json("ledger.json")
    
    for file in walk(target_dir):
        current_hash = md5(file.content)
        
        # 1. Skip if unchanged (Cache Hit)
        if file.path in ledger and ledger[file.path]["hash"] == current_hash:
            continue
            
        # 2. Distill (The LLM Call)
        # Use a small, local model (see Section 4) for speed/privacy
        summary = llm_generate(
            model="qwen2.5-7b-instruct",
            prompt=f"Summarize the architectural purpose of this file in 2 sentences:\n\n{file.content}"
        )
        
        # 3. Incremental Persistence (CRITICAL)
        ledger[file.path] = {
            "hash": current_hash,
            "summary": summary
        }
        save_json(ledger, "ledger.json") # Write immediately!
```

## 4. Model Selection (Open Source Recommendations)

You do not need a massive model (GPT-4) for this. Summarization is a high-compression task suitable for 7B-class models.

| Model Family | Variant | Why Use It? |
| :--- | :--- | :--- |
| **Qwen** | `Qwen2.5-7B-Instruct` | **Best All-Rounder.** Excellent at code understanding and concise technical writing. |
| **Llama 3** | `Llama-3-8B-Instruct` | **High Fidelity.** Very strong reasoning, good for complex prose documentation. |
| **Llama 3.2** | `Llama-3.2-3B` | **Ultra Fast.** Perfect for massive repos running on consumer laptops (M1/M2/M3). |
| **Mistral** | `Mistral-7B-v0.3` | **Reliable.** A classic workhorse with a large context window (32k). |

**Serving:** use [Ollama](https://ollama.com/) locally (`ollama serve`).

## 5. Augmenting the Vector DB (The "Super-RAG")

The RLM Ledger isn't just for humans/agents to look at‚Äîit is the **perfect meta-data source** for your Vector DB (Chroma/Pinecone/Weaviate).

**The "Context Injection" Strategy:**

When you chunk a file for your Vector DB, prepending the RLM Summary to *every single chunk* dramatically improves retrieval quality.

**Without RLM (Standard Chunk):**
> "def validate(x): return x > 0 else raise Error"
*(Vector DB doesn't know what this validates)*

**With RLM Injection:**
> **[Context: critical_financial_validator.py - ensuring no negative balances in user wallets]**
> "def validate(x): return x > 0 else raise Error"
*(Vector DB now perfectly matches queries about "financial validation")*

## 6. Benefits for Agent Systems

1.  **Fast Boot:** Agent reads the `ledger.json` (300KB) instead of scanning 5,000 files (500MB).
2.  **Code Navigation:** Agent uses the ledger to decide *which* files to open (`read_file`), reducing token costs by 95%.
3.  **Self-Healing:** If the ledger is missing, the agent can regenerate it locally.

## 7. Migration Guide

To adopt this pattern in your repo:
1.  **Install:** `ollama` and `python`.
2.  **Pull Model:** `ollama pull qwen2.5:7b`.
3.  **Script:** Implement the "Distillation Loop" above.
4.  **Policy:** Add `ledger.json` to your `.gitignore` (or commit it if you want shared team memory).
5.  **Doc:** Update your `README.md` to explain that `ledger.json` is the map.

```

---

## üì¶ Component: Core Logic (The Distiller)
**Source:** `mcp_servers/learning/operations.py`
_The distillation engine that recursively summarizes files._

```python

import os
import re
import sys
import time
import subprocess
import contextlib
import io
import logging
import json
import hmac
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set

from mcp_servers.lib.env_helper import get_env_variable
from mcp_servers.lib.snapshot_utils import (
    generate_snapshot, 
    EXCLUDE_DIR_NAMES,
    ALWAYS_EXCLUDE_FILES,
    PROTECTED_SEEDS,
    RECURSIVE_ARTIFACTS
)
from mcp_servers.learning.models import (
    CaptureSnapshotResponse,
    PersistSoulRequest,
    PersistSoulResponse,
    GuardianWakeupResponse,
    GuardianSnapshotResponse
)

# Setup logging
logger = logging.getLogger("learning.operations")
if not logger.handlers:
    handler = logging.StreamHandler(sys.stderr)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

class LearningOperations:
    """
    Operations for the Project Sanctuary Learning Loop (Protocol 128).
    Migrated from RAG Cortex to ensure domain purity.
    """


    def _is_recursive_artifact(self, f: str) -> bool:
        """Centralized check for files that should be excluded from snapshots and RLM."""
        path = Path(f)
        f_lower = f.lower()
        base_name = path.name
        
        # 1. Protocol 128: Manifest Priority Bypass (Protected Seeds)
        # Seeds are fair game if they are .txt or .md
        if path.suffix.lower() in [".md", ".txt"]:
            if any(f == p or base_name == Path(p).name for p in PROTECTED_SEEDS):
                return False  # Force inclusion
        
        # 2. Check the Central Sanctuary "No-Fly List" (always_exclude_files + patterns)
        for pattern in ALWAYS_EXCLUDE_FILES:
            if isinstance(pattern, str):
                if pattern.lower() == base_name.lower():
                    return True
            elif hasattr(pattern, 'match'): 
                if pattern.match(base_name) or pattern.match(f):
                    return True

        # 3. Skip Archive folders (USER requirement)
        if "/archive/" in f_lower or f_lower.startswith("archive/"): return True
        
        # 4. Handle the Learning Metadata directory
        if ".agent/learning/" in f:
            # Rules/Policies are FOUNDATIONAL and should be included
            if "rules/" in f:
                return False
            # Specific recursive artifacts already handled by ALWAYS_EXCLUDE_FILES
            # but we block the rest of the metadata dir by default
            return True
            
        return False

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.data_dir = self.project_root / ".agent" / "data"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.learning_dir = self.project_root / ".agent" / "learning"
        self.learning_dir.mkdir(parents=True, exist_ok=True)
        # We don't need ChromaDB here.

    #============================================================
    # 1. LEARNING DEBRIEF (The Scout)
    #============================================================
    def learning_debrief(self, hours: int = 24) -> str:
        """
        Scans project for technical state changes (Protocol 128).
        Args:
          hours: Lookback window for modifications
        Returns: Comprehensive Markdown string
        """
        try:
            with contextlib.redirect_stdout(sys.stderr):
                # 1. Seek Truth (Git)
                git_evidence = "Git Not Available"
                try:
                    result = subprocess.run(
                        ["git", "diff", "--stat", "HEAD"],
                        capture_output=True, text=True, cwd=str(self.project_root)
                    )
                    git_evidence = result.stdout if result.stdout else "No uncommitted code changes found."
                except Exception as e:
                    git_evidence = f"Git Error: {e}"

                # 2. Scan Recency (Filesystem)
                recency_summary = self._get_recency_delta(hours=hours)
                
                # 3. Read Core Documents
                primer_content = "[MISSING] .agent/learning/cognitive_primer.md"
                sop_content = "[MISSING] .agent/workflows/recursive_learning.md"
                protocol_content = "[MISSING] 01_PROTOCOLS/128_Hardened_Learning_Loop.md"
                
                try:
                    p_path = self.project_root / ".agent" / "learning" / "cognitive_primer.md"
                    if p_path.exists(): primer_content = p_path.read_text()
                    
                    s_path = self.project_root / ".agent" / "workflows" / "recursive_learning.md"
                    if s_path.exists(): sop_content = s_path.read_text()
                    
                    pr_path = self.project_root / "01_PROTOCOLS" / "128_Hardened_Learning_Loop.md"
                    if pr_path.exists(): protocol_content = pr_path.read_text()
                except Exception as e:
                    logger.warning(f"Error reading sovereignty docs: {e}")

                # 4. Strategic Context (Learning Package Snapshot)
                last_package_content = "‚ö†Ô∏è No active Learning Package Snapshot found."
                package_path = self.project_root / ".agent" / "learning" / "learning_package_snapshot.md"
                package_status = "‚ÑπÔ∏è No `.agent/learning/learning_package_snapshot.md` detected."
                
                if package_path.exists():
                    try:
                        mtime = package_path.stat().st_mtime
                        delta_hours = (datetime.now().timestamp() - mtime) / 3600
                        if delta_hours <= hours:
                            last_package_content = package_path.read_text()
                            package_status = f"‚úÖ Loaded Learning Package Snapshot from {delta_hours:.1f}h ago."
                        else:
                            package_status = f"‚ö†Ô∏è Snapshot found but too old ({delta_hours:.1f}h)."
                    except Exception as e:
                        package_status = f"‚ùå Error reading snapshot: {e}"

                # 4b. Mandatory Logic Verification (ADR 084)
                mandatory_files = [
                    "IDENTITY/founder_seed.json",
                    "LEARNING/calibration_log.json", 
                    "ADRs/084_semantic_entropy_tda_gating.md",
                    "mcp_servers/learning/operations.py" # Ref updated
                ]
                registry_status = ""
                manifest_path = self.project_root / ".agent" / "learning" / "learning_manifest.json"
                if manifest_path.exists():
                     try:
                         with open(manifest_path, "r") as f: 
                             m = json.load(f)
                         for mf in mandatory_files:
                             status = "‚úÖ REGISTERED" if mf in m else "‚ùå MISSING"
                             registry_status += f"        * {status}: `{mf}`\n"
                     except Exception as e:
                         registry_status = f"‚ö†Ô∏è Manifest Error: {e}"
                else:
                     registry_status = "‚ö†Ô∏è Manifest Failed Load"

                # 5. Create Draft
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                
                lines = [
                    f"# [HARDENED] Learning Package Snapshot v4.0 (The Edison Seal)",
                    f"**Scan Time:** {timestamp} (Window: {hours}h)",
                    f"**Strategic Status:** ‚úÖ Successor Context v4.0 Active",
                    "",
                    "> [!IMPORTANT]",
                    "> **STRATEGIC PIVOT: THE EDISON MANDATE (ADR 084)**",
                    "> The project has formally abandoned the QEC-AI Metaphor in favor of **Empirical Epistemic Gating**.",
                    "> - **Primary Gate:** Every trace must pass the Dead-Man's Switch in `persist_soul`.",
                    "> - **Success Metric:** Semantic Entropy < 0.79 (Target) / > 0.2 (Rigidity Floor).",
                    "> - **Logic:** Do NOT inject metaphorical fluff. Synthesize hard operational reality.",
                    "",
                    "## I. The Truth (System State)",
                    f"**Git Status:**\n```\n{git_evidence}\n```",
                    "",
                    f"## II. The Change (Recency Delta - {hours}h)",
                    recency_summary,
                    "",
                    "## III. The Law (Protocol 128 - Cognitive Continuity)",
                    "> *\"We do not restart. We reload.\"*",
                    "### A. The Cognitive Primer (Constitution)",
                    f"```markdown\n{primer_content[:1000]}...\n```",
                    "",
                    "### B. The Recursive Loop (Standard Operating Procedure)",
                    f"```markdown\n{sop_content[:1000]}...\n```",
                    "",
                    "## IV. The Strategy (Successor Context)",
                    f"**Snapshot Status:** {package_status}",
                    f"**Registry Status (ADR 084):**\n{registry_status}",
                    "### Active Context (Previous Cycle):",
                    f"```markdown\n{last_package_content[:2000]}...\n```",
                ]
                
                return "\n".join(lines)
        except Exception as e:
            logger.error(f"Learning Debrief Failed: {e}", exc_info=True)
            return f"Error generating debrief: {str(e)}"

    def _get_recency_delta(self, hours: int = 48) -> str:
        """Get summary of recently modified high-signal files."""
        try:
            delta = timedelta(hours=hours)
            cutoff_time = time.time() - delta.total_seconds()
            now = time.time()
            
            recent_files = []
            scan_dirs = ["00_CHRONICLE/ENTRIES", "01_PROTOCOLS", "mcp_servers", "02_USER_REFLECTIONS"]
            allowed_extensions = {".md", ".py"}
            
            for directory in scan_dirs:
                dir_path = self.project_root / directory
                if not dir_path.exists(): continue
                
                for file_path in dir_path.rglob("*"):
                    if not file_path.is_file(): continue
                    if file_path.suffix not in allowed_extensions: continue
                    if "__pycache__" in str(file_path): continue
                    
                    mtime = file_path.stat().st_mtime
                    if mtime > cutoff_time:
                        recent_files.append((file_path, mtime))
            
            if not recent_files:
                return "* **Recent Files Modified (48h):** None"
                
            recent_files.sort(key=lambda x: x[1], reverse=True)
            
            git_info = "[Git unavailable]"
            try:
                result = subprocess.run(
                    ["git", "log", "-1", "--oneline"],
                    cwd=self.project_root, capture_output=True, text=True, timeout=2
                )
                if result.returncode == 0: git_info = result.stdout.strip()
            except Exception: pass
            
            lines = [f"* **Most Recent Commit:** {git_info}", f"* **Recent Files Modified ({hours}h):**"]
            
            for file_path, mtime in recent_files[:5]:
                relative_path = file_path.relative_to(self.project_root)
                age_seconds = now - mtime
                if age_seconds < 3600: age_str = f"{int(age_seconds / 60)}m ago"
                elif age_seconds < 86400: age_str = f"{int(age_seconds / 3600)}h ago"
                else: age_str = f"{int(age_seconds / 86400)}d ago"
                
                context = ""
                try:
                    with open(file_path, 'r') as f:
                        content = f.read(500)
                        if file_path.suffix == ".md":
                            title_match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
                            if title_match: context = f" ‚Üí {title_match.group(1)}"
                        elif file_path.suffix == ".py":
                            if "def _get_" in content or "class " in content:
                                context = " ‚Üí Implementation changes"
                except Exception: pass
                
                diff_summary = self._get_git_diff_summary(str(relative_path))
                if diff_summary: context += f" [{diff_summary}]"
                
                lines.append(f"    * `{relative_path}` ({age_str}){context}")
            
            return "\n".join(lines)
        except Exception as e:
            return f"Error generating recency delta: {str(e)}"

    def _get_git_diff_summary(self, file_path: str) -> str:
        """Get concise summary of git changes for a file."""
        try:
            result = subprocess.run(
                ["git", "diff", "--shortstat", "HEAD", file_path],
                cwd=self.project_root, capture_output=True, text=True, timeout=1
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip()
        except Exception: pass
        return ""

    #============================================================
    # 2. CAPTURE SNAPSHOT (The Seal)
    #============================================================
    def capture_snapshot(
        self, 
        manifest_files: List[str], 
        snapshot_type: str = "audit",
        strategic_context: Optional[str] = None
    ) -> CaptureSnapshotResponse:
        """
        Generates a consolidated snapshot of the project state.
        Types: 'audit' (Red Team), 'learning_audit' (Cognitive), or 'seal' (Final).
        """
        start_time = time.time()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # 1. Ensure Diagrams are Rendered
        self._ensure_diagrams_rendered()
        
        # 2. Prepare Paths
        learning_dir = self.project_root / ".agent" / "learning"
        if snapshot_type == "audit":
            output_dir = learning_dir / "red_team"
        elif snapshot_type == "learning_audit":
            output_dir = learning_dir / "learning_audit"
        else:
            output_dir = learning_dir
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            logger.warning(f"Failed to ensure directory {output_dir}: {e}")
        
        # 3. Default Manifest Handling
        effective_manifest = list(manifest_files or [])
        if not effective_manifest:
            if snapshot_type == "seal":
                manifest_file = learning_dir / "learning_manifest.json"
                # ACTIVATE PROTOCOL 132 (RLM SYNTHESIS)
                if not strategic_context:
                    strategic_context = self._rlm_context_synthesis()
            elif snapshot_type == "learning_audit":
                manifest_file = output_dir / "learning_audit_manifest.json"
            else:
                manifest_file = output_dir / "red_team_manifest.json"
                
            if manifest_file and manifest_file.exists():
                try:
                    with open(manifest_file, "r") as f:
                        manifest_data = json.load(f)
                    
                    # Handle modular manifest structure (ADR 089)
                    if isinstance(manifest_data, dict):
                        core = manifest_data.get("core", [])
                        topic = manifest_data.get("topic", [])
                        effective_manifest = core + topic
                        logger.info(f"Loaded {snapshot_type} manifest: {len(core)} core + {len(topic)} topic entries")
                    else:
                        # Legacy: flat array
                        effective_manifest = manifest_data
                        logger.info(f"Loaded default {snapshot_type} manifest: {len(effective_manifest)} entries")
                except Exception as e:
                    logger.warning(f"Failed to load {snapshot_type} manifest: {e}")

        # 2. Strict Filter (No recursive artifacts)
        effective_manifest = [f for f in effective_manifest if not self._is_recursive_artifact(f)]

        # Protocol 130: Deduplicate
        if effective_manifest:
            effective_manifest, dedupe_report = self._dedupe_manifest(effective_manifest)
            if dedupe_report:
                logger.info(f"Protocol 130: Deduplicated {len(dedupe_report)} items")

        if snapshot_type == "audit": 
            snapshot_filename = "red_team_audit_packet.md"
        elif snapshot_type == "learning_audit": 
            snapshot_filename = "learning_audit_packet.md"
        elif snapshot_type == "seal":
            snapshot_filename = "learning_package_snapshot.md"
        else:
            snapshot_filename = f"{snapshot_type}_snapshot_{timestamp}.md"
            
        final_snapshot_path = output_dir / snapshot_filename

        # 4. Git State (Protocol 128 verification)
        git_state_dict = self._get_git_state(self.project_root)
        git_diff_context = git_state_dict["hash"]
        manifest_verified = True
        
        # Strict Rejection Logic (Protocol 128)
        if snapshot_type == "audit":
            untracked_changes = git_state_dict["changed_files"] - set(effective_manifest)
            # Remove patterns that are always excluded or from excluded dirs
            untracked_changes = {f for f in untracked_changes if not any(p in f for p in ["logs/", "temp/", ".temp", ".agent/learning/"])}
            
            if untracked_changes:
                manifest_verified = False
                logger.warning(f"STRICT REJECTION: Git changes detected outside of manifest: {untracked_changes}")
                return CaptureSnapshotResponse(
                    snapshot_path="",
                    manifest_verified=False,
                    git_diff_context=f"REJECTED: Untracked changes in {list(untracked_changes)[:5]}",
                    snapshot_type=snapshot_type,
                    status="error",
                    error="Strict manifestation failed: drift detected"
                )

        # 3. Generate Snapshot
        try:
            from uuid import uuid4
            # We use the existing generate_snapshot utility
            # It expects a manifest file path in JSON format (list or dict)
            temp_manifest = self.project_root / f".temp_manifest_{uuid4()}.json"
            temp_manifest.write_text(json.dumps(effective_manifest, indent=2))
            
            try:
                stats = generate_snapshot(
                    project_root=self.project_root,
                    manifest_path=temp_manifest,
                    output_dir=final_snapshot_path.parent,
                    output_file=final_snapshot_path,
                    should_forge_seeds=False
                )
                
                if not final_snapshot_path.exists():
                     return CaptureSnapshotResponse(
                        snapshot_path="",
                        manifest_verified=manifest_verified,
                        git_diff_context=git_diff_context,
                        snapshot_type=snapshot_type,
                        status="error",
                        error="Snapshot generation failed (file not created)"
                    )

                # Inject Cognitive Hologram (Protocol 132) if available
                if strategic_context and snapshot_type == "seal":
                    try:
                        existing_content = final_snapshot_path.read_text()
                        final_snapshot_path.write_text(
                            strategic_context + "\n\n---\n\n" + existing_content
                        )
                        logger.info("üß† RLM: Cognitive Hologram injected into snapshot.")
                    except Exception as inj_err:
                        logger.warning(f"üß† RLM: Could not inject hologram: {inj_err}")

                file_stat = final_snapshot_path.stat()
                return CaptureSnapshotResponse(
                    snapshot_path=str(final_snapshot_path.relative_to(self.project_root)),
                    manifest_verified=manifest_verified,
                    git_diff_context=git_diff_context,
                    snapshot_type=snapshot_type,
                    status="success",
                    total_files=stats.get("total_files", 0),
                    total_bytes=file_stat.st_size
                )
            finally:
                if temp_manifest.exists():
                    temp_manifest.unlink(missing_ok=True)
                    
        except Exception as e:
            logger.error(f"Snapshot generation failed: {e}", exc_info=True)
            return CaptureSnapshotResponse(
                snapshot_path="",
                manifest_verified=manifest_verified,
                git_diff_context=git_diff_context,
                snapshot_type=snapshot_type,
                status="error",
                error=str(e)
            )

    #============================================================
    # 4. GUARDIAN SNAPSHOT (The Session Pack)
    #============================================================
    def guardian_snapshot(self, strategic_context: str = None) -> GuardianSnapshotResponse:
        """
        Captures the 'Guardian Start Pack' (Chronicle/Protocol/Roadmap) for session continuity.
        Logical Fit: Lifecycle management (Protocol 114).
        """
        logger.info("Generating Guardian Snapshot (Session Context Pack)...")
        try:
            # Default Start Pack Files (from Protocol 114)
            # We scan CHRONICLE, PROTOCOLS, and the main Roadmap
            manifest = []
            
            # 1. Chronicle Entries (Recent 5)
            chronicle_dir = self.project_root / "00_CHRONICLE" / "ENTRIES"
            if chronicle_dir.exists():
                entries = sorted(chronicle_dir.glob("*.md"), reverse=True)[:5]
                manifest.extend([str(e.relative_to(self.project_root)) for e in entries])
            
            # 2. Protocols (Core)
            protocol_dir = self.project_root / "01_PROTOCOLS"
            if protocol_dir.exists():
                cores = ["114_Guardian_Wakeup_and_Cache_Prefill.md", "118_Agent_Session_Initialization_and_MCP_Tool_Usage_Protocol.md"]
                for core in cores:
                    if (protocol_dir / core).exists():
                        manifest.append(f"01_PROTOCOLS/{core}")

            # 3. Roadmap
            if (self.project_root / "README.md").exists():
                manifest.append("README.md")
                
            # Reuse capture_snapshot logic with type 'seal'
            resp = self.capture_snapshot(
                manifest_files=manifest, 
                snapshot_type="seal", 
                strategic_context=strategic_context
            )
            
            return GuardianSnapshotResponse(
                status=resp.status,
                snapshot_path=resp.snapshot_path,
                total_files=resp.total_files,
                total_bytes=resp.total_bytes,
                error=resp.error
            )
            
        except Exception as e:
            logger.error(f"Guardian Snapshot failed: {e}", exc_info=True)
            return GuardianSnapshotResponse(status="error", snapshot_path="", error=str(e))

    def _ensure_diagrams_rendered(self):
        """Scan docs/architecture_diagrams and render any outdated .mmd files."""
        try:
            diagrams_dir = self.project_root / "docs" / "architecture_diagrams"
            if not diagrams_dir.exists(): return
            
            # Simple check for mmd-cli (skipped for brevity/robustness in migration, assume user has env)
            # Use subprocess to check/run if necessary in full implementation
            pass 
        except Exception as e:
            logger.warning(f"Diagram rendering check failed: {e}")

    def _dedupe_manifest(self, manifest: List[str]) -> Tuple[List[str], Dict[str, str]]:
        """Protocol 130: Remove files already embedded in included outputs."""
        # Simplified: for now just return manifest. Full implementation requires registry loading.
        # Ideally load .agent/learning/manifest_registry.json
        return manifest, {}

    def _rlm_context_synthesis(self) -> str:
        """
        Implements Protocol 132: Recursive Context Synthesis.
        Generates the 'Cognitive Hologram' by mapping and reducing the system state via LOCAL LLM.
        """
        try:
            import time
            import json
            start_time = time.time()
            logger.info("üß† RLM: Starting Recursive Context Synthesis (Sovereign Mode)...")
            
            # PHASE 1: Map (Decomposition)
            # Use Manifest if available (ADR 083)
            manifest_path = self.learning_dir / "learning_manifest.json"
            target_files = []
            
            if manifest_path.exists():
                logger.info(f"üß† RLM: Loading scope from {manifest_path.name}...")
                manifest_data = json.loads(manifest_path.read_text())
                
                # Handle modular manifest structure (ADR 089)
                if isinstance(manifest_data, dict):
                    core = manifest_data.get("core", [])
                    topic = manifest_data.get("topic", [])
                    target_files = core + topic
                    logger.info(f"üß† RLM: Merged {len(core)} core + {len(topic)} topic entries.")
                else:
                    # Legacy: flat array
                    target_files = manifest_data
            else:
                logger.warning("üß† RLM: No manifest found. Falling back to default roots.")
                target_files = ["01_PROTOCOLS", "ADRs"] # Safe default

            perception_map = self._rlm_map(target_files)
            
            # Phase 2: Reduce (Synthesis)
            hologram = self._rlm_reduce(perception_map)
            
            duration = time.time() - start_time
            logger.info(f"üß† RLM: Synthesis Complete in {duration:.2f} seconds.")
            
            # Append timing to hologram for visibility
            hologram += f"\n\n**Process Metrics:**\n* Total Synthesis Time: {duration:.2f}s"
            
            return hologram
        except Exception as e:
            logger.error(f"RLM Synthesis failed: {e}")
            return "## Cognitive Hologram [Failure]\n* System failed to synthesize state."

    def _rlm_map(self, targets: List[str]) -> Dict[str, str]:
        """
        Level 1: Iterate targets (Files or Dirs) and generate atomic summaries using local Qwen2-7B.
        Uses hash-based caching to skip unchanged files.
        """
        import requests
        import os
        import hashlib
        from dotenv import load_dotenv
        
        load_dotenv()
        
        # Configuration
        OLLAMA_URL = os.getenv("OLLAMA_HOST", "http://localhost:11434") + "/api/generate"
        MODEL_NAME = os.getenv("OLLAMA_MODEL", "hf.co/richfrem/Sanctuary-Qwen2-7B-v1.0-GGUF-Final:Q4_K_M")
        CACHE_PATH = self.project_root / ".agent" / "learning" / "rlm_summary_cache.json"
        
        # Load Cache
        cache = {}
        if CACHE_PATH.exists():
            try:
                cache = json.loads(CACHE_PATH.read_text())
                logger.info(f"üß† RLM: Loaded cache with {len(cache)} entries.")
            except:
                pass
        
        results = {}
        cache_hits = 0
        
        # 1. Flatten Targets into File List
        all_files = []
        
        for target in targets:
            path = self.project_root / target
            if not path.exists(): continue
            
            # Helper to check if a file should be skipped
            def should_skip(p: Path):
                rel_p = str(p.relative_to(self.project_root))
                
                # 1. Check central recursive artifact exclusion logic
                if self._is_recursive_artifact(rel_p):
                    return True
                
                # RLM: Process .md, .txt, and .in files (captured unless globally excluded)
                if p.suffix.lower() not in [".md", ".txt", ".in"]: return True
                
                return False
 
            if path.is_file():
                # Manifest Entry (File)
                if not should_skip(path):
                    all_files.append((str(path.parent), path))
            elif path.is_dir():
                # Recursive Scan (Legacy/Folder Mode)
                for subpath in path.rglob("*.md"):
                    if not should_skip(subpath):
                        all_files.append((str(path), subpath))
                
        # Deduplicate
        all_files = list(set(all_files))
        
        total_files = len(all_files)
        logger.info(f"üß† RLM: Mapping {total_files} files with model {MODEL_NAME}...")

        file_number = 0
        for root, path in all_files:
            file_number += 1
            rel_path = str(path.relative_to(self.project_root))
            
            try:
                content = path.read_text(errors='ignore')
                
                # Optimization: Skip empty files
                if not content.strip(): 
                    results[rel_path] = "[Empty File]"
                    continue
                
                # Compute hash for cache lookup
                content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
                
                # Check cache
                if rel_path in cache and cache[rel_path].get("hash") == content_hash:
                    results[rel_path] = cache[rel_path]["summary"]
                    cache_hits += 1
                    logger.info(f"   [{file_number}/{total_files}] {rel_path} [CACHE HIT]")
                    continue
                
                # Log progress
                logger.info(f"   [{file_number}/{total_files}] Processing {rel_path}...")
                
                if len(content) > 10000: 
                    content = content[:10000] + "\n...[Truncated]"

                # The Real Prompt
                prompt = (
                    f"Analyze the following Project Sanctuary document. "
                    f"Provide a single, dense sentence summarizing its architectural purpose and status.\n"
                    f"Document: {rel_path}\n"
                    f"Content:\n{content}\n\n"
                    f"Architectural Summary:"
                )

                # The Real Call (Ollama)
                # Timeout INCREASED to 300s per file to accommodate slow generations on large files
                response = requests.post(
                    OLLAMA_URL, 
                    json={
                        "model": MODEL_NAME,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "num_ctx": 4096,
                            "temperature": 0.1  # Low temp for factual precision
                        } 
                    },
                    timeout=300
                )
                
                if response.status_code == 200:
                    summary = response.json().get("response", "").strip()
                    # Clean up common LLM chatting artifacts
                    if summary.startswith("Here is a"): summary = summary.split(":", 1)[-1].strip()
                    results[rel_path] = summary
                    
                    # Update cache with file metadata
                    file_mtime = path.stat().st_mtime
                    cache[rel_path] = {
                        "hash": content_hash,
                        "summary": summary,
                        "file_mtime": file_mtime,
                        "summarized_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
                    }
                    
                    # PERSIST IMMEDIATELY (Incremental Save)
                    try:
                        CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
                        with open(CACHE_PATH, "w") as f:
                            json.dump(cache, f, indent=2, sort_keys=True)
                        logger.debug(f"üß† RLM: Incremental save for {rel_path}")
                    except Exception as e:
                        logger.warning(f"üß† RLM: Failed to save cache: {e}")

                else:
                    logger.warning(f"Ollama Error {response.status_code} for {rel_path}")
                    results[rel_path] = f"[Ollama Generation Failed: {response.status_code}]"

            except requests.exceptions.Timeout:
                logger.warning(f"Timeout processing {rel_path}")
                results[rel_path] = "[RLM Read Timeout]"
            except Exception as e:
                logger.warning(f"Failed to map {rel_path}: {e}")
                results[rel_path] = f"[Processing Error: {str(e)}]"
        
        return results

    def _rlm_reduce(self, map_data: Dict[str, str]) -> str:
        """
        Level 2: Synthesize atomic summaries into the Hologram.
        """
        lines = [
            "# Cognitive Hologram (Protocol 132)", 
            f"**Synthesis Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 
            f"**Engine:** Local Sovereign (Sanctuary-Qwen2-7B)",
            "",
            "> [!NOTE]",
            "> This context is recursively synthesized from the current system state using the local fine-tuned model.",
            ""
        ]
        
        # Group by Domain
        protocols = sorted([f"`{k}`: {v}" for k,v in map_data.items() if "PROTOCOL" in k])
        adrs = sorted([f"`{k}`: {v}" for k,v in map_data.items() if "ADR" in k])
        code = sorted([f"`{k}`: {v}" for k,v in map_data.items() if "mcp_servers" in k])
        
        lines.append(f"## 1. Constitutional State ({len(protocols)} Protocols)")
        lines.append("\n".join([f"* {p}" for p in protocols]))
        
        lines.append(f"\n## 2. Decision Record ({len(adrs)} Decisions)")
        lines.append("\n".join([f"* {a}" for a in adrs]))
        
        lines.append(f"\n## 3. Active Capabilities ({len(code)} Modules)")
        lines.append("\n".join([f"* {c}" for c in code[:30]])) # Slightly larger display
        if len(code) > 30: lines.append(f"* ... and {len(code)-30} more modules.")
        
        return "\n".join(lines)

    def _get_git_state(self, project_root: Path) -> Dict[str, Any]:

        """Captures current Git state signature."""
        try:
            git_status_proc = subprocess.run(
                ["git", "status", "--porcelain"],
                capture_output=True, text=True, cwd=str(project_root)
            )
            git_lines = git_status_proc.stdout.splitlines()
            changed_files = set()
            for line in git_lines:
                status_bits = line[:2]
                path = line[3:].split(" -> ")[-1].strip()
                if not path: # Handle cases where space might be missing or different
                    path = line[2:].strip()
                if 'D' not in status_bits: changed_files.add(path)
            
            state_str = "".join(sorted(git_lines))
            state_hash = hashlib.sha256(state_str.encode()).hexdigest()
            return {"lines": git_lines, "changed_files": changed_files, "hash": state_hash}
        except Exception as e:
            return {"lines": [], "changed_files": set(), "hash": "error"}

    #============================================================
    # 3. PERSIST SOUL (The Chronicle)
    #============================================================
    def persist_soul(self, request: PersistSoulRequest) -> PersistSoulResponse:
        """Broadcasts the session soul to Hugging Face."""
        from mcp_servers.lib.hf_utils import ensure_dataset_card
        from mcp_servers.lib.content_processor import ContentProcessor

        try:
            # 1. Environment & Metacognitive checks (Simplified)
            # ... (Checks skipped for brevity, full impl requires env vars)
            
            # 2. Dead Man's Switch (ADR 084)
            se_score = 0.5 # Default
            # In full impl: self._calculate_semantic_entropy(content)
            
            # 3. Initialization
            snapshot_path = self.project_root / request.snapshot_path
            if not snapshot_path.exists():
                return PersistSoulResponse(status="error", error=f"Snapshot not found: {snapshot_path}")
                
            # 4. Upload Logic (Delegated to hf_utils)
            import asyncio
            from mcp_servers.lib.hf_utils import upload_soul_snapshot, upload_semantic_cache
            
            logger.info(f"Uploading snapshot to HF: {snapshot_path}")
            result = asyncio.run(upload_soul_snapshot(
                snapshot_path=str(snapshot_path),
                valence=request.valence
            ))
            
            # 5. Upload Semantic Cache (ADR 094)
            cache_file = self.project_root / ".agent/learning/rlm_summary_cache.json"
            if cache_file.exists():
                logger.info("Syncing Semantic Ledger (RLM Cache) to HF...")
                cache_result = asyncio.run(upload_semantic_cache(str(cache_file)))
                if not cache_result.success:
                    logger.warning(f"Failed to sync Semantic Ledger: {cache_result.error}")
            
            if result.success:
                return PersistSoulResponse(
                    status="success",
                    repo_url=result.repo_url,
                    snapshot_name=result.remote_path
                )
            else:
                return PersistSoulResponse(status="error", error=result.error)

        except Exception as e:
            return PersistSoulResponse(status="error", error=str(e))

    def persist_soul_full(self) -> PersistSoulResponse:
        """
        Regenerate full Soul JSONL from all project files and deploy to HuggingFace.
        This is the "full sync" operation that rebuilds data/soul_traces.jsonl from scratch.
        """
        import asyncio
        import hashlib
        from datetime import datetime
        from mcp_servers.lib.content_processor import ContentProcessor
        from mcp_servers.lib.hf_utils import get_dataset_repo_id, get_hf_config
        from huggingface_hub import HfApi
        
        try:
            # 1. Generate Soul Data (same logic as scripts/generate_soul_data.py)
            staging_dir = self.project_root / "hugging_face_dataset_repo"
            data_dir = staging_dir / "data"
            data_dir.mkdir(exist_ok=True, parents=True)
            
            processor = ContentProcessor(str(self.project_root))
            
            ROOT_ALLOW_LIST = {
                "README.md", "chrysalis_core_essence.md", "Council_Inquiry_Gardener_Architecture.md",
                "Living_Chronicle.md", "PROJECT_SANCTUARY_SYNTHESIS.md", "Socratic_Key_User_Guide.md",
                "The_Garden_and_The_Cage.md", "GARDENER_TRANSITION_GUIDE.md",
            }
            
            records = []
            logger.info("üß† Generating full Soul JSONL...")
            
            for file_path in processor.traverse_directory(self.project_root):
                try:
                    rel_path = file_path.relative_to(self.project_root)
                except ValueError:
                    continue
                    
                if str(rel_path).startswith("hugging_face_dataset_repo"):
                    continue
                
                if rel_path.parent == Path("."):
                    if rel_path.name not in ROOT_ALLOW_LIST:
                        continue
                
                try:
                    content = processor.transform_to_markdown(file_path)
                    content_bytes = content.encode('utf-8')
                    checksum = hashlib.sha256(content_bytes).hexdigest()
                    timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
                    
                    clean_id = str(rel_path).replace("/", "_").replace("\\", "_")
                    while clean_id.endswith('.md'):
                        clean_id = clean_id[:-3]
                    
                    # ADR 084: Calculate SE for each record (Dead-Man's Switch)
                    try:
                        # Placeholder for SE logic until migrated
                        se_score = 0.5 
                        alignment_score = 0.85
                        stability_class = "STABLE"
                    except Exception as se_error:
                        logger.warning(f"ADR 084: SE calculation failed for {rel_path}: {se_error}")
                        se_score = 1.0
                        alignment_score = 0.0
                        stability_class = "VOLATILE"
                    
                    record = {
                        "id": clean_id,
                        "sha256": checksum,
                        "timestamp": timestamp,
                        "model_version": "Sanctuary-Qwen2-7B-v1.0-GGUF-Final",
                        "snapshot_type": "genome",
                        "valence": 0.5,
                        "uncertainty": 0.1,
                        "semantic_entropy": se_score,  # ADR 084
                        "alignment_score": alignment_score,  # ADR 084
                        "stability_class": stability_class,  # ADR 084
                        "adr_version": "084",  # ADR 084
                        "content": content,
                        "source_file": str(rel_path)
                    }
                    records.append(record)
                except Exception as e:
                    logger.debug(f"Skipping {rel_path}: {e}")
            
            # Write JSONL
            jsonl_path = data_dir / "soul_traces.jsonl"
            logger.info(f"üìù Writing {len(records)} records to {jsonl_path}")
            
            with open(jsonl_path, "w", encoding="utf-8") as f:
                for record in records:
                    f.write(json.dumps(record, ensure_ascii=True) + "\n")
            
            # 2. Deploy to HuggingFace
            config = get_hf_config()
            repo_id = get_dataset_repo_id(config)
            token = config["token"]
            api = HfApi(token=token)
            
            logger.info(f"üöÄ Deploying to {repo_id}...")
            
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(asyncio.to_thread(
                api.upload_folder,
                folder_path=str(data_dir),
                path_in_repo="data",
                repo_id=repo_id,
                repo_type="dataset",
                commit_message=f"Full Soul Genome Sync | {len(records)} records"
            ))
            
            logger.info("‚úÖ Full Soul Sync Complete")
            
            return PersistSoulResponse(
                status="success",
                repo_url=f"https://huggingface.co/datasets/{repo_id}",
                snapshot_name=f"data/soul_traces.jsonl ({len(records)} records)"
            )
        except Exception as e:
            logger.error(f"Full Soul Sync failed: {e}", exc_info=True)
            return PersistSoulResponse(status="error", error=str(e))

    #============================================================
    # 4. GUARDIAN WAKEUP (The Bootloader)
    #============================================================
    def guardian_wakeup(self, mode: str = "HOLISTIC") -> GuardianWakeupResponse:
        """Generate Guardian boot digest using manifest-driven content."""
        start = time.time()
        try:
            health_color, health_reason = self._get_system_health_traffic_light()
            integrity_status = "GREEN"
            container_status = self._get_container_status()
            
            digest_lines = [
                "# üõ°Ô∏è Guardian Wakeup Briefing (v3.0 - Manifest Driven)",
                f"**System Status:** {health_color} - {health_reason}",
                f"**Integrity Mode:** {integrity_status}",
                f"**Infrastructure:** {container_status}",
                f"**Generated Time:** {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())} UTC",
                "",
                "## I. Strategic Directives",
                self._get_strategic_synthesis(),
                "",
                "## II. Tactical Priorities",
                self._get_tactical_priorities(),
                "",
            ]
            
            # Load Guardian Manifest (ADR 089 format)
            learning_dir = self.project_root / ".agent" / "learning"
            manifest_path = learning_dir / "guardian_manifest.json"
            if manifest_path.exists():
                try:
                    manifest_data = json.loads(manifest_path.read_text())
                    if isinstance(manifest_data, dict):
                        core = manifest_data.get("core", [])
                        topic = manifest_data.get("topic", [])
                        all_files = core + topic
                    else:
                        all_files = manifest_data
                    
                    digest_lines.append("## III. Context Files (from guardian_manifest.json)")
                    digest_lines.append(f"*Loaded {len(all_files)} files.*")
                    digest_lines.append("")
                    
                    # Include key file summaries (first 500 chars each)
                    for file_path in all_files[:10]:  # Limit to first 10 files
                        full_path = self.project_root / file_path
                        if full_path.exists() and full_path.is_file():
                            try:
                                content = full_path.read_text(errors='ignore')[:500]
                                digest_lines.append(f"### {file_path}")
                                digest_lines.append(f"```\n{content}\n```\n")
                            except:
                                pass
                except Exception as e:
                    logger.warning(f"Failed to load guardian manifest: {e}")
            
            # Write to file
            digest_path = self.project_root / ".agent" / "learning" / "guardian_boot_digest.md"
            digest_path.parent.mkdir(parents=True, exist_ok=True)
            digest_path.write_text("\n".join(digest_lines))
            
            return GuardianWakeupResponse(
                status="success", digest_path=str(digest_path), 
                total_time_ms=(time.time()-start)*1000
            )
            
        except Exception as e:
            return GuardianWakeupResponse(status="error", digest_path="", error=str(e))

    #============================================================
    # HELPER STUBS (Migrated)
    #============================================================
    def _get_system_health_traffic_light(self):
        # Simplified Check - Real one checks Vector DB
        return "GREEN", "Nominal (Learning Mode)"

    def _get_container_status(self):
        # Using podman check
        try:
            result = subprocess.run(
                ["podman", "ps", "--format", "{{.Names}}"],
                capture_output=True, text=True, timeout=2
            )
            if "sanctuary" in result.stdout: return "‚úÖ Fleet Active"
        except: pass
        return "‚ö†Ô∏è Container Check Failed"

    def _get_strategic_synthesis(self):
        return ("* **Core Mandate:** I am the Gemini Orchestrator. Values: Integrity, Efficiency, Clarity. "
                "Executing Protocol 128.")

    def _get_tactical_priorities(self):
        # Scans for tasks
        scan_dir = self.project_root / "tasks" / "in-progress"
        if scan_dir.exists():
            tasks = list(scan_dir.glob("*.md"))
            if tasks: return f"* Found {len(tasks)} active tasks."
        return "* No active tasks found."

```

---

## üì¶ Component: Ledger Schema
**Source:** `mcp_servers/learning/rlm_schema.json`
_Formal JSON schema defining the cache structure._

```json
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Reactive Ledger Memory (RLM) Cache",
    "description": "A flat key-value store mapping file paths to their semantic summaries.",
    "type": "object",
    "additionalProperties": {
        "type": "object",
        "required": [
            "hash",
            "mtime",
            "summarized_at",
            "summary"
        ],
        "properties": {
            "hash": {
                "type": "string",
                "description": "MD5 hash of the file content at time of summarization."
            },
            "file_mtime": {
                "type": "number",
                "description": "File modification timestamp (epoch)."
            },
            "summarized_at": {
                "type": "string",
                "format": "date-time",
                "description": "ISO 8601 timestamp of when the summary was generated."
            },
            "summary": {
                "type": "string",
                "description": "High-entropy semantic summary of the file's purpose and content."
            }
        }
    }
}
```

---

## üì¶ Component: Inventory Auditor (The Check)
**Source:** `scripts/rlm_factory/inventory.py`
_The script to verify coverage of the semantic ledger._

```python
import json
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

def get_inventory():
    # Adjusted for location in scripts/rlm_factory/
    project_root = Path(__file__).parent.parent.parent.absolute()
    # Load from Env or Default
    env_cache = os.getenv("RLM_CACHE_PATH")
    if env_cache:
        cache_path = project_root / env_cache
    else:
        cache_path = project_root / ".agent" / "learning" / "rlm_summary_cache.json"

    env_targets = os.getenv("RLM_TARGET_DIRS")
    if env_targets:
        target_dirs = [t.strip() for t in env_targets.split(",") if t.strip()]
    else:
        # Default fallback for this repo
        target_dirs = ["docs", "ADRs", "01_PROTOCOLS", "mcp_servers", "LEARNING"]
    
    if not cache_path.exists():
        print(f"‚ùå Cache not found at {cache_path}")
        return

    with open(cache_path, "r") as f:
        cache = json.load(f)
    
    print(f"üß† RLM Semantic Ledger Inventory")
    print(f"==============================")
    print(f"{'Directory':<15} | {'Total':<5} | {'Cached':<6} | {'Status':<10}")
    print(f"--------------------------------------------------")

    missing_files = []

    for d in target_dirs:
        dir_path = project_root / d
        if not dir_path.exists():
            continue
            
        # Find all .md and .txt files (as per RLM rules)
        files = []
        for ext in ["**/*.md", "**/*.txt"]:
            files.extend([p for p in dir_path.glob(ext) if p.is_file()])
        
        # Filter out anything in 'archive' subdirs
        files = [f for f in files if "archive" not in str(f).lower()]
        
        total = len(files)
        cached_count = 0
        dir_missing = []

        for f in files:
            rel_path = str(f.relative_to(project_root))
            if rel_path in cache:
                cached_count += 1
            else:
                dir_missing.append(rel_path)
        
        status = "‚úÖ COMPLETE" if total == cached_count else f"‚ö†Ô∏è  {total - cached_count} MISSING"
        print(f"{d:<15} | {total:<5} | {cached_count:<6} | {status}")
        
        missing_files.extend(dir_missing)

    if missing_files:
        print(f"\nüìë Missing Files (Ready for targeting):")
        print(f"--------------------------------------")
        # Group by folder for readability
        current_folder = ""
        for mf in sorted(missing_files):
            folder = os.path.dirname(mf)
            if folder != current_folder:
                print(f"\nüìÇ {folder}/")
                current_folder = folder
            print(f"  - {mf}")
        
        print(f"\nüí° Tip: Run 'python3 scripts/cortex_cli.py rlm-distill <file_path>' to fix specific gaps.")
    else:
        print(f"\nüéâ All priority directories are fully distilled into the Semantic Ledger.")

if __name__ == "__main__":
    get_inventory()

```

---

## üì¶ Component: ChromaDB Ingestor (The Vector Memory)
**Source:** `scripts/rlm_factory/chroma_ingest.py`
_Standalone script for ingesting files into ChromaDB._

```python
#!/usr/bin/env python3
"""
Standalone ChromaDB Ingestion Script for RLM Kit.
Provides both Full and Incremental ingestion capabilities.
"""
import os
import sys
import logging
from typing import List, Optional
from pathlib import Path
from dotenv import load_dotenv

# Third-party imports (Install: pip install langchain-chroma langchain-huggingface python-dotenv)
import chromadb
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Setup Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("chroma_ingest")

load_dotenv()

class VectorMemory:
    def __init__(self, persist_directory: str = ".vector_data"):
        self.persist_directory = persist_directory
        self.embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        
        # Initialize Client
        self.vector_store = Chroma(
            collection_name="project_memory",
            embedding_function=self.embedding_function,
            persist_directory=self.persist_directory
        )
        
    def ingest_file(self, file_path: Path):
        """Ingest a single file with chunking."""
        try:
            loader = TextLoader(str(file_path), encoding="utf-8")
            documents = loader.load()
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                add_start_index=True
            )
            chunks = text_splitter.split_documents(documents)
            
            # Add to DB
            self.vector_store.add_documents(chunks)
            logger.info(f"‚úÖ Ingested {file_path} ({len(chunks)} chunks)")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to ingest {file_path}: {e}")

    def ingest_directory(self, directory: Path, extensions: List[str] = [".md", ".txt", ".py"]):
        """Recursively ingest a directory."""
        if not directory.exists():
            logger.warning(f"Directory not found: {directory}")
            return
            
        logger.info(f"üìÇ Scanning {directory}...")
        for root, _, files in os.walk(directory):
            for file in files:
                if any(file.endswith(ext) for ext in extensions):
                    file_path = Path(root) / file
                    self.ingest_file(file_path)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python chroma_ingest.py <file_or_directory>")
        sys.exit(1)
        
    target = Path(sys.argv[1])
    memory = VectorMemory()
    
    if target.is_dir():
        memory.ingest_directory(target)
    else:
        memory.ingest_file(target)

```

---

## üì¶ Component: CLI Interface (The Driver)
**Source:** `scripts/cortex_cli.py`
_The command-line interface for triggering RLM operations._

```python
#============================================
# scripts/cortex_cli.py
# Purpose: CLI Orchestrator for the Mnemonic Cortex RAG server.
# Role: Single Source of Truth for Terminal Operations.
# Reference: Protocol 128 (Hardened Learning Loop)
#
# INGESTION EXAMPLES:
#   python3 scripts/cortex_cli.py ingest                    # Full purge & rebuild (Default behavior)
#   python3 scripts/cortex_cli.py ingest --no-purge         # Append to existing Vector DB
#   python3 scripts/cortex_cli.py ingest --dirs "LEARNING"  # Target specific directory ingestion
#   python3 scripts/cortex_cli.py ingest --type incremental --files "path/to/file.md"  # Targeted update
#
# SNAPSHOT EXAMPLES (Protocol 128 Workflow):
#   python3 scripts/cortex_cli.py snapshot --type audit --manifest .agent/learning/red_team/red_team_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type learning_audit --manifest .agent/learning/learning_audit/learning_audit_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type seal --manifest .agent/learning/learning_manifest.json
#   python3 scripts/cortex_cli.py snapshot --type learning_audit --context "Egyptian Labyrinth research"
#
# GUARDIAN WAKEUP (Protocol 128 Bootloader):
#   python3 scripts/cortex_cli.py guardian                     # Standard wakeup
#   python3 scripts/cortex_cli.py guardian --mode TELEMETRY    # Telemetry-focused wakeup
#   python3 scripts/cortex_cli.py guardian --show              # Display digest content after generation
#
# BOOTSTRAP DEBRIEF (Fresh Repo Onboarding):
#   python3 scripts/cortex_cli.py bootstrap-debrief            # Generate onboarding context packet
#
# DIAGNOSTICS & RETRIEVAL:
#   python3 scripts/cortex_cli.py stats                     # View child/parent counts & health
#   python3 scripts/cortex_cli.py query "Protocol 128"      # Semantic search across Mnemonic Cortex
#   python3 scripts/cortex_cli.py debrief --hours 48        # Session diff & recency scan
#   python3 scripts/cortex_cli.py cache-stats               # Check semantic cache (CAG) efficiency
#   python3 scripts/cortex_cli.py cache-warmup              # Pre-populate CAG with genesis queries
#
# SOUL PERSISTENCE (ADR 079 / 081):
#   Incremental (append 1 seal to JSONL + upload MD to lineage/):
#     python3 scripts/cortex_cli.py persist-soul
#     python3 scripts/cortex_cli.py persist-soul --valence 0.8 --snapshot .agent/learning/learning_package_snapshot.md
#
#   Full Sync (regenerate entire JSONL from all files + deploy data/):
#     python3 scripts/cortex_cli.py persist-soul-full
#
# EVOLUTIONARY METRICS (Protocol 131):
#   python3 scripts/cortex_cli.py evolution fitness "Some content"
#   python3 scripts/cortex_cli.py evolution depth --file .agent/learning/learning_debrief.md
#
# RLM DISTILLATION (Protocol 132):
#   python3 scripts/cortex_cli.py rlm-distill README.md        # Distill summary for a file
#   python3 scripts/cortex_cli.py rlm-distill "ADRs"            # Distill summaries for a directory (Recursive)
#============================================
import argparse
import sys
import json
from pathlib import Path

# Add project root to path for imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from mcp_servers.rag_cortex.operations import CortexOperations
from mcp_servers.learning.operations import LearningOperations
from mcp_servers.evolution.operations import EvolutionOperations
import subprocess

# ADR 090: Iron Core Definitions
IRON_CORE_PATHS = [
    "01_PROTOCOLS",
    "ADRs",
    "cognitive_continuity_policy.md",
    "founder_seed.json"
]

def verify_iron_core(root_path):
    """
    Verifies that Iron Core paths have not been tampered with (uncommitted/unstaged changes).
    ADR 090 (Evolution-Aware):
    - Unstaged changes (Dirty Worktree) -> VIOLATION (Drift)
    - Staged changes (Index) -> ALLOWED (Evolution)
    """
    violations = []
    try:
        # Check for modifications in Iron Core paths
        # --porcelain format:
        # XY Path
        # X = Index (Staged), Y = Worktree (Unstaged)
        # We only care if Y is modified (meaning unstaged changes exist)
        cmd = ["git", "status", "--porcelain"] + IRON_CORE_PATHS
        result = subprocess.run(
            cmd, 
            cwd=root_path, 
            capture_output=True, 
            text=True, 
            check=False
        )
        
        if result.stdout.strip():
            for line in result.stdout.strip().split('\n'):
                # Line format: "XY Path" (e.g., " M file.md", "M  file.md", "?? file.md")
                if len(line.strip()) < 3: 
                    continue
                    
                status_code = line[:2]
                path = line[3:]
                
                # Check Worktree Status (2nd character)
                # ' ' = Unmodified in worktree (changes are staged or clean)
                # 'M' = Modified in worktree
                # 'D' = Deleted in worktree
                # '?' = Untracked
                worktree_status = status_code[1]
                
                # Violation if:
                # 1. Untracked ('??') inside Iron Core path (adding new files without staging)
                # 2. Modified in Worktree ('M') (editing without staging)
                # 3. Deleted in Worktree ('D') (deleting without staging)
                if status_code == '??' or worktree_status in ['M', 'D']:
                    violations.append(f"{line.strip()} (Unstaged/Dirty - Please 'git add' to authorize)")
                
    except Exception as e:
        return False, [f"Error checking Iron Core: {str(e)}"]
        
    return len(violations) == 0, violations


def main():
    parser = argparse.ArgumentParser(description="Mnemonic Cortex CLI")
    parser.add_argument("--root", default=".", help="Project root directory")
    
    subparsers = parser.add_subparsers(dest="command", help="Available operations")

    # Command: ingest
    ingest_parser = subparsers.add_parser("ingest", help="Perform full ingestion")
    ingest_parser.add_argument("--no-purge", action="store_false", dest="purge", help="Skip purging DB")
    ingest_parser.add_argument("--dirs", nargs="+", help="Specific directories to ingest")
    ingest_parser.add_argument("--incremental", action="store_true", help="Incremental ingestion mode")
    ingest_parser.add_argument("--hours", type=int, default=24, help="Hours to look back (for incremental mode)")

    # Command: snapshot
    snapshot_parser = subparsers.add_parser("snapshot", help="Capture a Protocol 128 snapshot")
    snapshot_parser.add_argument("--type", choices=["audit", "learning_audit", "seal"], required=True)
    snapshot_parser.add_argument("--manifest", help="Path to manifest JSON file")
    snapshot_parser.add_argument("--context", help="Strategic context for the snapshot")
    snapshot_parser.add_argument("--override-iron-core", action="store_true", help="‚ö†Ô∏è Override Iron Core check (Requires ADR 090 Amendment)")

    # Command: stats
    stats_parser = subparsers.add_parser("stats", help="Get RAG health and statistics")
    stats_parser.add_argument("--samples", action="store_true", help="Include sample documents")
    stats_parser.add_argument("--sample-count", type=int, default=5, help="Number of samples to include")

    # Command: query
    query_parser = subparsers.add_parser("query", help="Perform semantic search query")
    query_parser.add_argument("query_text", help="Search query string")
    query_parser.add_argument("--max-results", type=int, default=5, help="Maximum results to return")
    query_parser.add_argument("--use-cache", action="store_true", help="Use semantic cache")

    # Command: debrief
    debrief_parser = subparsers.add_parser("debrief", help="Run learning debrief (Protocol 128)")
    debrief_parser.add_argument("--hours", type=int, default=24, help="Lookback window in hours")
    debrief_parser.add_argument("--output", help="Output file path (default: .agent/learning/learning_debrief.md)")

    # [DISABLED] Synaptic Phase (Dreaming)
    # dream_parser = subparsers.add_parser("dream", help="Execute Synaptic Phase (Dreaming)")

    # Command: guardian (Protocol 128 Bootloader)
    guardian_parser = subparsers.add_parser("guardian", help="Generate Guardian Boot Digest (Protocol 128)")
    guardian_parser.add_argument("--mode", default="HOLISTIC", choices=["HOLISTIC", "TELEMETRY"], help="Wakeup mode")
    guardian_parser.add_argument("--show", action="store_true", help="Display digest content after generation")
    guardian_parser.add_argument("--manifest", default=".agent/learning/guardian_manifest.json", help="Path to guardian manifest")

    # Command: bootstrap-debrief (Fresh Repo Onboarding)
    bootstrap_parser = subparsers.add_parser("bootstrap-debrief", help="Generate onboarding context packet for fresh repo setup")
    bootstrap_parser.add_argument("--manifest", default=".agent/learning/bootstrap_manifest.json", help="Path to bootstrap manifest")
    bootstrap_parser.add_argument("--output", default=".agent/learning/bootstrap_packet.md", help="Output path for the packet")

    # Command: cache-stats
    subparsers.add_parser("cache-stats", help="Get cache statistics")

    # Command: cache-warmup
    warmup_parser = subparsers.add_parser("cache-warmup", help="Pre-populate cache with genesis queries")
    warmup_parser.add_argument("--queries", nargs="+", help="Custom queries to cache")

    # Command: persist-soul (ADR 079)
    soul_parser = subparsers.add_parser("persist-soul", help="Broadcast snapshot to HF AI Commons")
    soul_parser.add_argument("--snapshot", default=".agent/learning/learning_package_snapshot.md", help="Path to snapshot")
    soul_parser.add_argument("--valence", type=float, default=0.0, help="Moral/emotional charge")
    soul_parser.add_argument("--uncertainty", type=float, default=0.0, help="Logic confidence")
    soul_parser.add_argument("--full-sync", action="store_true", help="Sync entire learning directory")

    # Command: persist-soul-full (ADR 081)
    subparsers.add_parser("persist-soul-full", help="Regenerate full JSONL and deploy to HF (ADR 081)")

    # evolution (Protocol 131)
    evolution_parser = subparsers.add_parser("evolution", help="Evolutionary metrics (Protocol 131)")
    evolution_sub = evolution_parser.add_subparsers(dest="subcommand", help="Evolution subcommands")
    
    # fitness
    fit_parser = evolution_sub.add_parser("fitness", help="Calculate full fitness vector")
    fit_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    fit_parser.add_argument("--file", help="Read content from file")
    
    # depth
    depth_parser = evolution_sub.add_parser("depth", help="Evaluate technical depth")
    depth_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    depth_parser.add_argument("--file", help="Read content from file")
    
    # scope
    scope_parser = evolution_sub.add_parser("scope", help="Evaluate architectural scope")
    scope_parser.add_argument("content", nargs="?", help="Text content to evaluate")
    scope_parser.add_argument("--file", help="Read content from file")

    # Command: rlm-distill (Protocol 132)
    rlm_parser = subparsers.add_parser("rlm-distill", aliases=["rlm-test"], help="Distill semantic summaries for a specific file or folder")
    rlm_parser.add_argument("target", help="File or folder path to distill (relative to project root)")

    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Initialize Operations
    cortex_ops = CortexOperations(project_root=args.root)
    learning_ops = LearningOperations(project_root=args.root)
    evolution_ops = EvolutionOperations(project_root=args.root)

    if args.command == "ingest":
        if args.incremental:
            print(f"üîÑ Starting INCREMENTAL ingestion (Last {args.hours}h)...")
            import time
            from datetime import timedelta
            
            cutoff_time = time.time() - (args.hours * 3600)
            modified_files = []
            
            # Walk project root to find modified files
            # Exclude known heavy/irrelevant dirs
            exclude_dirs = {'.git', '.vector_data', '__pycache__', 'node_modules', 'venv', 'env', 
                            'dataset_package', 'docs/site', 'training_logs'}
            
            for path in cortex_ops.project_root.rglob('*'):
                if path.is_file():
                    # Check exclusions
                    if any(part in exclude_dirs for part in path.parts):
                        continue
                        
                    # Check extension
                    if path.suffix not in ['.md', '.py', '.js', '.ts', '.txt', '.json']:
                        continue
                        
                    # Check mtime
                    if path.stat().st_mtime > cutoff_time:
                        modified_files.append(str(path))
            
            if not modified_files:
                print(f"‚ö†Ô∏è No files modified in the last {args.hours} hours. Skipping ingestion.")
                sys.exit(0)
                
            print(f"üìÑ Found {len(modified_files)} modified files.")
            res = cortex_ops.ingest_incremental(file_paths=modified_files)
            
            if res.status == "success":
                print(f"‚úÖ Success: {res.documents_added} added, {res.chunks_created} chunks in {res.ingestion_time_ms/1000:.2f}s")
            else:
                print(f"‚ùå Error: {res.error}")
                sys.exit(1)
        
        else:
            # Full Ingestion
            print(f"üîÑ Starting full ingestion (Purge: {args.purge})...")
            res = cortex_ops.ingest_full(purge_existing=args.purge, source_directories=args.dirs)
            if res.status == "success":
                print(f"‚úÖ Success: {res.documents_processed} docs, {res.chunks_created} chunks in {res.ingestion_time_ms/1000:.2f}s")
            else:
                print(f"‚ùå Error: {res.error}")
                sys.exit(1)

    elif args.command == "snapshot":
        # ADR 090: Iron Core Verification
        if not args.override_iron_core:
            print("üõ°Ô∏è  Running Iron Core Verification (ADR 090)...")
            is_pristine, violations = verify_iron_core(args.root)
            if not is_pristine:
                print(f"\n\033[91m‚õî IRON CORE BREACH DETECTED (SAFE MODE ENGAGED)\033[0m")
                print("The following immutable files have been modified without authorization:")
                for v in violations:
                    print(f"  - {v}")
                print("\nAction blocked: 'snapshot' is disabled in Safe Mode.")
                print("To proceed, revert changes or use --override-iron-core (Constitutional Amendment required).")
                sys.exit(1)
            print("‚úÖ Iron Core Integrity Verified.")
        else:
            print(f"‚ö†Ô∏è  \033[93mWARNING: IRON CORE CHECK OVERRIDDEN\033[0m")

        manifest = []
        if args.manifest:
            manifest_path = Path(args.manifest)
            if not manifest_path.exists():
                print(f"‚ùå Manifest file not found: {args.manifest}")
                sys.exit(1)
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"üìã Loaded manifest with {len(manifest)} files")
        
        print(f"üì∏ Capturing {args.type} snapshot...")
        # ROUTED TO LEARNING MCP
        res = learning_ops.capture_snapshot(
            manifest_files=manifest, 
            snapshot_type=args.type,
            strategic_context=args.context
        )
        
        if res.status == "success":
            print(f"‚úÖ Snapshot created at: {res.snapshot_path}")
            print(f"üìä Files: {res.total_files} | Bytes: {res.total_bytes}")
            print(f"üîç Manifest verified: {res.manifest_verified}")
            print(f"üìù Git context: {res.git_diff_context}")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "stats":
        stats = cortex_ops.get_stats(include_samples=args.samples, sample_count=args.sample_count)
        print(f"üè• Health: {stats.health_status}")
        print(f"üìö Documents: {stats.total_documents}")
        print(f"üß© Chunks: {stats.total_chunks}")
        
        if stats.collections:
            print("\nüìä Collections:")
            for name, coll in stats.collections.items():
                print(f"  - {coll.name}: {coll.count} items")
        
        if stats.samples:
            print(f"\nüîç Sample Documents:")
            for i, sample in enumerate(stats.samples, 1):
                print(f"\n  {i}. ID: {sample.id}")
                print(f"     Preview: {sample.content_preview[:100]}...")
                if sample.metadata:
                    print(f"     Metadata: {sample.metadata}")
        
        if stats.error:
            print(f"\n‚ùå Error: {stats.error}")

    # [DISABLED] Synaptic Phase (Dreaming)
    # elif args.command == "dream":
    #     print("üí§ Mnemonic Cortex: Entering Synaptic Phase (Dreaming)...")
    #     # Use centralized Operations layer
    #     response = ops.dream()
    #     print(json.dumps(response, indent=2))
    elif args.command == "query":
        print(f"üîç Querying: {args.query_text}")
        res = cortex_ops.query(
            query=args.query_text,
            max_results=args.max_results,
            use_cache=args.use_cache
        )
        
        if res.status == "success":
            print(f"‚úÖ Found {len(res.results)} results in {res.query_time_ms:.2f}ms")
            print(f"üíæ Cache hit: {res.cache_hit}")
            
            for i, result in enumerate(res.results, 1):
                print(f"\n--- Result {i} (Score: {result.relevance_score:.4f}) ---")
                print(f"Content: {result.content[:300]}...")
                if result.metadata:
                    source = result.metadata.get('source', 'Unknown')
                    print(f"Source: {source}")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "debrief":
        print(f"üìã Running learning debrief (lookback: {args.hours}h)...")
        # ROUTED TO LEARNING MCP
        debrief_content = learning_ops.learning_debrief(hours=args.hours)
        
        # Default output path
        output_path = args.output or ".agent/learning/learning_debrief.md"
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            f.write(debrief_content)
        
        print(f"‚úÖ Debrief written to: {output_file}")
        print(f"üìä Content length: {len(debrief_content)} characters")

    elif args.command == "guardian":
        print(f"üõ°Ô∏è Generating Guardian Boot Digest (mode: {args.mode})...")
        
        # Load manifest if exists
        manifest_path = Path(args.manifest)
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"üìã Loaded guardian manifest: {len(manifest)} files")
        else:
            print(f"‚ö†Ô∏è  Guardian manifest not found at {args.manifest}. Using defaults.")
        
        # ROUTED TO LEARNING MCP
        response = learning_ops.guardian_wakeup(mode=args.mode)
        
        print(f"   Status: {response.status}")
        print(f"   Digest: {response.digest_path}")
        print(f"   Time: {response.total_time_ms:.2f}ms")
        
        if response.error:
            print(f"‚ùå Error: {response.error}")
            sys.exit(1)
        
        if args.show and response.digest_path:
            print("\n" + "="*60)
            with open(response.digest_path, 'r') as f:
                print(f.read())
        
        print(f"‚úÖ Guardian Boot Digest generated.")

    elif args.command == "bootstrap-debrief":
        print(f"üèóÔ∏è  Generating Bootstrap Context Packet...")
        
        # Load manifest
        manifest_path = Path(args.manifest)
        manifest = []
        if manifest_path.exists():
            with open(manifest_path, 'r') as f:
                manifest = json.load(f)
            print(f"üìã Loaded bootstrap manifest: {len(manifest)} files")
        else:
            print(f"‚ö†Ô∏è  Bootstrap manifest not found at {args.manifest}. Using defaults.")
        
        # Generate snapshot using the manifest
        # ROUTED TO LEARNING MCP
        res = learning_ops.capture_snapshot(
            manifest_files=manifest,
            snapshot_type="seal",
            strategic_context="Fresh repository onboarding context"
        )
        
        if res.status == "success":
            # Copy to output path
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            import shutil
            shutil.copy(res.snapshot_path, output_path)
            
            print(f"‚úÖ Bootstrap packet generated: {output_path}")
            print(f"üìä Files: {res.total_files} | Bytes: {res.total_bytes}")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "cache-stats":
        stats = cortex_ops.get_cache_stats()
        print(f"üíæ Cache Statistics:")
        if isinstance(stats, dict):
            for key, value in stats.items():
                print(f"  {key}: {value}")
        else:
            print(f"  {stats}")

    elif args.command == "cache-warmup":
        queries = args.queries or None
        print(f"üî• Warming up cache...")
        res = cortex_ops.cache_warmup(genesis_queries=queries)
        
        if res.status == "success":
            print(f"‚úÖ Cached {res.queries_cached} queries")
            print(f"üíæ Cache hits: {res.cache_hits}")
            print(f"‚ùå Cache misses: {res.cache_misses}")
            print(f"‚è±Ô∏è  Total time: {res.total_time_ms/1000:.2f}s")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "persist-soul":
        from mcp_servers.learning.models import PersistSoulRequest
        print(f"üå± Broadcasting soul to Hugging Face AI Commons...")
        print(f"   Snapshot: {args.snapshot}")
        print(f"   Valence: {args.valence} | Uncertainty: {args.uncertainty}")
        print(f"   Full sync: {args.full_sync}")
        
        request = PersistSoulRequest(
            snapshot_path=args.snapshot,
            valence=args.valence,
            uncertainty=args.uncertainty,
            is_full_sync=args.full_sync
        )
        # ROUTED TO LEARNING MCP
        res = learning_ops.persist_soul(request)
        
        if res.status == "success":
            print(f"‚úÖ Soul planted successfully!")
            print(f"üîó Repository: {res.repo_url}")
            print(f"üìÑ Snapshot: {res.snapshot_name}")
        elif res.status == "quarantined":
            print(f"üö´ Quarantined: {res.error}")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "persist-soul-full":
        print(f"üß¨ Regenerating full Soul JSONL and deploying to HuggingFace...")
        # ROUTED TO LEARNING MCP
        res = learning_ops.persist_soul_full()
        
        if res.status == "success":
            print(f"‚úÖ Full sync complete!")
            print(f"üîó Repository: {res.repo_url}")
            print(f"üìÑ Output: {res.snapshot_name}")
        else:
            print(f"‚ùå Error: {res.error}")
            sys.exit(1)

    elif args.command == "evolution":
        if not args.subcommand:
            print("‚ùå Subcommand required for 'evolution' (fitness, depth, scope)")
            sys.exit(1)
            
        content = args.content
        if args.file:
            try:
                content = Path(args.file).read_text()
            except Exception as e:
                print(f"‚ùå Error reading file {args.file}: {e}")
                sys.exit(1)
        
        if not content:
            print("‚ùå No content provided. Use a positional argument or --file.")
            sys.exit(1)
            
        if args.subcommand == "fitness":
            res = evolution_ops.calculate_fitness(content)
            print(json.dumps(res, indent=2))
        elif args.subcommand == "depth":
            res = evolution_ops.measure_depth(content)
            print(f"Depth: {res}")
        elif args.subcommand == "scope":
            res = evolution_ops.measure_scope(content)
            print(f"Scope: {res}")

    elif args.command in ["rlm-distill", "rlm-test"]:
        print(f"üß† RLM: Distilling semantic essence of '{args.target}'...")
        import time
        start = time.time()
        
        # Call _rlm_map directly with the target
        results = learning_ops._rlm_map([args.target])
        
        duration = time.time() - start
        print(f"‚è±Ô∏è  Completed in {duration:.2f}s")
        print(f"üìä Files Processed: {len(results)}")
        print("=" * 60)
        
        for file_path, summary in results.items():
            print(f"\nüìÑ {file_path}")
            print(f"   {summary}")


if __name__ == "__main__":
    main()

```

---

## üì¶ Component: Wait-for-Green Helper
**Source:** `scripts/library/wait_for_green.py`
_Helper for safe background operations._

```python
[ERROR: File not found: /Users/richardfremmerlid/Projects/Project_Sanctuary/scripts/library/wait_for_green.py]
```

---

## üõ†Ô∏è Environment Setup Guide
### 1. Configuration (.env)
Create a `.env` file in your project root:
```bash
RLM_CACHE_PATH=.agent/learning/rlm_summary_cache.json
RLM_TARGET_DIRS=src,docs,lib  # Customize this list for your repo
```

### 2. Prerequisite: Ollama (The Brain)
1.  **Install:** `curl -fsSL https://ollama.com/install.sh | sh`
2.  **Pull Model:** `ollama pull qwen2.5:7b` (Recommended default)
3.  **Serve:** `ollama serve`

### Prerequisite: Vector DB (Optional Augmentation)
If augmenting a Vector DB (Chroma), ensure you prepend the RLM summary to chunks:
```python
# Pseudo-code for Augmentation
summary = ledger[file_path]['summary']
chunk_content = f"[CONTEXT_SUMMARY: {summary}]\n\n{original_chunk}"
vector_db.add(documents=[chunk_content])
```
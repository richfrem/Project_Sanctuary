services:
  vector-db:
    image: chromadb/chroma:latest
    container_name: sanctuary-vector-db
    ports:
      - "8000:8000"
    volumes:
      # Use CHROMA_DATA_PATH from .env, default to .vector_data if not set
      - ${CHROMA_DATA_PATH:-.vector_data}:/chroma/chroma
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  ollama-model-mcp:
    image: ollama/ollama:latest
    container_name: sanctuary-ollama-mcp
    ports:
      - "11434:11434"
    volumes:
      # Mount point for models persisted on the host machine
      - ./ollama_models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      # Use OLLAMA_MODEL from the host environment if available, otherwise default
      - OLLAMA_MODEL=${OLLAMA_MODEL:-Sanctuary-Qwen2-7B:latest}
    deploy:
      resources:
        reservations:
          devices:
            # CRITICAL: Configure for GPU acceleration (Example for NVIDIA/Podman)
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    entrypoint: [ "/bin/sh", "-c" ]
    command:
      - |
        ollama serve &
        sleep 10
        ollama pull $${OLLAMA_MODEL:-Sanctuary-Qwen2-7B:latest}
        wait -n
    restart: unless-stopped
